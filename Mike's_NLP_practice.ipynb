{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mike's NLP-practice.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "OSx_rgGh9bf6"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chendingyan/NLP490H/blob/master/Mike's_NLP_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_N5pSxDNnGfF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NLP Coursework"
      ]
    },
    {
      "metadata": {
        "id": "QnmwkL__fdDG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Initialization"
      ]
    },
    {
      "metadata": {
        "id": "mxXsBVkuocdI",
        "colab_type": "code",
        "outputId": "e5c4d453-f8b8-479c-d22a-768b0afd07ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Etz7hcPYCCaB",
        "colab_type": "code",
        "outputId": "bd2480af-9f52-4f86-d7c3-9e7dcb2c8016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip drive/My\\ Drive/data/OffensEval_task_data/Test\\ C\\ Release.zip -d drive/My\\ Drive/data/OffensEval_task_data/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/data/OffensEval_task_data/Test C Release.zip\n",
            "  inflating: drive/My Drive/data/OffensEval_task_data/test_set_taskc.tsv  \n",
            "  inflating: drive/My Drive/data/OffensEval_task_data/readme-testsetc-v1.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JrLzlL1BCs9y",
        "colab_type": "code",
        "outputId": "298fe481-1000-45ec-bcf5-d5b6af941557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.9 GB  | Proc size: 142.2 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AHhTNuEWLAcz",
        "colab_type": "code",
        "outputId": "ce194514-b921-45d9-842c-0d67abe3c715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm \n",
        "import codecs\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score, classification_report,f1_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D,Dropout\n",
        "from keras.layers import Dense, Embedding, Activation, merge, Input, Lambda, Reshape\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import np_utils\n",
        "from keras.layers.merge import concatenate\n",
        "from keras import optimizers\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "stop = stopwords.words(\"english\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "#we fix the seeds to get consistent results\n",
        "\n",
        "SEED = 234\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rpePPeOvtW46",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ]
    },
    {
      "metadata": {
        "id": "IIyUwk6-LMC4",
        "colab_type": "code",
        "outputId": "a948e25e-3603-4255-b704-5428ecceae59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "PATH = 'drive/My Drive/data/OffensEval_task_data/offenseval-training-v1.tsv'\n",
        "df = pd.read_csv(PATH,sep='\\t')\n",
        "\n",
        "\n",
        "print('Indexing word vectors.')\n",
        "glove_path = 'drive/My Drive/data/glove.6B.300d.txt'\n",
        "embeddings_index = {}\n",
        "f = open(glove_path, encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors.\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K69PmvHpS2xM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "metadata": {
        "id": "waDE_uxbEhXx",
        "colab_type": "code",
        "outputId": "f5c2e7e7-fb61-48d9-dd8e-faad8965aa65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "def preprocess(df):\n",
        "    print('-------Remove Stop Word--------')\n",
        "    stopword_set = set(stopwords.words(\"english\"))\n",
        "\n",
        "    # convert to lower case and split \n",
        "    df.tweet = df.tweet.apply(lambda x: ' '.join([word.lower() for word in x.split() if word not in stopword_set]))\n",
        "\n",
        "    # keep only words\n",
        "    pat1 = r'@[A-Za-z0-9]+'\n",
        "    pat2 =r'[^a-zA-Z\\s]'\n",
        "    pat3 =r\"\\bURL\\b\"\n",
        "    combined_pat = r'|'.join((pat1, pat2,pat3))\n",
        "    regex_pat = re.compile(combined_pat, flags=re.IGNORECASE)\n",
        "    df.tweet = df.tweet.str.replace(regex_pat, '')\n",
        "\n",
        "    # join the cleaned words in a list\n",
        "    df.tweet.str.join(\"\")\n",
        "    return df\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>86426</td>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>90194</td>\n",
              "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>TIN</td>\n",
              "      <td>IND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16820</td>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>62688</td>\n",
              "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>43605</td>\n",
              "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                              tweet subtask_a  \\\n",
              "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
              "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
              "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
              "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
              "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
              "\n",
              "  subtask_b subtask_c  \n",
              "0       UNT       NaN  \n",
              "1       TIN       IND  \n",
              "2       NaN       NaN  \n",
              "3       UNT       NaN  \n",
              "4       NaN       NaN  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "z4yW6GYRDlNq",
        "colab_type": "code",
        "outputId": "5a459d40-210e-456f-ad0e-dec2c5deca47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "testA_path ='drive/My Drive/data/OffensEval_task_data/testset-taska.tsv'\n",
        "testB_path = 'drive/My Drive/data/OffensEval_task_data/testset-taskb.tsv'\n",
        "testC_path ='drive/My Drive/data/OffensEval_task_data/test_set_taskc.tsv'\n",
        "\n",
        "df_a = pd.read_csv(testA_path,sep='\\t')\n",
        "df_a = preprocess(df_a)\n",
        "df_b = pd.read_csv(testB_path,sep='\\t')\n",
        "df_b = preprocess(df_b)\n",
        "df_c = pd.read_csv(testC_path,sep='\\t')\n",
        "df_c = preprocess(df_c)\n",
        "\n",
        "x_test_a = df_a.tweet.values\n",
        "x_test_b = df_b.tweet.values\n",
        "x_test_c = df_c.tweet.values\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------Remove Stop Word--------\n",
            "-------Remove Stop Word--------\n",
            "-------Remove Stop Word--------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CHx784gegkXc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.to_csv('drive/My Drive/data/OffensEval_task_data/clean_dataset.tsv',encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XjuHofVLk99V",
        "colab_type": "code",
        "outputId": "2cf0d9c1-0bb1-4af0-9fa6-d19492048b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# split data for different subtasks\n",
        "lbl = LabelEncoder()\n",
        "y = lbl.fit_transform(df.subtask_a.values)\n",
        "x = df.tweet.values\n",
        "\n",
        "# Need data cleaning here!\n",
        "x_train_a, x_val_a, y_train_a,y_val_a =train_test_split(x,y,stratify=y,random_state=SEED,test_size=0.1,shuffle=True)\n",
        "\n",
        "\n",
        "taskb_idx= y==1\n",
        "y_b =df.subtask_b.values[taskb_idx]\n",
        "y_b = np.where(y_b=='TIN',1,0)\n",
        "x_b = x[taskb_idx]\n",
        "x_train_b, x_val_b, y_train_b,y_val_b =train_test_split(x_b,y_b,stratify=y_b,random_state=SEED,test_size=0.1,shuffle=True)\n",
        "\n",
        "\n",
        "taskc_idx= y_b==1\n",
        "y_c =df.subtask_c.values[taskb_idx][taskc_idx]\n",
        "print(y_c)\n",
        "lbl_enc = preprocessing.LabelEncoder()\n",
        "y_c = lbl_enc.fit_transform(y_c)\n",
        "x_c = x_b[taskc_idx]\n",
        "x_train_c, x_val_c, y_train_c,y_val_c =train_test_split(x_c,y_c,stratify=y_c,random_state=SEED,test_size=0.1,shuffle=True)\n",
        "# one-hot encode\n",
        "y_train_c_enc = np_utils.to_categorical(y_train_c)\n",
        "y_val_c_enc = np_utils.to_categorical(y_val_c)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['IND' 'OTH' 'GRP' ... 'GRP' 'IND' 'OTH']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LImqaPLYXKuM",
        "colab_type": "code",
        "outputId": "7037e749-1043-42f0-f15d-024bb86d0541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 0, ..., 0, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "metadata": {
        "id": "5J9P31KQnxyg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature extract"
      ]
    },
    {
      "metadata": {
        "id": "fV3K6QKOjtkb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_f=10000\n",
        "\n",
        "def tf_idf(x_train, x_val,x_test=None):\n",
        "\n",
        "    tfv = TfidfVectorizer(min_df=3, max_df=0.5, max_features=max_f,ngram_range=(1, 3),use_idf=True,smooth_idf=True)\n",
        "    tfv.fit(list(x_train) + list(x_val))\n",
        "    x_train_tfv =  tfv.transform(x_train).todense()\n",
        "    x_val_tfv = tfv.transform(x_val).todense()\n",
        "    if x_test is not None:\n",
        "        x_test_tfv = tfv.transform(x_test).todense()\n",
        "        return x_train_tfv, x_val_tfv, x_test_tfv\n",
        "    return x_train_tfv, x_val_tfv\n",
        "        \n",
        "        \n",
        "def bag_of_word(x_train,x_val):\n",
        "    ctv = CountVectorizer(min_df=3,max_df=0.5,ngram_range=(1,2))\n",
        "    ctv.fit(list(x_train) + list(x_val))\n",
        "    x_train_ctv =  ctv.transform(x_train) \n",
        "    x_val_ctv = ctv.transform(x_val)\n",
        "    return x_train_ctv, x_val_ctv\n",
        "\n",
        "def word_vectors(x):\n",
        "    N =[]\n",
        "    for s in tqdm(x):\n",
        "        M = []\n",
        "        for w in s:\n",
        "            try:\n",
        "                M.append(embeddings_index[w])\n",
        "            except:\n",
        "                continue\n",
        "        M = np.array(M)\n",
        "        v = M.sum(axis=0)\n",
        "        if type(v) != np.ndarray:\n",
        "            return np.zeros(300)\n",
        "        N.append(v / np.sqrt((v ** 2).sum()))\n",
        "    N = np.array(N)\n",
        "    return N\n",
        "\n",
        "\n",
        "def tokenize_text(train, val):\n",
        "    token = text.Tokenizer(num_words=None)\n",
        " \n",
        "    tokenized_corpus = [text.text_to_word_sequence(i) for i in train]\n",
        "    sent_lengths = [len(sent) for sent in tokenized_corpus]\n",
        "    max_len = np.max(np.array(sent_lengths))\n",
        "    token.fit_on_texts(train)\n",
        "    x_train_seq = token.texts_to_sequences(train)\n",
        "    x_val_seq = token.texts_to_sequences(val)\n",
        "\n",
        "    # zero padding\n",
        "    x_train_pad = sequence.pad_sequences(x_train_seq, maxlen=max_len)\n",
        "    x_val_pad = sequence.pad_sequences(x_val_seq, maxlen=max_len)\n",
        "\n",
        "    word_index = token.word_index\n",
        "\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "    for word, i in tqdm(word_index.items()):\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "    return x_train_pad, x_val_pad, max_len, word_index, embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kSNcNv78oB2O",
        "colab_type": "code",
        "outputId": "3f723166-2fa0-4f46-d128-2c12c558ce1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a,x_val_a)\n",
        "model = Sequential()\n",
        "model.add(Dense(1024,input_shape=(max_f,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=[f1])\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "nb_epochs = 20\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "model.fit(x_train_tfv, y_train_a, batch_size=batch_size,validation_data=(x_val_tfv, y_val_a),epochs=nb_epochs,verbose=1,callbacks=[earlystop])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/20\n",
            "11916/11916 [==============================] - 5s 450us/step - loss: 0.5933 - f1: 0.2255 - val_loss: 0.5556 - val_f1: 0.4328\n",
            "Epoch 2/20\n",
            "11916/11916 [==============================] - 4s 337us/step - loss: 0.4054 - f1: 0.7010 - val_loss: 0.5832 - val_f1: 0.5218\n",
            "Epoch 3/20\n",
            "11916/11916 [==============================] - 4s 337us/step - loss: 0.2264 - f1: 0.8672 - val_loss: 0.7491 - val_f1: 0.5448\n",
            "Epoch 4/20\n",
            "11916/11916 [==============================] - 4s 337us/step - loss: 0.1004 - f1: 0.9500 - val_loss: 1.0486 - val_f1: 0.5433\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2313ecd240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "kVTpoXGmPOw6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classical Method "
      ]
    },
    {
      "metadata": {
        "id": "s6pJveipL2nF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def logistic_regression(x_train, y_train, x_val, y_val, if_tune=False):\n",
        "    if if_tune == True:\n",
        "        param_grid, clf =logistic_tune(x_train, y_train)\n",
        "    else:\n",
        "\n",
        "        clf = LogisticRegression(C=1.0,solver='lbfgs')\n",
        "        clf.fit(x_train, y_train)\n",
        "    acc=clf.score(x_val, y_val)\n",
        "    pred = clf.predict(x_val)\n",
        "    print('Logistic regression f1 score on validation set:', f1_score(y_val,pred,average='macro'))\n",
        "    print('Logistic regression accuracy on validation set:', acc)\n",
        "    return param_grid, clf\n",
        "        \n",
        "        \n",
        "def naive_bayes(x_train, y_train, x_val, y_val, if_tune=False):\n",
        "    if if_tune == True:\n",
        "        param_grid, clf = naive_bayes_tune(x_train, y_train)\n",
        "    else:\n",
        "\n",
        "        clf = MultinomialNB()\n",
        "        clf.fit(x_train, y_train)\n",
        "    acc = clf.score(x_val, y_val)\n",
        "    pred = clf.predict(x_val)\n",
        "    print('Naive Bayes f1 score on validation set:', f1_score(y_val,pred,average='macro'))\n",
        "    print('Naive Bayes accuracy on validation set:', acc)\n",
        "    return param_grid, clf\n",
        "        \n",
        "\n",
        "def svm(x_train, y_train, x_val, y_val, if_tune=False):\n",
        "    if if_tune == True:\n",
        "        param_grid, clf = svm_tune(x_train, y_train)\n",
        "    else:\n",
        "\n",
        "        svd = decomposition.TruncatedSVD(n_components=120)\n",
        "        svd.fit(x_train)\n",
        "        x_train_svd = svd.transform(x_train)\n",
        "        x_val_svd = svd.transform(x_val)\n",
        "\n",
        "        scl = StandardScaler()\n",
        "        scl.fit(x_train_svd)\n",
        "        x_train_svd_scl = scl.transform(x_train_svd)\n",
        "        x_val_svd_scl = scl.transform(x_val_svd)\n",
        "        clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
        "        clf.fit(x_train_svd_scl, y_train)\n",
        "        \n",
        "        \n",
        "    acc = clf.score(x_val_svd_scl, y_val)\n",
        "    pred = clf.predict(x_val_svd_scl)\n",
        "    print('SVM f1 score on validation set:', f1_score(y_val,pred,average='macro'))\n",
        "    print('SVM accuracy on validation set:', acc)\n",
        "    return param_grid, clf\n",
        "\n",
        "\n",
        "def xgboost(x_train, y_train, x_val, y_val, if_tune=False):\n",
        "    if if_tune == True:\n",
        "        param_grid , clf = xgboost_tune(x_train,y_train)\n",
        "    else:\n",
        "\n",
        "        clf = XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
        "                            subsample=0.8, nthread=10, learning_rate=0.1)\n",
        "        clf.fit(x_train, y_train)\n",
        "    acc = clf.score(x_val,y_val)\n",
        "    pred = clf.predict(x_val)\n",
        "    print('Xgboost f1 score on validation set:', f1_score(y_val,pred,average='macro'))\n",
        "    print('Xgboost accuracy on validation set:', acc)\n",
        "    return param_grid, clf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uzGQtZ4C3pdR",
        "colab_type": "code",
        "outputId": "eba39ac4-7ffd-4829-ed7b-d1bb81e8bb61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "logistic_regression(x_train_tfv, y_train_a, x_val_tfv, y_val_a)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression f1 score : 0.6369537838570313\n",
            "Logistic regression : 0.7401812688821753\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "nxN0yoRCfaKc",
        "colab_type": "code",
        "outputId": "c75f6998-fbeb-4351-b3dd-ec56e71aca43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "naive_bayes(x_train_tfv, y_train_a, x_val_tfv, y_val_a)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes f1 score : 0.5902853957993772\n",
            "Naive Bayes : 0.722809667673716\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "bvAEZg50Wc1d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5f92b3cd-fd4c-45fe-bfb2-4955aeb5eaa5"
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "svm(x_train_tfv, y_train_a, x_val_tfv, y_val_a)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM f1 score : 0.5504922856388397\n",
            "SVM : 0.7077039274924471\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
              "  kernel='rbf', max_iter=-1, probability=True, random_state=None,\n",
              "  shrinking=True, tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "XUnIzATjWgTQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "553d51f6-b988-4e02-c418-6ea99f426b4a"
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "xgboost(x_train_tfv, y_train_a, x_val_tfv, y_val_a)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xgboost f1 score : 0.6554154154154155\n",
            "Xgboost : 0.7447129909365559\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Oc2t_dLKlgUu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "metadata": {
        "id": "ojNYSU85WXYV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "\n",
        "# Use pipeline for parameter tuning \n",
        "# mll_scorer = metrics.make_scorer(accuracy_score, greater_is_better=False)\n",
        "\n",
        "def svm_tune(x_train_tfv, y_train_a):\n",
        "    svd = TruncatedSVD()\n",
        "\n",
        "    # Standard Scaler\n",
        "    scl = preprocessing.StandardScaler()\n",
        "\n",
        "    svc = SVC()\n",
        "    # SVM\n",
        "    clf = pipeline.Pipeline([('svd',svd), ('scl',scl), ('svm',svc)])\n",
        "\n",
        "    param_grid = {'svd__n_components':[120,180],\n",
        "                 'svm__C':[1, 10],\n",
        "                  'svm__kernel':('linear', 'rbf'),\n",
        "                  'svm__gamma':['auto','scale']\n",
        "                 }\n",
        "    model = GridSearchCV(cv = 5, estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                         verbose=10, n_jobs=-1, iid=True, refit=True)\n",
        "    model.fit(x_train_tfv, y_train_a) \n",
        "\n",
        "    print(\"Best score while tuning: %0.3f\" % model.best_score_)\n",
        "\n",
        "    return param_grid, model\n",
        "\n",
        "\n",
        "#Logistic Regression\n",
        "def logistic_tune(x_train_tfv, y_train_a):\n",
        "    lr_model = LogisticRegression()\n",
        "\n",
        "    # pipeline \n",
        "    clf = pipeline.Pipeline([('lr', lr_model)])\n",
        "    param_grid = {\n",
        "                         'lr__C': [0.1, 1.0, 10], \n",
        "                        'lr__penalty': ['l1', 'l2']}\n",
        "\n",
        "    model = GridSearchCV(cv = 5, estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                     verbose=10, n_jobs=-1, iid=True, refit=True)\n",
        "    model.fit(x_train_tfv, y_train_a) \n",
        "\n",
        "    print(\"Best score while tuning: %0.3f\" % model.best_score_)\n",
        "    \n",
        "    return param_grid, model\n",
        "\n",
        "# Naive Bayes\n",
        "def naive_bayes_tune(x_train_tfv, y_train_a):\n",
        "    nb_model = MultinomialNB()\n",
        "\n",
        "    clf = pipeline.Pipeline([('nb', nb_model)])\n",
        "\n",
        "    param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "    # （Grid Search Model）\n",
        "    model = GridSearchCV(cv = 5,estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                     verbose=10, n_jobs=-1, iid=True, refit=True,return_train_score=True)\n",
        "\n",
        "    model.fit(x_train_tfv, y_train_a) \n",
        "    print(\"Best score while tuning: %0.3f\" % model.best_score_)\n",
        "    \n",
        "    return param_grid, model\n",
        "\n",
        "# Xgboost\n",
        "def xgboost_tune(x_train_tfv, y_train_a):\n",
        "    \n",
        "    xgb_model = XGBClassifier()\n",
        "    clf = pipeline.Pipeline([('xgb', xgb_model)])\n",
        "    param_grid = {'learning_rate' :[0.1,0.01,0.001],\n",
        "                        'n_estimators':[200,400,600,800,1000],\n",
        "                        'max_depth':[3,5,7],\n",
        "                        'min_child_weight': [1,3,5]}\n",
        "    \n",
        "    model = GridSearchCV(cv = 5, estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                     verbose=10, n_jobs=-1, iid=True, refit=True,return_train_score=True)\n",
        "\n",
        "    model.fit(x_train_tfv.tocsc(), y_train_a) \n",
        "    print(\"Best score while tuning: %0.3f\" % model.best_score_)\n",
        "    \n",
        "    return param_grid, model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i7nY6RnBR4fS",
        "colab_type": "code",
        "outputId": "f4c9fd4e-0e1f-4c7e-e2e7-cc60b711a444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "param_grid, model = logistic_regression(x_train_tfv, y_train_a, x_val_tfv, y_val_a, True)\n",
        "best_param = model.best_estimator_\n",
        "best_parameters = best_param.get_params()\n",
        "for param_name in sorted(param_grid.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    7.9s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   22.3s\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   44.6s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  2.3min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.767\n",
            "Logistic regression f1 score : 0.6567407407407407\n",
            "Logistic regression : 0.7462235649546828\n",
            "\tlr__C: 1.0\n",
            "\tlr__penalty: 'l1'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dzn_CWDJQtZv",
        "colab_type": "code",
        "outputId": "49fa2157-22e7-4af8-ad4d-4b12f98aba36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "param_grid, model = naive_bayes(x_train_tfv, y_train_a, x_val_tfv, y_val_a, True)\n",
        "best_param = model.best_estimator_\n",
        "best_parameters = best_param.get_params()\n",
        "for param_name in sorted(param_grid.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   10.5s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   34.0s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.9min\n",
            "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:  2.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score while tuning: 0.724\n",
            "Naive Bayes f1 score on validation set: 0.6399211680246182\n",
            "Naive Bayes accuracy on validation set: 0.724320241691843\n",
            "\tnb__alpha: 0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P_16z0YjR32C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1663
        },
        "outputId": "2627a355-8b95-4607-faa6-2e6ef26ea75e"
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "param_grid, model = svm(x_train_tfv, y_train_a, x_val_tfv, y_val_a, True)\n",
        "best_param = model.best_estimator_\n",
        "best_parameters = best_param.get_params()\n",
        "for param_name in sorted(param_grid.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.8min\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  3.8min\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  7.8min\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed: 31.7min\n",
            "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed: 31.7min\n",
            "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed: 31.7min\n",
            "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 31.7min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e8445572e6f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_train_tfv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val_tfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_tfv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val_tfv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbest_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbest_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-f3cbbb73cf67>\u001b[0m in \u001b[0;36msvm\u001b[0;34m(x_train, y_train, x_val, y_val, if_tune)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_tune\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_tune\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_tune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-662343e05fda>\u001b[0m in \u001b[0;36msvm_tune\u001b[0;34m(x_train_tfv, y_train_a)\u001b[0m\n\u001b[1;32m     21\u001b[0m     model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy',\n\u001b[1;32m     22\u001b[0m                                          verbose=10, n_jobs=-1, iid=True, refit=True)\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_tfv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best score while tuning: %0.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "tO4qskejxDst",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "e0f61e7f-df62-45b8-83b5-9333a54dde58"
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "param_grid, model = xgboost((x_train_tfv, y_train_a, x_val_tfv, y_val_a, True)\n",
        "best_param = model.best_estimator_\n",
        "best_parameters = best_param.get_params()\n",
        "for param_name in sorted(param_grid.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-e3f2d5f0ef18>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    best_param = model.best_estimator_\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "6ex30wVspovB",
        "colab_type": "code",
        "outputId": "7f05d82d-e651-43c4-a210-06e5d5a7eb36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "cell_type": "code",
      "source": [
        "nb_model = MultinomialNB()\n",
        "\n",
        "clf = pipeline.Pipeline([('nb', nb_model)])\n",
        "\n",
        "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# （Grid Search Model）\n",
        "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                 verbose=10, n_jobs=-1, iid=True, refit=True,return_train_score=True)\n",
        "\n",
        "model.fit(x_train_tfv, y_train_a) # 为了减少计算量，这里我们仅使用xtrain\n",
        "sc= model.score( x_val_tfv, y_val_a)\n",
        "print('valiation score: %0.3f' % sc )\n",
        "print(\"Best score: %0.3f\" % model.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters = model.best_estimator_.get_params()\n",
        "for param_name in sorted(param_grid.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   11.7s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   28.7s\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   50.1s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:  1.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valiation score: 0.724\n",
            "Best score: 0.724\n",
            "Best parameters set:\n",
            "\tnb__alpha: 0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uOXEBILyxCR5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cNTe6jjVqHkM",
        "colab_type": "code",
        "outputId": "05017ccc-b486-4ce7-dfdf-cda69f1a3f17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "cell_type": "code",
      "source": [
        "from hypopt import GridSearch\n",
        "\n",
        "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "opt = GridSearch(model= MultinomialNB(), param_grid=param_grid)\n",
        "opt.fit(x_train_tfv, y_train_a, x_val_tfv, y_val_a, scoring = 'accuracy')\n",
        "print('Test Score for Optimized Parameters:', opt.score(x_val_tfv, y_val_a))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-59deb79f1293>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhypopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'alpha'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hypopt'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HFliM2YVYDua",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Embedding"
      ]
    },
    {
      "metadata": {
        "id": "XRvvEopZhSOB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Neural Network\n"
      ]
    },
    {
      "metadata": {
        "id": "cuzX9VzC1A3E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from keras import backend as K\n",
        "\n",
        "# def f1(y_true, y_pred):\n",
        "#     def recall(y_true, y_pred):\n",
        "#         \"\"\"Recall metric.\n",
        "\n",
        "#         Only computes a batch-wise average of recall.\n",
        "\n",
        "#         Computes the recall, a metric for multi-label classification of\n",
        "#         how many relevant items are selected.\n",
        "#         \"\"\"\n",
        "#         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "#         possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "#         recall = true_positives / (possible_positives + K.epsilon())\n",
        "#         return recall\n",
        "\n",
        "#     def precision(y_true, y_pred):\n",
        "#         \"\"\"Precision metric.\n",
        "\n",
        "#         Only computes a batch-wise average of precision.\n",
        "\n",
        "#         Computes the precision, a metric for multi-label classification of\n",
        "#         how many selected items are relevant.\n",
        "#         \"\"\"\n",
        "#         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "#         predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "#         precision = true_positives / (predicted_positives + K.epsilon())\n",
        "#         return precision\n",
        "#     precision = precision(y_true, y_pred)\n",
        "#     recall = recall(y_true, y_pred)\n",
        "#     return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "# macro f1 score\n",
        "def f1(y_true, y_pred):\n",
        "    y_pred = K.round(y_pred)\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return K.mean(f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zjtIbW8h-CYU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "205002c1-dd47-4455-ce6e-5b66968dee5e"
      },
      "cell_type": "code",
      "source": [
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_a,x_val_a)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20039/20039 [00:00<00:00, 402286.21it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "BLTZ0AXKcIBB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "1a0f3c8e-7cad-4572-ada0-72415e010fa8"
      },
      "cell_type": "code",
      "source": [
        "#Fully Connected Network\n",
        "def fcn_model(num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam', metrics=[f1] ):\n",
        "    fcn_model = Sequential()\n",
        "    fcn_model.add(Dense(300, input_dim=300, activation='relu'))\n",
        "    fcn_model.add(Dropout(0.2))\n",
        "    fcn_model.add(BatchNormalization())\n",
        "\n",
        "    fcn_model.add(Dense(300, activation='relu'))\n",
        "    fcn_model.add(Dropout(0.3))\n",
        "    fcn_model.add(BatchNormalization())\n",
        "    fcn_model.add(Dense(num_classes, activation=final_activation))\n",
        "\n",
        "    fcn_model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
        "    return fcn_model\n",
        "  \n",
        "fcn_model = fcn_model()\n",
        "fcn_model.summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_34 (Dense)             (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 1)                 301       \n",
            "=================================================================\n",
            "Total params: 183,301\n",
            "Trainable params: 182,101\n",
            "Non-trainable params: 1,200\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bPOt81aCjNOP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "c8d7d014-6b47-479d-c457-0881e7ae6e86"
      },
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "def lstm_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam', metrics=[f1]):\n",
        "    lstm_model = Sequential()\n",
        "    lstm_model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix],  input_length=max_len, trainable=False))\n",
        "    lstm_model.add(SpatialDropout1D(0.3))\n",
        "    lstm_model.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.3)))\n",
        "    lstm_model.add(Dense(512, activation='relu'))\n",
        "    lstm_model.add(Dropout(0.3))\n",
        "    lstm_model.add(Dense(256, activation='relu'))\n",
        "    lstm_model.add(Dropout(0.3))\n",
        "\n",
        "    lstm_model.add(Dense(num_classes,activation=final_activation))\n",
        "    \n",
        "    lstm_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return lstm_model\n",
        "lstm_model = lstm_model(word_index = word_index, max_len = max_len, embedding_matrix = embedding_matrix)\n",
        "lstm_model.summary()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 105, 300)          6012000   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 105, 300)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 200)               320800    \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 512)               102912    \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 6,567,297\n",
            "Trainable params: 555,297\n",
            "Non-trainable params: 6,012,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NYL2KFv6mJra",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "a202d571-58e2-4068-d74c-2aaac2ef9db0"
      },
      "cell_type": "code",
      "source": [
        "# GRU Network\n",
        "def gru_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam',  metrics=[f1]):\n",
        "\n",
        "    gru_model = Sequential()\n",
        "    gru_model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix],  input_length=max_len, trainable=False))\n",
        "    gru_model.add(SpatialDropout1D(0.3))\n",
        "    gru_model.add(GRU(100, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
        "    gru_model.add(GRU(120, dropout=0.3, recurrent_dropout=0.3))\n",
        "\n",
        "    gru_model.add(Dense(512, activation='relu'))\n",
        "    gru_model.add(Dropout(0.3))\n",
        "\n",
        "    gru_model.add(Dense(256, activation='relu'))\n",
        "    gru_model.add(Dropout(0.3))\n",
        "    gru_model.add(Dense(num_classes, activation=final_activation))\n",
        "                  \n",
        "    gru_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return gru_model\n",
        "model = gru_model(word_index,max_len,embedding_matrix)\n",
        "model.summary()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 105, 300)          6012000   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_3 (Spatial (None, 105, 300)          0         \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 105, 100)          120300    \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (None, 120)               79560     \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 512)               61952     \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 6,405,397\n",
            "Trainable params: 393,397\n",
            "Non-trainable params: 6,012,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4FnHvXIz1PHB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "e010d23b-c1dd-4b4d-e77d-b04e4a9ba9b3"
      },
      "cell_type": "code",
      "source": [
        "# CNN network\n",
        "def cnn_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam',  metrics=[f1]):\n",
        "\n",
        "    cnn_model = Sequential()\n",
        "    cnn_model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],input_length=max_len, trainable=False))\n",
        "    cnn_model.add(Conv1D(256, 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(128, 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(64, 3, padding='same'))\n",
        "    cnn_model.add(Flatten())\n",
        "    cnn_model.add(Dropout(0.3))\n",
        "    cnn_model.add(BatchNormalization()) \n",
        "    cnn_model.add(Dense(256,activation='relu'))\n",
        "    cnn_model.add(Dropout(0.1))\n",
        "    cnn_model.add(Dense(num_classes, activation=final_activation))\n",
        "    \n",
        "    cnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return cnn_model\n",
        "  \n",
        "model = cnn_model(word_index,max_len,embedding_matrix)\n",
        "model.summary()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 105, 300)          6012000   \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, 105, 256)          230656    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1 (None, 35, 256)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_8 (Conv1D)            (None, 35, 128)           98432     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1 (None, 12, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_9 (Conv1D)            (None, 12, 64)            24640     \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 768)               3072      \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 256)               196864    \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 6,565,921\n",
            "Trainable params: 552,385\n",
            "Non-trainable params: 6,013,536\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VhJ3A4uU1V_m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TextCNN\n",
        "def textcnn_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam', metrics=['accuracy']):\n",
        "\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embedder = Embedding(len(word_index) + 1, 300, input_length = max_len, weights = [embedding_matrix], trainable = False)\n",
        "    embed = embedder(main_input)\n",
        "    cnn1 = Convolution1D(256, 3, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn1 = MaxPool1D(pool_size=4)(cnn1)\n",
        "    cnn2 = Convolution1D(256, 4, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn2 = MaxPool1D(pool_size=4)(cnn2)\n",
        "    cnn3 = Convolution1D(256, 5, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn3 = MaxPool1D(pool_size=4)(cnn3)\n",
        "\n",
        "    # Combine three model into one\n",
        "    cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
        "    flat = Flatten()(cnn)\n",
        "    drop = Dropout(0.2)(flat)\n",
        "    main_output = Dense(num_classes, activation=final_activation)(drop)\n",
        "    textcnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    textcnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return textcnn_model\n",
        "  \n",
        "model = textcnn_model(word_index,max_len,embedding_matrix)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vlLbPaIQ1ePw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# CNN+RNN\n",
        "def crnn_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam',  metrics=[f1]):\n",
        "\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embed = Embedding(len(word_index)+1, 300, input_length=max_len)(main_input)\n",
        "    cnn = Convolution1D(256, 3, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn = MaxPool1D(pool_size=4)(cnn)\n",
        "    cnn = Flatten()(cnn)\n",
        "    cnn = Dense(256)(cnn)\n",
        "    cnn = Dropout(0.3)(cnn)\n",
        "    rnn = Bidirectional(GRU(256, dropout=0.2, recurrent_dropout=0.1))(embed)\n",
        "    rnn = Dense(256)(rnn)\n",
        "    rnn = Dropout(0.3)(rnn)\n",
        "    con = concatenate([cnn,rnn], axis=-1)\n",
        "    main_output = Dense(num_classes, activation=final_activation)(con)\n",
        "    crnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    crnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return crnn.model\n",
        "\n",
        "model = crnn_model(word_index,max_len,embedding_matrix)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YmYuFT_31lmU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extended textCNN\n",
        "def etextcnn_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam',  metrics=['accuracy']):\n",
        "\n",
        "\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embedder = Embedding(len(word_index) + 1, 300, input_length = max_len, weights = [embedding_matrix], trainable = False)\n",
        "    embed = embedder(main_input)\n",
        "\n",
        "    # cnn1，kernel_size = 3\n",
        "    conv1_1 = Conv1D(256, 3, padding='same')(embed)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    relu1_1 = Activation('relu')(bn1_1)\n",
        "    drop1_1 = Dropout(0.3)(relu1_1)\n",
        "    conv1_2 = Conv1D(128, 3, padding='same')(drop1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    relu1_2 = Activation('relu')(bn1_2)\n",
        "    drop1_2 = Dropout(0.3)(relu1_2)\n",
        "    cnn1 = MaxPooling1D(pool_size=4)(drop1_1)\n",
        "    # cnn2，kernel_size = 4\n",
        "    conv2_1 = Conv1D(256, 4, padding='same')(embed)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    relu2_1 = Activation('relu')(bn2_1)\n",
        "    drop2_1 = Dropout(0.3)(relu2_1)\n",
        "    conv2_2 = Conv1D(128, 4, padding='same')(drop2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    relu2_2 = Activation('relu')(bn2_2)\n",
        "    drop2_2 = Dropout(0.3)(relu2_2)\n",
        "    cnn2 = MaxPooling1D(pool_size=4)(drop2_2)\n",
        "    # cnn3，kernel_size = 5\n",
        "    conv3_1 = Conv1D(256, 5, padding='same')(embed)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    relu3_1 = Activation('relu')(bn3_1)\n",
        "    drop3_1 = Dropout(0.3)(relu3_1)\n",
        "    conv3_2 = Conv1D(128, 5, padding='same')(drop3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    relu3_2 = Activation('relu')(bn3_2)\n",
        "    drop3_2 = Dropout(0.3)(relu3_2)\n",
        "    cnn3 = MaxPooling1D(pool_size=4)(drop3_2)\n",
        "\n",
        "    # Combine three block\n",
        "    cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
        "    flat = Flatten()(cnn)\n",
        "    drop = Dropout(0.5)(flat)\n",
        "    fc = Dense(512)(drop)\n",
        "    bn = BatchNormalization()(fc)\n",
        " \n",
        "    main_output = Dense(num_classes, activation=final_activation)(bn)\n",
        "    etextcnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    etextcnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return etextcnn_model\n",
        "\n",
        "model = etextcnn_model(word_index,max_len,embedding_matrix)\n",
        "model.summary()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MM0X0n192mBp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def simple_visualize(model):\n",
        "    plt.subplot(211)\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.plot(history.history[\"acc\"], color=\"g\", label=\"Train\")\n",
        "    plt.plot(history.history[\"val_acc color=\"b\", label=\"Test\")\n",
        "    plt.legend(loc=\"best\")\n",
        "\n",
        "    plt.subplot(212)\n",
        "    plt.title(\"Loss\")\n",
        "    plt.plot(result.history[\"loss\"], color=\"g\", label=\"Train\")\n",
        "    plt.plot(result.history[\"val_loss\"], color=\"b\", label=\"Test\")\n",
        "    plt.legend(loc=\"best\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OSx_rgGh9bf6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Tuning "
      ]
    },
    {
      "metadata": {
        "id": "Hc8iJGp89abo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_a,x_val_a)\n",
        "\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import Adadelta, Adam, rmsprop\n",
        "    \n",
        "space = {\n",
        "            'units1': hp.choice('units1', [16,32,64,128,256,512]),\n",
        "            'units2': hp.choice('units2', [16,32,64,128,256,512]),\n",
        "            'units3': hp.choice('units3', [16,32,64,128,256,512]),\n",
        "            'units4': hp.choice('units4', [16,32,64,128,256,512]),\n",
        "            'dropout1': hp.uniform('dropout1', .25,.75),\n",
        "            'dropout2': hp.uniform('dropout2',  .25,.75),\n",
        "            'dropout3': hp.uniform('dropout3',  .25,.75),\n",
        "            'dropout4': hp.uniform('dropout4',  .25,.75),\n",
        "\n",
        "            'batch_size' : hp.choice('batch_size', [32,64,128,256,512]),\n",
        "\n",
        "            'epochs' :  3,\n",
        "            'optimizer': hp.choice('optimizer',['sgd','adadelta','adam','rmsprop']),\n",
        "        }\n",
        "\n",
        "def gru_tune(params):   \n",
        "\n",
        "    # GRU Network\n",
        "\n",
        "    gru_model = Sequential()\n",
        "    gru_model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix],  input_length=max_len, trainable=False))\n",
        "    gru_model.add(SpatialDropout1D(0.3))\n",
        "    gru_model.add(GRU(params['units1'], dropout=params['dropout1'], recurrent_dropout=0.3, return_sequences=True))\n",
        "    gru_model.add(GRU(params['units2'], dropout=params['dropout2'], recurrent_dropout=0.3))\n",
        "\n",
        "    gru_model.add(Dense(params['units3'], activation='relu',kernel_initializer = \"glorot_uniform\"))\n",
        "    gru_model.add(Dropout(params['dropout3']))\n",
        "\n",
        "    gru_model.add(Dense(params['units4'], activation='relu',kernel_initializer = \"glorot_uniform\"))\n",
        "    gru_model.add(Dropout(params['dropout4']))\n",
        "    gru_model.add(Dense(1, activation='sigmoid'))\n",
        "                  \n",
        "    gru_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "    gru_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = gru_model.evaluate(x_val_pad, y_val_a)\n",
        "    print(loss)\n",
        "    return {'loss': loss[0], 'status': STATUS_OK}\n",
        "\n",
        "def lstm_tune(params):\n",
        "    lstm_model = Sequential()\n",
        "    lstm_model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix],  input_length=max_len, trainable=False))\n",
        "    lstm_model.add(SpatialDropout1D(0.3))\n",
        "    lstm_model.add(Bidirectional(LSTM(params['units1'], dropout=params['dropout1'], recurrent_dropout=0.3)))\n",
        "    lstm_model.add(Dense(params['units2'], activation='relu'))\n",
        "    lstm_model.add(Dropout(params['dropout2']))\n",
        "    lstm_model.add(Dense(params['units3'], activation='relu'))\n",
        "    lstm_model.add(Dropout(params['dropout3']))\n",
        "\n",
        "    lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    lstm_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "    lstm_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = lstm_model.evaluate(x_val_pad, y_val_a)\n",
        "    print('validation accuracy :', loss[1])\n",
        "    return {'loss': -loss[1], 'status': STATUS_OK}\n",
        "\n",
        "def etextcnn_tune(params):\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embedder = Embedding(len(word_index) + 1, 300, input_length = max_len, weights = [embedding_matrix], trainable = False)\n",
        "    embed = embedder(main_input)\n",
        "\n",
        "    # cnn1，kernel_size = 3\n",
        "    conv1_1 = Conv1D(params['units1'], 3, padding='same')(embed)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    relu1_1 = Activation('relu')(bn1_1)\n",
        "    drop1_1 = Dropout(params['dropout1'])(relu1_1)\n",
        "    conv1_2 = Conv1D(params['units2'], 3, padding='same')(drop1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    relu1_2 = Activation('relu')(bn1_2)\n",
        "    drop1_2 = Dropout(params['dropout2'])(relu1_2)\n",
        "    cnn1 = MaxPooling1D(pool_size=4)(drop1_1)\n",
        "    # cnn2，kernel_size = 4\n",
        "    conv2_1 = Conv1D(params['units1'], 4, padding='same')(embed)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    relu2_1 = Activation('relu')(bn2_1)\n",
        "    drop2_1 = Dropout(params['dropout1'])(relu2_1)\n",
        "    conv2_2 = Conv1D(params['units2'], 4, padding='same')(drop2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    relu2_2 = Activation('relu')(bn2_2)\n",
        "    drop2_2 = Dropout(params['dropout2'])(relu2_2)\n",
        "    cnn2 = MaxPooling1D(pool_size=4)(drop2_2)\n",
        "    # cnn3，kernel_size = 5\n",
        "    conv3_1 = Conv1D(params['units1'], 5, padding='same')(embed)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    relu3_1 = Activation('relu')(bn3_1)\n",
        "    drop3_1 = Dropout(params['dropout1'])(relu3_1)\n",
        "    conv3_2 = Conv1D(params['units2'], 5, padding='same')(drop3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    relu3_2 = Activation('relu')(bn3_2)\n",
        "    drop3_2 = Dropout(params['dropout2'])(relu3_2)\n",
        "    cnn3 = MaxPooling1D(pool_size=4)(drop3_2)\n",
        "\n",
        "    # Combine three block\n",
        "    cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
        "    flat = Flatten()(cnn)\n",
        "    drop = Dropout(params['dropout3'])(flat)\n",
        "    fc = Dense(params['units3'])(drop)\n",
        "    bn = BatchNormalization()(fc)\n",
        " \n",
        "    main_output = Dense(1, activation='sigmoid')(bn)\n",
        "    etextcnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    \n",
        "    etextcnn_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "    etextcnn_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = etextcnn_model.evaluate(x_val_pad, y_val_a)\n",
        "    print('validation accuracy :', loss[1])\n",
        "    return {'loss': loss[0], 'status': STATUS_OK}\n",
        "\n",
        "def rcnn_tune(params):\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embed = Embedding(len(word_index)+1, 300, input_length=max_len)(main_input)\n",
        "    cnn = Conv1D(params['units1'], 3, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn = MaxPooling1D(pool_size=4)(cnn)\n",
        "    cnn = Flatten()(cnn)\n",
        "    cnn = Dense(params['units2'])(cnn)\n",
        "    cnn = Dropout(params['dropout1'])(cnn)\n",
        "    rnn = Bidirectional(GRU(params['units1'], dropout=0.2, recurrent_dropout=0.1))(embed)\n",
        "    rnn = Dense(params['units3'])(rnn)\n",
        "    rnn = Dropout(params['dropout2'])(rnn)\n",
        "    con = concatenate([cnn,rnn], axis=-1)\n",
        "    main_output = Dense(1, activation='sigmoid')(con)\n",
        "    crnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    crnn_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "\n",
        "    crnn_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = crnn_model.evaluate(x_val_pad, y_val_a)\n",
        "    print('validation accuracy :', loss[1])\n",
        "    return {'loss': loss[0], 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "def cnn_tune(params):\n",
        "    cnn_model = Sequential()\n",
        "    cnn_model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],input_length=max_len, trainable=False))\n",
        "    cnn_model.add(Conv1D(params['units1'], 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(params['units2'], 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(params['units3'], 3, padding='same'))\n",
        "    cnn_model.add(Flatten())\n",
        "    cnn_model.add(Dropout(params['dropout1']))\n",
        "    cnn_model.add(BatchNormalization()) \n",
        "    cnn_model.add(Dense(params['units4'],activation='relu'))\n",
        "    cnn_model.add(Dropout(params['dropout2']))\n",
        "    cnn_model.add(Dense(1,activation ='sigmoid'))\n",
        "    \n",
        "    cnn_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "\n",
        "    cnn_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = cnn_model.evaluate(x_val_pad, y_val_a)\n",
        "    print('validation accuracy :', loss)\n",
        "    return {'loss': -loss[1], 'status': STATUS_OK}\n",
        "\n",
        "trials = Trials()\n",
        "best = fmin(lstm_tune, space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
        "print('best:', best)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "inYlGBtjQPWl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cnn_model_opti(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam', metrics=['accuracy']):\n",
        "\n",
        "\n",
        "    cnn_model = Sequential()\n",
        "    cnn_model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],input_length=max_len, trainable=False))\n",
        "    cnn_model.add(Conv1D(512, 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(16, 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(32, 3, padding='same'))\n",
        "    cnn_model.add(Flatten())\n",
        "    cnn_model.add(Dropout(0.74))\n",
        "    cnn_model.add(BatchNormalization()) \n",
        "    cnn_model.add(Dense(32,activation='relu'))\n",
        "    cnn_model.add(Dropout(0.38))\n",
        "    cnn_model.add(Dense(num_classes,activation =final_activation))\n",
        "                  \n",
        "    cnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return cnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vzt4jjeHyZRc",
        "colab_type": "code",
        "outputId": "99a74f38-d4ec-40c1-a74c-8177a8715970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        }
      },
      "cell_type": "code",
      "source": [
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "\n",
        "model = cnn_model_opti(word_index,max_len,embedding_matrix)\n",
        "model.fit(x_train_pad, y=y_train_a, batch_size=256, epochs=20, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_a), callbacks=[earlystop])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/20\n",
            "11916/11916 [==============================] - 25s 2ms/step - loss: 0.7834 - acc: 0.5719 - val_loss: 0.5851 - val_acc: 0.6994\n",
            "Epoch 2/20\n",
            "11916/11916 [==============================] - 2s 154us/step - loss: 0.6244 - acc: 0.6944 - val_loss: 0.5417 - val_acc: 0.7266\n",
            "Epoch 3/20\n",
            "11916/11916 [==============================] - 2s 148us/step - loss: 0.5444 - acc: 0.7444 - val_loss: 0.5383 - val_acc: 0.7538\n",
            "Epoch 4/20\n",
            "11916/11916 [==============================] - 2s 148us/step - loss: 0.4922 - acc: 0.7784 - val_loss: 0.5289 - val_acc: 0.7508\n",
            "Epoch 5/20\n",
            "11916/11916 [==============================] - 2s 150us/step - loss: 0.4365 - acc: 0.8054 - val_loss: 0.5765 - val_acc: 0.7508\n",
            "Epoch 6/20\n",
            "11916/11916 [==============================] - 2s 150us/step - loss: 0.3547 - acc: 0.8512 - val_loss: 0.5758 - val_acc: 0.7409\n",
            "Epoch 7/20\n",
            "11916/11916 [==============================] - 2s 151us/step - loss: 0.2767 - acc: 0.8887 - val_loss: 0.6066 - val_acc: 0.7424\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff6c658eb00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "psYSnNLz2ZVK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sub_task A"
      ]
    },
    {
      "metadata": {
        "id": "orkMGSGk6V8K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_model(model, batch_size, epochs):\n",
        "  x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_a,x_val_a)\n",
        "  earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "  model.fit(x_train_pad, y=y_train_a, batch_size=batch_size, epochs= epochs, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_a), callbacks=[earlystop])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ergaPK7-FMjd",
        "colab_type": "code",
        "outputId": "e80eacd2-e0af-4a39-eeab-711b38b94a9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1646
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_a,x_val_a)\n",
        "_,x_test_pad,_,_,_= tokenize_text(x_train_a,x_test_a)\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "\n",
        "model = gru_model(word_index,max_len,embedding_matrix)\n",
        "# model.summary()\n",
        "model.fit(x_train_pad, y=y_train_a, batch_size=512, epochs=100, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_a), callbacks=[earlystop])\n",
        "pred= model.predict(x_test_pad)\n",
        "pred=np.where(pred>=0.5,1,0).reshape(-1,)\n",
        "f1_score(np.zeros(len(x_test_pad)),pred, average='macro')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20039/20039 [00:00<00:00, 390554.44it/s]\n",
            "100%|██████████| 20039/20039 [00:00<00:00, 423481.57it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 105, 300)          6012000   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_2 (Spatial (None, 105, 300)          0         \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 105, 100)          120300    \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 120)               79560     \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 512)               61952     \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 6,405,397\n",
            "Trainable params: 393,397\n",
            "Non-trainable params: 6,012,000\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/100\n",
            "11916/11916 [==============================] - 13s 1ms/step - loss: 0.6316 - f1: 0.0333 - val_loss: 0.5965 - val_f1: 0.2378\n",
            "Epoch 2/100\n",
            "11916/11916 [==============================] - 10s 858us/step - loss: 0.5838 - f1: 0.3752 - val_loss: 0.5581 - val_f1: 0.4452\n",
            "Epoch 3/100\n",
            "11916/11916 [==============================] - 10s 855us/step - loss: 0.5520 - f1: 0.4629 - val_loss: 0.5340 - val_f1: 0.4596\n",
            "Epoch 4/100\n",
            "11916/11916 [==============================] - 10s 857us/step - loss: 0.5267 - f1: 0.5368 - val_loss: 0.5213 - val_f1: 0.5991\n",
            "Epoch 5/100\n",
            "11916/11916 [==============================] - 10s 853us/step - loss: 0.5066 - f1: 0.5892 - val_loss: 0.5218 - val_f1: 0.5904\n",
            "Epoch 6/100\n",
            "11916/11916 [==============================] - 10s 857us/step - loss: 0.4963 - f1: 0.5881 - val_loss: 0.5177 - val_f1: 0.5700\n",
            "Epoch 7/100\n",
            "11916/11916 [==============================] - 10s 877us/step - loss: 0.4931 - f1: 0.6011 - val_loss: 0.5097 - val_f1: 0.5924\n",
            "Epoch 8/100\n",
            "11776/11916 [============================>.] - ETA: 0s - loss: 0.4846 - f1: 0.6026"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-14681469786b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m model.fit(x_train_pad, y=y_train_a, batch_size=512, epochs=100, \n\u001b[0;32m----> 8\u001b[0;31m           verbose=1, validation_data=(x_val_pad, y_val_a), callbacks=[earlystop])\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "slw8LHW2F14T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model.evaluate(x_val_pad, y_val_a)\n",
        "pred= model.predict(x_test_pad)\n",
        "pred=np.where(pred>=0.5,1,0).reshape(-1,)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xKEzaUkZx-7U",
        "colab_type": "code",
        "outputId": "ca82c2c2-7415-423a-fdc7-34032ff4d1bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "f1_score(np.zeros(len(x_test_pad)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4511805998723676"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "metadata": {
        "id": "Z_cmcuiOHfTE",
        "colab_type": "code",
        "outputId": "4ac40fe4-977b-4902-deb4-8afcc7cc225b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv, x_test_tfv = tf_idf(x_train_a, x_val_a,x_test_a)\n",
        "clf =logistic_regression(x_train_tfv,y_train_a, x_val_tfv, y_val_a)\n",
        "pred= clf.predict(x_test_tfv)\n",
        "f1_score(np.zeros(len(x_test_a)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression f1 score : 0.6505225610843589\n",
            "Logistic regression : 0.7484894259818731\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46716232961586124"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "metadata": {
        "id": "yq_ibz4zNuek",
        "colab_type": "code",
        "outputId": "26c2051c-7a64-4309-bf37-2be4f7abe946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_w2v,x_val_w2v = word_count(x_train_a,x_val_a)\n",
        "svm(x_train_w2v,y_train_a, x_val_w2v, y_val_a)\n",
        "naive_bayes(x_train_w2v,y_train_a, x_val_w2v, y_val_a)\n",
        "logistic_regression(x_train_w2v,y_train_a, x_val_w2v, y_val_a)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM : 0.7061933534743202\n",
            "Naive Bayes : 0.716012084592145\n",
            "Logistic regression : 0.7409365558912386\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "metadata": {
        "id": "EhgSgj-RMTBX",
        "colab_type": "code",
        "outputId": "2744d9a1-0d10-4a34-9453-b3e4ed0cd4f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "model = etextcnn_model(word_index,max_len,embedding_matrix)\n",
        "model.fit(x_train_pad, y=y_train_a, batch_size=512, epochs=100, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_a), callbacks=[earlystop])\n",
        "pred= model.predict(x_test_pad)\n",
        "pred=np.where(pred>=0.5,1,0).reshape(-1,)\n",
        "f1_score(np.zeros(len(x_test_pad)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/100\n",
            "11916/11916 [==============================] - 14s 1ms/step - loss: 1.3878 - acc: 0.6164 - val_loss: 1.2405 - val_acc: 0.6813\n",
            "Epoch 2/100\n",
            "11916/11916 [==============================] - 5s 393us/step - loss: 0.6044 - acc: 0.7349 - val_loss: 0.6317 - val_acc: 0.7258\n",
            "Epoch 3/100\n",
            "11916/11916 [==============================] - 5s 397us/step - loss: 0.5061 - acc: 0.7708 - val_loss: 0.6321 - val_acc: 0.7258\n",
            "Epoch 4/100\n",
            "11916/11916 [==============================] - 5s 393us/step - loss: 0.4708 - acc: 0.7849 - val_loss: 0.6212 - val_acc: 0.7319\n",
            "Epoch 5/100\n",
            "11916/11916 [==============================] - 5s 394us/step - loss: 0.4382 - acc: 0.8038 - val_loss: 0.6879 - val_acc: 0.7175\n",
            "Epoch 6/100\n",
            "11916/11916 [==============================] - 5s 393us/step - loss: 0.4076 - acc: 0.8162 - val_loss: 0.7296 - val_acc: 0.7190\n",
            "Epoch 7/100\n",
            "11916/11916 [==============================] - 5s 393us/step - loss: 0.3770 - acc: 0.8374 - val_loss: 0.7877 - val_acc: 0.7069\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4837935174069628"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "metadata": {
        "id": "j_VACJnNqujs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sub_task B\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "V3qSHBfcJ-d3",
        "colab_type": "code",
        "outputId": "d683962d-6dc5-43cd-8d5e-1ebc806e9682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv, x_test_tfv = tf_idf(x_train_b, x_val_b,x_test_b)\n",
        "clf =logistic_regression(x_train_tfv,y_train_b, x_val_tfv, y_val_b)\n",
        "pred= clf.predict(x_test_tfv)\n",
        "f1_score(np.ones(len(x_test_b)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression f1 score : 0.487041520939826\n",
            "Logistic regression : 0.8818181818181818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4989561586638831"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "metadata": {
        "id": "6Cn6RqC47nHV",
        "colab_type": "code",
        "outputId": "35260cef-951b-4feb-930a-55b924a8634d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_b,x_val_b)\n",
        "_,x_test_pad,_,_,_= tokenize_text(x_train_b,x_test_b)\n",
        "model = lstm_model(word_index,max_len,embedding_matrix)\n",
        "model.fit(x_train_pad, y=y_train_b, batch_size=512, epochs=100, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_b), callbacks=[earlystop])\n",
        "pred= model.predict(x_test_pad)\n",
        "pred=np.where(pred>=0.5,1,0).reshape(-1,)\n",
        "f1_score(np.ones(len(x_test_pad)),pred, average='macro')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10007/10007 [00:00<00:00, 405648.02it/s]\n",
            "100%|██████████| 10007/10007 [00:00<00:00, 389904.13it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 3960 samples, validate on 440 samples\n",
            "Epoch 1/100\n",
            "3960/3960 [==============================] - 13s 3ms/step - loss: 0.4700 - f1: 0.8965 - val_loss: 0.4065 - val_f1: 0.9372\n",
            "Epoch 2/100\n",
            "3960/3960 [==============================] - 2s 546us/step - loss: 0.3827 - f1: 0.9365 - val_loss: 0.3654 - val_f1: 0.9372\n",
            "Epoch 3/100\n",
            "3960/3960 [==============================] - 2s 528us/step - loss: 0.3576 - f1: 0.9366 - val_loss: 0.3438 - val_f1: 0.9372\n",
            "Epoch 4/100\n",
            "3960/3960 [==============================] - 2s 527us/step - loss: 0.3485 - f1: 0.9366 - val_loss: 0.3461 - val_f1: 0.9372\n",
            "Epoch 5/100\n",
            "3960/3960 [==============================] - 2s 528us/step - loss: 0.3402 - f1: 0.9365 - val_loss: 0.3479 - val_f1: 0.9372\n",
            "Epoch 6/100\n",
            "3960/3960 [==============================] - 2s 530us/step - loss: 0.3402 - f1: 0.9366 - val_loss: 0.3466 - val_f1: 0.9372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "metadata": {
        "id": "3RyU9NJZVVIp",
        "colab_type": "code",
        "outputId": "af46adc1-5493-489d-8e8b-eef32668e0bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_b,x_val_b)\n",
        "_,x_test_pad,_,_,_= tokenize_text(x_train_b,x_test_b)\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "model = etextcnn_model(word_index,max_len,embedding_matrix)\n",
        "model.fit(x_train_pad, y=y_train_b, batch_size=512, epochs=100, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_b), callbacks=[earlystop])\n",
        "pred= model.predict(x_test_pad)\n",
        "pred=np.where(pred>=0.5,1,0).reshape(-1,)\n",
        "f1_score(np.ones(len(x_test_pad)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10007/10007 [00:00<00:00, 458915.37it/s]\n",
            "100%|██████████| 10007/10007 [00:00<00:00, 467107.37it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 3960 samples, validate on 440 samples\n",
            "Epoch 1/100\n",
            "3960/3960 [==============================] - 13s 3ms/step - loss: 2.3099 - acc: 0.5886 - val_loss: 1.4039 - val_acc: 0.4273\n",
            "Epoch 2/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 1.2392 - acc: 0.6381 - val_loss: 0.5397 - val_acc: 0.8795\n",
            "Epoch 3/100\n",
            "3960/3960 [==============================] - 1s 355us/step - loss: 0.8273 - acc: 0.7063 - val_loss: 0.5826 - val_acc: 0.7364\n",
            "Epoch 4/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 0.7141 - acc: 0.7672 - val_loss: 0.5014 - val_acc: 0.8795\n",
            "Epoch 5/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 0.6213 - acc: 0.8306 - val_loss: 0.4214 - val_acc: 0.8795\n",
            "Epoch 6/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 0.5643 - acc: 0.8510 - val_loss: 0.3715 - val_acc: 0.8795\n",
            "Epoch 7/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 0.4990 - acc: 0.8591 - val_loss: 0.3483 - val_acc: 0.8818\n",
            "Epoch 8/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 0.4405 - acc: 0.8790 - val_loss: 0.3431 - val_acc: 0.8818\n",
            "Epoch 9/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 0.3990 - acc: 0.8864 - val_loss: 0.3547 - val_acc: 0.8818\n",
            "Epoch 10/100\n",
            "3960/3960 [==============================] - 1s 353us/step - loss: 0.3614 - acc: 0.9015 - val_loss: 0.3706 - val_acc: 0.8818\n",
            "Epoch 11/100\n",
            "3960/3960 [==============================] - 1s 356us/step - loss: 0.3269 - acc: 0.9134 - val_loss: 0.3827 - val_acc: 0.8818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4989561586638831"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "metadata": {
        "id": "MCIFOHbexhuW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Subtask C"
      ]
    },
    {
      "metadata": {
        "id": "ncrukCSa_06U",
        "colab_type": "code",
        "outputId": "d150335e-9d27-4f74-872c-f9e279452f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv, x_test_tfv = tf_idf(x_train_c, x_val_c,x_test_c)\n",
        "clf =logistic_regression(x_train_tfv,y_train_c, x_val_tfv, y_val_c)\n",
        "pred= clf.predict(x_test_tfv)\n",
        "print(pred)\n",
        "f1_score(np.ones(len(x_test_c)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Logistic regression f1 score : 0.4442148960250622\n",
            "Logistic regression : 0.6881443298969072\n",
            "[0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0\n",
            " 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.42857142857142855"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "metadata": {
        "id": "sxmUgN8uUUai",
        "colab_type": "code",
        "outputId": "a5ce9436-e5be-4920-ab90-9b7145573bfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(x_train_tfv, y_train_c_enc)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-198-af48c66456df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multinomial'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_tfv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_c_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1288\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    759\u001b[0m                         dtype=None)\n\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad input shape {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: bad input shape (3488, 3)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "sfTUq1siZgkB",
        "colab_type": "code",
        "outputId": "e2143ac5-6478-48dd-e642-dbb145f76bed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_test_c[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nopasaran unity demo oppose farright london  antifa oct  enough enough '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    }
  ]
}