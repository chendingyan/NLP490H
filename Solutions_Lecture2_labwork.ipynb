{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Solutions_Lecture2_labwork.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chendingyan/NLP490H/blob/master/Solutions_Lecture2_labwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5qS8y5_Ewv6n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XG82efAzvcbK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Loading packages\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from scipy.spatial.distance import euclidean\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "abMSnMwJx8bu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Sample corpora\n",
        "\n",
        "corpus = [\n",
        "    'he is a king',\n",
        "    'she is a queen',\n",
        "    'he is a man',\n",
        "    'she is a woman',\n",
        "    'warsaw is the capital of poland',\n",
        "    'berlin is the capital of germany',\n",
        "    'paris is the capital of france',\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UQ5IL1e1GVn6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Tokenization\n",
        "\n",
        "# Q: What is a token? \n",
        "\n",
        "tokenized_corpus = [] # Let us put the tokenized corpus in a list\n",
        "for sentence in corpus:\n",
        "  tokenized_sentence = []\n",
        "  for token in sentence.split(' '): # simplest split is \n",
        "    tokenized_sentence.append(token)\n",
        "  tokenized_corpus.append(tokenized_sentence)\n",
        "\n",
        "# print(tokenized_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9bWQ3zKYKlQU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Vocabulary\n",
        "\n",
        "vocabulary = [] # Let us put all the tokens (mostly words) \n",
        "                # appearing in the vocabulary in a list\n",
        "  \n",
        "for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "\n",
        "\n",
        "# print(vocabulary)\n",
        "\n",
        "\n",
        "\n",
        "# Q. what is the size of the vocabulary?\n",
        "\n",
        "# A. \n",
        "vocabulary_size = len(vocabulary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4LWut1gtXGQN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Helper Functions\n",
        "\n",
        "# We need a mapping from word to index and index to word. \n",
        "\n",
        "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
        "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
        "\n",
        "# print(word2idx)\n",
        "# print(idx2word)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Vp8OTZI-UrYU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title A simple look-up table function\n",
        "\n",
        "# Q. why do we need this? \n",
        "\n",
        "def look_up_table(word_idx):\n",
        "    x = torch.zeros(vocabulary_size).float()\n",
        "    x[word_idx] = 1.0\n",
        "    return x\n",
        "  \n",
        "# This is a one hot representation\n",
        "\n",
        "# Q. try printing it for word_idx = 1\n",
        "\n",
        "\n",
        "# word_idx = word2idx['he']\n",
        "# print(look_up_table(word_idx))\n",
        "\n",
        "# Q. can we say anything about the word with the vector? \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wrtOwTUsyArb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Extracting contexts and the center word!\n",
        "\n",
        "\n",
        "# Let us assume the context size is '2'\n",
        "\n",
        "window_size = 2\n",
        "\n",
        "idx_pairs = []\n",
        "\n",
        "# variables of interest: \n",
        "#   center_word_pos: center word position\n",
        "#   context_word_pos: context_word_position\n",
        "#   add sentence length as a constraint\n",
        "\n",
        "for sentence in tokenized_corpus:\n",
        "    indices = [word2idx[word] for word in sentence]\n",
        "\n",
        "    for center_word_pos in range(len(indices)):\n",
        "\n",
        "        for w in range(-window_size, window_size + 1):\n",
        "            context_word_pos = center_word_pos + w\n",
        "            \n",
        "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
        "                continue\n",
        "                \n",
        "            context_word_idx = indices[context_word_pos]            \n",
        "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
        "\n",
        "# print(idx_pairs)\n",
        "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1kBhE6FeRuDu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Parameters and Hyperparameters\n",
        "\n",
        "# Hyperparameters:\n",
        "embedding_dims = 5\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "\n",
        "# The two weight matrices:\n",
        "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), \n",
        "              requires_grad=True)\n",
        "# W1 is the `embedding matrix' \n",
        "\n",
        "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), \n",
        "              requires_grad=True)\n",
        "# W2 is the `parameter matrix'\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Cx2eJi1a8R6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Training the model in the standard way\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "    loss_val = 0\n",
        "    \n",
        "    for data, target in idx_pairs:\n",
        "      \n",
        "        x = Variable(look_up_table(data)).float() # x is a variable \n",
        "        \n",
        "        # Q. what would y_true be? \n",
        "        # y_true = \n",
        "        \n",
        "        # A. \n",
        "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
        "\n",
        "        # \n",
        "        z1 = torch.matmul(W1, x) \n",
        "        # Q. what is z1? \n",
        "        \n",
        "        z2 = torch.matmul(W2, z1)\n",
        "        # Q. what is the above operation? \n",
        "        \n",
        "    \n",
        "        # Let us obtain prediction over the vocabulary\n",
        "        log_softmax = F.log_softmax(z2, dim=0)\n",
        "        \n",
        "        # Our loss is a negative log-likelihood loss \n",
        "        # (what does this mean?)\n",
        "        \n",
        "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
        "        \n",
        "        \n",
        "        loss_val += loss.item()\n",
        "        \n",
        "        # propagate the error\n",
        "        loss.backward()\n",
        "        \n",
        "        # gradient descent\n",
        "        W1.data -= learning_rate * W1.grad.data\n",
        "        W2.data -= learning_rate * W2.grad.data\n",
        "\n",
        "        # zero out gradient accumulation\n",
        "        W1.grad.data.zero_()\n",
        "        W2.grad.data.zero_()\n",
        "        \n",
        "        \n",
        "    if epoch % 10 == 0:    \n",
        "        print(f'Loss at epoch {epoch}: {loss_val/len(idx_pairs)}')        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NUPETZaOgvgB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Using embeddings\n",
        "\n",
        "# Let us get two vectors from the trained model\n",
        "\n",
        "\n",
        "x = Variable(look_up_table(1)).float()\n",
        "x_emb = torch.matmul(W1, x).detach().numpy()\n",
        "y = Variable(look_up_table(2)).float()\n",
        "y_emb = torch.matmul(W1, y).detach().numpy()\n",
        "\n",
        "\n",
        "# let us print the euclidean distance\n",
        "print(euclidean(x_emb, y_emb))\n",
        "\n",
        "# Q. What would euclidean distance do? What are we measuring in this case? (HINT: vector algebra)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lq1lcWbqRwNY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The two weight matrices:\n",
        "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), \n",
        "              requires_grad=True)\n",
        " \n",
        "\n",
        "W2 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), \n",
        "              requires_grad=True)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "    loss_val = 0\n",
        "    \n",
        "    for data, target in idx_pairs:\n",
        "        \n",
        "        x_var = Variable(look_up_table(data)).float() \n",
        "        \n",
        "        y_pos = Variable(torch.from_numpy(np.array([target])).long())\n",
        "        y_pos_var = Variable(look_up_table(target)).float()\n",
        "        \n",
        "        neg_sample = np.random.choice(list(range(vocabulary_size)),size=(1))[0]\n",
        "        y_neg = Variable(torch.from_numpy(np.array([neg_sample])))\n",
        "        y_neg_var = Variable(look_up_table(neg_sample)).float()\n",
        "\n",
        "         \n",
        "        x_emb = torch.matmul(W1, x_var) \n",
        "        y_pos_emb = torch.matmul(W2, y_pos_var)\n",
        "        y_neg_emb = torch.matmul(W2, y_neg_var)\n",
        "        \n",
        "        \n",
        "        # get positive sample score\n",
        "        pos_var = torch.mul(x_emb, y_pos_emb).squeeze()        \n",
        "        pos_score = F.logsigmoid(pos_var)       \n",
        "        pos_loss = sum(pos_score)\n",
        "        \n",
        "        # get negsample score\n",
        "        neg_var = torch.mul(x_emb, y_neg_emb).squeeze()      \n",
        "        neg_score = F.logsigmoid(-1 * neg_var)        \n",
        "        neg_loss = sum(neg_score)\n",
        "        \n",
        "        loss = -1 * sum([neg_loss + neg_loss])\n",
        "        \n",
        "        # propagate the error\n",
        "        loss.backward()\n",
        "        \n",
        "        # gradient descent\n",
        "        W1.data -= learning_rate * W1.grad.data\n",
        "        W2.data -= learning_rate * W2.grad.data\n",
        "\n",
        "        # zero out gradient accumulation\n",
        "        W1.grad.data.zero_()\n",
        "        W2.grad.data.zero_()\n",
        "        \n",
        "    if epoch % 10 == 0:    \n",
        "        print(f'Loss at epoch {epoch}: {loss/len(idx_pairs)}')\n",
        "\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1kBD-ua58B6D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# france : paris ::  rome : ?\n",
        "# britain : london :: rome : ?\n",
        "# athens : greece ::  ?  : iraq\n",
        "# unclear: clear :: ? : certain\n",
        "\n",
        "\n",
        "avec = (wvecs[w2i['unclear']] - wvecs[w2i['clear']] + wvecs[w2i['certain']])\n",
        "\n",
        "i2w[(cosine_distances(avec.reshape(-1,1).T, wvecs).argsort()[::-1])[0,0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pe__SZxi9_8R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}