{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-practice.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "Oc2t_dLKlgUu",
        "OSx_rgGh9bf6"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chendingyan/NLP490H/blob/master/NLP_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_N5pSxDNnGfF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NLP Coursework"
      ]
    },
    {
      "metadata": {
        "id": "QnmwkL__fdDG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Initialization"
      ]
    },
    {
      "metadata": {
        "id": "mxXsBVkuocdI",
        "colab_type": "code",
        "outputId": "7638bbff-97a3-494b-88e4-fb9909583fce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Etz7hcPYCCaB",
        "colab_type": "code",
        "outputId": "bd2480af-9f52-4f86-d7c3-9e7dcb2c8016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip drive/My\\ Drive/data/OffensEval_task_data/Test\\ C\\ Release.zip -d drive/My\\ Drive/data/OffensEval_task_data/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/data/OffensEval_task_data/Test C Release.zip\n",
            "  inflating: drive/My Drive/data/OffensEval_task_data/test_set_taskc.tsv  \n",
            "  inflating: drive/My Drive/data/OffensEval_task_data/readme-testsetc-v1.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JrLzlL1BCs9y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AHhTNuEWLAcz",
        "colab_type": "code",
        "outputId": "d147665b-5489-45e2-b687-26e0852d71ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm \n",
        "import codecs\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score, classification_report,f1_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D,Dropout\n",
        "from keras.layers import Dense, Embedding, Activation, merge, Input, Lambda, Reshape\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import np_utils\n",
        "from keras.layers.merge import concatenate\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "stop = stopwords.words(\"english\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "#we fix the seeds to get consistent results\n",
        "\n",
        "SEED = 234\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rpePPeOvtW46",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ]
    },
    {
      "metadata": {
        "id": "IIyUwk6-LMC4",
        "colab_type": "code",
        "outputId": "c9ef85f7-81e9-4a89-feed-6afa84d5b73f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "PATH = 'drive/My Drive/data/OffensEval_task_data/offenseval-training-v1.tsv'\n",
        "df = pd.read_csv(PATH,sep='\\t')\n",
        "\n",
        "\n",
        "print('Indexing word vectors.')\n",
        "glove_path = 'drive/My Drive/data/glove.6B.300d.txt'\n",
        "embeddings_index = {}\n",
        "f = open(glove_path, encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors.\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K69PmvHpS2xM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "metadata": {
        "id": "waDE_uxbEhXx",
        "colab_type": "code",
        "outputId": "70f57441-8c0b-406a-9997-b7df8d23e0f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "def preprocess(df):\n",
        "    print('-------Remove Stop Word--------')\n",
        "    stopword_set = set(stopwords.words(\"english\"))\n",
        "\n",
        "    # convert to lower case and split \n",
        "    df.tweet = df.tweet.apply(lambda x: ' '.join([word.lower() for word in x.split() if word not in stopword_set]))\n",
        "\n",
        "    # keep only words\n",
        "    pat1 = r'@[A-Za-z0-9]+'\n",
        "    pat2 =r'[^a-zA-Z\\s]'\n",
        "    pat3 =r\"\\bURL\\b\"\n",
        "    combined_pat = r'|'.join((pat1, pat2,pat3))\n",
        "    regex_pat = re.compile(combined_pat, flags=re.IGNORECASE)\n",
        "    df.tweet = df.tweet.str.replace(regex_pat, '')\n",
        "\n",
        "    # join the cleaned words in a list\n",
        "    df.tweet.str.join(\"\")\n",
        "    return df\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>86426</td>\n",
              "      <td>She ask native Americans take is</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>90194</td>\n",
              "      <td>Go home youre drunk  MAGA Trump</td>\n",
              "      <td>OFF</td>\n",
              "      <td>TIN</td>\n",
              "      <td>IND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16820</td>\n",
              "      <td>Amazon investigating Chinese employees selling...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>62688</td>\n",
              "      <td>Someone shouldveTaken piece shit volcano</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>43605</td>\n",
              "      <td>Obama wanted liberals amp illegals move red ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                              tweet subtask_a  \\\n",
              "0  86426                   She ask native Americans take is       OFF   \n",
              "1  90194                  Go home youre drunk  MAGA Trump         OFF   \n",
              "2  16820  Amazon investigating Chinese employees selling...       NOT   \n",
              "3  62688          Someone shouldveTaken piece shit volcano        OFF   \n",
              "4  43605    Obama wanted liberals amp illegals move red ...       NOT   \n",
              "\n",
              "  subtask_b subtask_c  \n",
              "0       UNT       NaN  \n",
              "1       TIN       IND  \n",
              "2       NaN       NaN  \n",
              "3       UNT       NaN  \n",
              "4       NaN       NaN  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "metadata": {
        "id": "z4yW6GYRDlNq",
        "colab_type": "code",
        "outputId": "ba4b7022-a921-4864-c99f-6e5d62ef4df5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "testA_path ='drive/My Drive/data/OffensEval_task_data/testset-taska.tsv'\n",
        "testB_path = 'drive/My Drive/data/OffensEval_task_data/testset-taskb.tsv'\n",
        "testC_path ='drive/My Drive/data/OffensEval_task_data/testset-taskb.tsv'\n",
        "\n",
        "df_a = pd.read_csv(testA_path,sep='\\t')\n",
        "df_a = preprocess(df_a)\n",
        "df_b = pd.read_csv(testB_path,sep='\\t')\n",
        "df_b = preprocess(df_b)\n",
        "df_c = pd.read_csv(testC_path,sep='\\t')\n",
        "df_c = preprocess(df_c)\n",
        "\n",
        "x_test_a = df_a.tweet.values\n",
        "x_test_b = df_b.tweet.values\n",
        "x_test_c = df_c.tweet.values\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------Remove Stop Word--------\n",
            "-------Remove Stop Word--------\n",
            "-------Remove Stop Word--------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CHx784gegkXc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.to_csv('drive/My Drive/data/OffensEval_task_data/clean_dataset.tsv',encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XjuHofVLk99V",
        "colab_type": "code",
        "outputId": "78706c79-5226-4934-fb6a-13a37c4b908a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# split data for different subtasks\n",
        "lbl = LabelEncoder()\n",
        "y = lbl.fit_transform(df.subtask_a.values)\n",
        "x = df.tweet.values\n",
        "\n",
        "# Need data cleaning here!\n",
        "x_train_a, x_val_a, y_train_a,y_val_a =train_test_split(x,y,stratify=y,random_state=SEED,test_size=0.1,shuffle=True)\n",
        "\n",
        "\n",
        "taskb_idx= y==1\n",
        "y_b =df.subtask_b.values[taskb_idx]\n",
        "y_b = np.where(y_b=='TIN',1,0)\n",
        "x_b = x[taskb_idx]\n",
        "x_train_b, x_val_b, y_train_b,y_val_b =train_test_split(x_b,y_b,stratify=y_b,random_state=SEED,test_size=0.1,shuffle=True)\n",
        "\n",
        "\n",
        "taskc_idx= y_b==1\n",
        "y_c =df.subtask_c.values[taskb_idx][taskc_idx]\n",
        "print(y_c)\n",
        "lbl_enc = preprocessing.LabelEncoder()\n",
        "y_c = lbl_enc.fit_transform(y_c)\n",
        "x_c = x_b[taskc_idx]\n",
        "x_train_c, x_val_c, y_train_c,y_val_c =train_test_split(x_c,y_c,stratify=y_c,random_state=SEED,test_size=0.1,shuffle=True)\n",
        "# one-hot encode\n",
        "y_train_c_enc = np_utils.to_categorical(y_train_c)\n",
        "y_val_c_enc = np_utils.to_categorical(y_val_c)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['IND' 'OTH' 'GRP' ... 'GRP' 'IND' 'OTH']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LImqaPLYXKuM",
        "colab_type": "code",
        "outputId": "7037e749-1043-42f0-f15d-024bb86d0541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 0, ..., 0, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "metadata": {
        "id": "5J9P31KQnxyg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature extract"
      ]
    },
    {
      "metadata": {
        "id": "fV3K6QKOjtkb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_f=10000\n",
        "\n",
        "def tf_idf(x_train, x_val,x_test=None):\n",
        "\n",
        "    tfv = TfidfVectorizer(min_df=3, max_df=0.5, max_features=max_f,ngram_range=(1, 3),use_idf=True,smooth_idf=True)\n",
        "    tfv.fit(list(x_train) + list(x_val))\n",
        "    x_train_tfv =  tfv.transform(x_train).todense()\n",
        "    x_val_tfv = tfv.transform(x_val).todense()\n",
        "    if x_test is not None:\n",
        "        x_test_tfv = tfv.transform(x_test).todense()\n",
        "        return x_train_tfv, x_val_tfv, x_test_tfv\n",
        "    return x_train_tfv, x_val_tfv\n",
        "        \n",
        "        \n",
        "def bag_of_word(x_train,x_val):\n",
        "    ctv = CountVectorizer(min_df=3,max_df=0.5,ngram_range=(1,2))\n",
        "    ctv.fit(list(x_train) + list(x_val))\n",
        "    x_train_ctv =  ctv.transform(x_train) \n",
        "    x_val_ctv = ctv.transform(x_val)\n",
        "    return x_train_ctv, x_val_ctv\n",
        "\n",
        "def word_vectors(x):\n",
        "    N =[]\n",
        "    for s in tqdm(x):\n",
        "        M = []\n",
        "        for w in s:\n",
        "            try:\n",
        "                M.append(embeddings_index[w])\n",
        "            except:\n",
        "                continue\n",
        "        M = np.array(M)\n",
        "        v = M.sum(axis=0)\n",
        "        if type(v) != np.ndarray:\n",
        "            return np.zeros(300)\n",
        "        N.append(v / np.sqrt((v ** 2).sum()))\n",
        "    N = np.array(N)\n",
        "    return N\n",
        "\n",
        "\n",
        "def tokenize_text(train, val):\n",
        "    token = text.Tokenizer(num_words=None)\n",
        " \n",
        "    tokenized_corpus = [text.text_to_word_sequence(i) for i in train]\n",
        "    sent_lengths = [len(sent) for sent in tokenized_corpus]\n",
        "    max_len = np.max(np.array(sent_lengths))\n",
        "    token.fit_on_texts(train)\n",
        "    x_train_seq = token.texts_to_sequences(train)\n",
        "    x_val_seq = token.texts_to_sequences(val)\n",
        "\n",
        "    # zero padding\n",
        "    x_train_pad = sequence.pad_sequences(x_train_seq, maxlen=max_len)\n",
        "    x_val_pad = sequence.pad_sequences(x_val_seq, maxlen=max_len)\n",
        "\n",
        "    word_index = token.word_index\n",
        "\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "    for word, i in tqdm(word_index.items()):\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "    return x_train_pad, x_val_pad, max_len, word_index, embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kSNcNv78oB2O",
        "colab_type": "code",
        "outputId": "3f723166-2fa0-4f46-d128-2c12c558ce1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a,x_val_a)\n",
        "model = Sequential()\n",
        "model.add(Dense(1024,input_shape=(max_f,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=[f1])\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "nb_epochs = 20\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "model.fit(x_train_tfv, y_train_a, batch_size=batch_size,validation_data=(x_val_tfv, y_val_a),epochs=nb_epochs,verbose=1,callbacks=[earlystop])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/20\n",
            "11916/11916 [==============================] - 5s 450us/step - loss: 0.5933 - f1: 0.2255 - val_loss: 0.5556 - val_f1: 0.4328\n",
            "Epoch 2/20\n",
            "11916/11916 [==============================] - 4s 337us/step - loss: 0.4054 - f1: 0.7010 - val_loss: 0.5832 - val_f1: 0.5218\n",
            "Epoch 3/20\n",
            "11916/11916 [==============================] - 4s 337us/step - loss: 0.2264 - f1: 0.8672 - val_loss: 0.7491 - val_f1: 0.5448\n",
            "Epoch 4/20\n",
            "11916/11916 [==============================] - 4s 337us/step - loss: 0.1004 - f1: 0.9500 - val_loss: 1.0486 - val_f1: 0.5433\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2313ecd240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "kVTpoXGmPOw6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classical Method "
      ]
    },
    {
      "metadata": {
        "id": "s6pJveipL2nF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def logistic_regression(x_train, y_train, x_val, y_val, if_tune=False):\n",
        "    if if_tune == True:\n",
        "        clf =logistic_tune(x_train, y_train)\n",
        "    else:\n",
        "\n",
        "        clf = LogisticRegression(C=1.0,solver='lbfgs')\n",
        "        clf.fit(x_train, y_train)\n",
        "    acc=clf.score(x_val, y_val)\n",
        "    pred = clf.predict(x_val)\n",
        "    print('Logistic regression f1 score :', f1_score(y_val,pred,average='macro'))\n",
        "    print('Logistic regression :',acc)\n",
        "    return clf\n",
        "        \n",
        "        \n",
        "def naive_bayes(x_train, y_train, x_val, y_val, if_tune=False):\n",
        "    if if_tune == True:\n",
        "        clf = naive_bayes(x_train, y_train)\n",
        "    else:\n",
        "        clf = MultinomialNB()\n",
        "        clf.fit(x_train, y_train)\n",
        "    acc = clf.score(x_val, y_val)\n",
        "    pred = clf.predict(x_val)\n",
        "    print('Naive Bayes f1 score :', f1_score(y_val,pred,average='macro'))\n",
        "    print('Naive Bayes :',acc)\n",
        "    return clf\n",
        "        \n",
        "\n",
        "def svm(x_train, y_train, x_val, y_val, if_tune=False):\n",
        "    if if_tune == True:\n",
        "        clf = svm_tune(x_train, y_train)\n",
        "    else:\n",
        "        svd = decomposition.TruncatedSVD(n_components=120)\n",
        "        svd.fit(x_train)\n",
        "        x_train_svd = svd.transform(x_train)\n",
        "        x_val_svd = svd.transform(x_val)\n",
        "\n",
        "        scl = StandardScaler()\n",
        "        scl.fit(x_train_svd)\n",
        "        x_train_svd_scl = scl.transform(x_train_svd)\n",
        "        x_val_svd_scl = scl.transform(x_val_svd)\n",
        "        clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
        "        clf.fit(x_train_svd_scl, y_train)\n",
        "    acc = clf.score(x_val, y_val)\n",
        "    pred = clf.predict(x_val)\n",
        "    print('SVM f1 score :', f1_score(y_val,pred,average='macro'))\n",
        "    print('SVM :',acc)\n",
        "    return clf\n",
        "\n",
        "\n",
        "def xgboost(x_train, y_train, x_val, y_val, if_tune=False):\n",
        "    if if_tune == True:\n",
        "        clf = xgboost_tune(x_train,y_train)\n",
        "    else:\n",
        "        clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
        "                            subsample=0.8, nthread=10, learning_rate=0.1)\n",
        "        clf.fit(x_train.tocsc(), y_train)\n",
        "    acc = clf.score(x_val.tocsc(),y_val)\n",
        "    pred = clf.predict(x_val)\n",
        "    print('Xgboost f1 score :', f1_score(y_val,pred,average='macro'))\n",
        "    print('Xgboost :',acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uzGQtZ4C3pdR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nxN0yoRCfaKc",
        "colab_type": "code",
        "outputId": "4eba3b5e-21fd-4342-a95d-c46e59801adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "naive_bayes(x_train_tfv, y_train_a, x_val_tfv, y_val_a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes f1 score : 0.38030560271646857\n",
            "Naive Bayes : 0.724320241691843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "Oc2t_dLKlgUu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "metadata": {
        "id": "ojNYSU85WXYV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "\n",
        "# Use pipeline for parameter tuning \n",
        "# mll_scorer = metrics.make_scorer(accuracy_score, greater_is_better=False)\n",
        "\n",
        "def svm_tune(x_train_tfv, y_train_a):\n",
        "    svd = TruncatedSVD()\n",
        "\n",
        "    # Standard Scaler\n",
        "    scl = preprocessing.StandardScaler()\n",
        "\n",
        "    svc = SVC()\n",
        "    # SVM\n",
        "    clf = pipeline.Pipeline([('svd',svd), ('scl',scl), ('svm',svc)])\n",
        "\n",
        "    param_grid = {'svd__n_components':[120,180],\n",
        "                 'svm__C':[1, 10],\n",
        "                  'svm__kernel':('linear', 'rbf'),\n",
        "                  'svm__gamma':['auto','scale']\n",
        "                 }\n",
        "    model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                         verbose=10, n_jobs=-1, iid=True, refit=True)\n",
        "    model.fit(x_train_tfv, y_train_a) \n",
        "\n",
        "    print(\"Best score: %0.3f\" % model.best_score_)\n",
        "\n",
        "    return model.best_estimator_\n",
        "\n",
        "\n",
        "\n",
        "#Logistic Regression\n",
        "def logistic_tune(x_train_tfv, y_train_a):\n",
        "    lr_model = LogisticRegression()\n",
        "\n",
        "    # pipeline \n",
        "    clf = pipeline.Pipeline([('lr', lr_model)])\n",
        "    param_grid = {\n",
        "                         'lr__C': [0.1, 1.0, 10], \n",
        "                        'lr__penalty': ['l1', 'l2']}\n",
        "\n",
        "    model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                     verbose=10, n_jobs=-1, iid=True, refit=True)\n",
        "    model.fit(x_train_tfv, y_train_a) \n",
        "\n",
        "    print(\"Best score: %0.3f\" % model.best_score_)\n",
        "    \n",
        "    return model.best_estimator_\n",
        "\n",
        "# Naive Bayes\n",
        "def naive_bayes_tune(x_train_tfv, y_train_a):\n",
        "    nb_model = MultinomialNB()\n",
        "\n",
        "    clf = pipeline.Pipeline([('nb', nb_model)])\n",
        "\n",
        "    param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "    # （Grid Search Model）\n",
        "    model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                     verbose=10, n_jobs=-1, iid=True, refit=True,return_train_score=True)\n",
        "\n",
        "    model.fit(x_train_tfv, y_train_a) \n",
        "    print(\"Best score: %0.3f\" % model.best_score_)\n",
        "    \n",
        "    return model.best_estimator_\n",
        "\n",
        "# Xgboost\n",
        "def xgboost_tune(x_train_tfv, y_train_a):\n",
        "    \n",
        "    xgb_model = xgb.XGBClassifier()\n",
        "    clf = pipeline.Pipeline([('xgb', xgb_model)])\n",
        "    param_grid = {'learning_rate' :[0.1,0.01,0.001],\n",
        "                        'n_estimators':[200,400,600,800,1000],\n",
        "                        'max_depth':[3,5,7],\n",
        "                        'min_child_weight': [1,3,5]}\n",
        "    \n",
        "    model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                     verbose=10, n_jobs=-1, iid=True, refit=True,return_train_score=True)\n",
        "\n",
        "    model.fit(x_train_tfv.tocsc(), y_train_a) \n",
        "    print(\"Best score: %0.3f\" % model.best_score_)\n",
        "    \n",
        "    return model.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ex30wVspovB",
        "colab_type": "code",
        "outputId": "58b7762d-6a54-4468-ab44-5362e2e7cd01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "cell_type": "code",
      "source": [
        "nb_model = MultinomialNB()\n",
        "\n",
        "clf = pipeline.Pipeline([('nb', nb_model)])\n",
        "\n",
        "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# （Grid Search Model）\n",
        "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                 verbose=10, n_jobs=-1, iid=True, refit=True,return_train_score=True)\n",
        "\n",
        "model.fit(x_train_tfv, y_train_a) # 为了减少计算量，这里我们仅使用xtrain\n",
        "sc= model.score( x_val_tfv, y_val_a)\n",
        "print('valiation score: %0.3f' % sc )\n",
        "print(\"Best score: %0.3f\" % model.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters = model.best_estimator_.get_params()\n",
        "for param_name in sorted(param_grid.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valiation score: 0.724\n",
            "Best score: 0.720\n",
            "Best parameters set:\n",
            "\tnb__alpha: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1873s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  18 | elapsed:    1.3s remaining:    0.3s\n",
            "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    1.4s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "cNTe6jjVqHkM",
        "colab_type": "code",
        "outputId": "e1bdfdf1-2ff1-42bc-f8c7-869d6ff3e8d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        }
      },
      "cell_type": "code",
      "source": [
        "from hypopt import GridSearch\n",
        "\n",
        "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "opt = GridSearch(model= MultinomialNB(), param_grid=param_grid)\n",
        "opt.fit(x_train_tfv, y_train_a, x_val_tfv, y_val_a, scoring = 'accuracy')\n",
        "print('Test Score for Optimized Parameters:', opt.score(x_val_tfv, y_val_a))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Score for Optimized Parameters: 0.724320241691843\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HFliM2YVYDua",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Embedding"
      ]
    },
    {
      "metadata": {
        "id": "XRvvEopZhSOB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Deep Neural Network\n"
      ]
    },
    {
      "metadata": {
        "id": "cuzX9VzC1A3E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from keras import backend as K\n",
        "\n",
        "# def f1(y_true, y_pred):\n",
        "#     def recall(y_true, y_pred):\n",
        "#         \"\"\"Recall metric.\n",
        "\n",
        "#         Only computes a batch-wise average of recall.\n",
        "\n",
        "#         Computes the recall, a metric for multi-label classification of\n",
        "#         how many relevant items are selected.\n",
        "#         \"\"\"\n",
        "#         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "#         possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "#         recall = true_positives / (possible_positives + K.epsilon())\n",
        "#         return recall\n",
        "\n",
        "#     def precision(y_true, y_pred):\n",
        "#         \"\"\"Precision metric.\n",
        "\n",
        "#         Only computes a batch-wise average of precision.\n",
        "\n",
        "#         Computes the precision, a metric for multi-label classification of\n",
        "#         how many selected items are relevant.\n",
        "#         \"\"\"\n",
        "#         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "#         predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "#         precision = true_positives / (predicted_positives + K.epsilon())\n",
        "#         return precision\n",
        "#     precision = precision(y_true, y_pred)\n",
        "#     recall = recall(y_true, y_pred)\n",
        "#     return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "# macro f1 score\n",
        "def f1(y_true, y_pred):\n",
        "    y_pred = K.round(y_pred)\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return K.mean(f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BLTZ0AXKcIBB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Fully Connected Network\n",
        "def fcn_model(num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam', metrics=[f1] ):\n",
        "    fcn_model = Sequential()\n",
        "    fcn_model.add(Dense(300, input_dim=300, activation='relu'))\n",
        "    fcn_model.add(Dropout(0.2))\n",
        "    fcn_model.add(BatchNormalization())\n",
        "\n",
        "    fcn_model.add(Dense(300, activation='relu'))\n",
        "    fcn_model.add(Dropout(0.3))\n",
        "    fcn_model.add(BatchNormalization())\n",
        "    fcn_model.add(Dense(num_classes, activation=final_activation))\n",
        "\n",
        "    fcn_model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
        "    return fcn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bPOt81aCjNOP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "def lstm_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam', metrics=[f1]):\n",
        "    lstm_model = Sequential()\n",
        "    lstm_model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix],  input_length=max_len, trainable=False))\n",
        "    lstm_model.add(SpatialDropout1D(0.3))\n",
        "    lstm_model.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.3)))\n",
        "    lstm_model.add(Dense(512, activation='relu'))\n",
        "    lstm_model.add(Dropout(0.3))\n",
        "    lstm_model.add(Dense(256, activation='relu'))\n",
        "    lstm_model.add(Dropout(0.3))\n",
        "\n",
        "    lstm_model.add(Dense(num_classes,activation=final_activation))\n",
        "    \n",
        "    lstm_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return lstm_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NYL2KFv6mJra",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# GRU Network\n",
        "def gru_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam',  metrics=[f1]):\n",
        "\n",
        "    gru_model = Sequential()\n",
        "    gru_model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix],  input_length=max_len, trainable=False))\n",
        "    gru_model.add(SpatialDropout1D(0.3))\n",
        "    gru_model.add(GRU(100, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
        "    gru_model.add(GRU(120, dropout=0.3, recurrent_dropout=0.3))\n",
        "\n",
        "    gru_model.add(Dense(512, activation='relu'))\n",
        "    gru_model.add(Dropout(0.3))\n",
        "\n",
        "    gru_model.add(Dense(256, activation='relu'))\n",
        "    gru_model.add(Dropout(0.3))\n",
        "    gru_model.add(Dense(num_classes, activation=final_activation))\n",
        "                  \n",
        "    gru_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return gru_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4FnHvXIz1PHB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# CNN network\n",
        "def cnn_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam',  metrics=[f1]):\n",
        "\n",
        "\n",
        "    cnn_model = Sequential()\n",
        "    cnn_model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],input_length=max_len, trainable=False))\n",
        "    cnn_model.add(Convolution1D(256, 3, padding='same'))\n",
        "    cnn_model.add(MaxPool1D(3,3,padding='same'))\n",
        "    cnn_model.add(Convolution1D(128, 3, padding='same'))\n",
        "    cnn_model.add(MaxPool1D(3,3,padding='same'))\n",
        "    cnn_model.add(Convolution1D(64, 3, padding='same'))\n",
        "    cnn_model.add(Flatten())\n",
        "    cnn_model.add(Dropout(0.3))\n",
        "    cnn_model.add(BatchNormalization()) \n",
        "    cnn_model.add(Dense(256,activation='relu'))\n",
        "    cnn_model.add(Dropout(0.1))\n",
        "    cnn_model.add(Dense(num_classes),activation =final_activation)\n",
        "                  \n",
        "    cnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return cnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VhJ3A4uU1V_m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TextCNN\n",
        "def textcnn_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam', metrics=['accuracy']):\n",
        "\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embedder = Embedding(len(word_index) + 1, 300, input_length = max_len, weights = [embedding_matrix], trainable = False)\n",
        "    embed = embedder(main_input)\n",
        "    cnn1 = Convolution1D(256, 3, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn1 = MaxPool1D(pool_size=4)(cnn1)\n",
        "    cnn2 = Convolution1D(256, 4, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn2 = MaxPool1D(pool_size=4)(cnn2)\n",
        "    cnn3 = Convolution1D(256, 5, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn3 = MaxPool1D(pool_size=4)(cnn3)\n",
        "\n",
        "    # Combine three model into one\n",
        "    cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
        "    flat = Flatten()(cnn)\n",
        "    drop = Dropout(0.2)(flat)\n",
        "    main_output = Dense(num_classes, activation=final_activation)(drop)\n",
        "    textcnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    textcnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return textcnn_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vlLbPaIQ1ePw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# CNN+RNN\n",
        "def crnn_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam',  metrics=[f1]):\n",
        "\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embed = Embedding(len(word_index)+1, 300, input_length=max_len)(main_input)\n",
        "    cnn = Convolution1D(256, 3, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn = MaxPool1D(pool_size=4)(cnn)\n",
        "    cnn = Flatten()(cnn)\n",
        "    cnn = Dense(256)(cnn)\n",
        "    cnn = Dropout(0.3)(cnn)\n",
        "    rnn = Bidirectional(GRU(256, dropout=0.2, recurrent_dropout=0.1))(embed)\n",
        "    rnn = Dense(256)(rnn)\n",
        "    rnn = Dropout(0.3)(rnn)\n",
        "    con = concatenate([cnn,rnn], axis=-1)\n",
        "    main_output = Dense(num_classes, activation=final_activation)(con)\n",
        "    crnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    crnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return crnn.model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YmYuFT_31lmU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extended textCNN\n",
        "def etextcnn_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam',  metrics=['accuracy']):\n",
        "\n",
        "\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embedder = Embedding(len(word_index) + 1, 300, input_length = max_len, weights = [embedding_matrix], trainable = False)\n",
        "    embed = embedder(main_input)\n",
        "\n",
        "    # cnn1，kernel_size = 3\n",
        "    conv1_1 = Conv1D(256, 3, padding='same')(embed)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    relu1_1 = Activation('relu')(bn1_1)\n",
        "    drop1_1 = Dropout(0.3)(relu1_1)\n",
        "    conv1_2 = Conv1D(128, 3, padding='same')(drop1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    relu1_2 = Activation('relu')(bn1_2)\n",
        "    drop1_2 = Dropout(0.3)(relu1_2)\n",
        "    cnn1 = MaxPooling1D(pool_size=4)(drop1_1)\n",
        "    # cnn2，kernel_size = 4\n",
        "    conv2_1 = Conv1D(256, 4, padding='same')(embed)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    relu2_1 = Activation('relu')(bn2_1)\n",
        "    drop2_1 = Dropout(0.3)(relu2_1)\n",
        "    conv2_2 = Conv1D(128, 4, padding='same')(drop2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    relu2_2 = Activation('relu')(bn2_2)\n",
        "    drop2_2 = Dropout(0.3)(relu2_2)\n",
        "    cnn2 = MaxPooling1D(pool_size=4)(drop2_2)\n",
        "    # cnn3，kernel_size = 5\n",
        "    conv3_1 = Conv1D(256, 5, padding='same')(embed)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    relu3_1 = Activation('relu')(bn3_1)\n",
        "    drop3_1 = Dropout(0.3)(relu3_1)\n",
        "    conv3_2 = Conv1D(128, 5, padding='same')(drop3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    relu3_2 = Activation('relu')(bn3_2)\n",
        "    drop3_2 = Dropout(0.3)(relu3_2)\n",
        "    cnn3 = MaxPooling1D(pool_size=4)(drop3_2)\n",
        "\n",
        "    # Combine three block\n",
        "    cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
        "    flat = Flatten()(cnn)\n",
        "    drop = Dropout(0.5)(flat)\n",
        "    fc = Dense(512)(drop)\n",
        "    bn = BatchNormalization()(fc)\n",
        " \n",
        "    main_output = Dense(num_classes, activation=final_activation)(bn)\n",
        "    etextcnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    etextcnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return etextcnn_model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MM0X0n192mBp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def simple_visualize(model):\n",
        "    plt.subplot(211)\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.plot(history.history[\"acc\"], color=\"g\", label=\"Train\")\n",
        "    plt.plot(history.history[\"val_acc color=\"b\", label=\"Test\")\n",
        "    plt.legend(loc=\"best\")\n",
        "\n",
        "    plt.subplot(212)\n",
        "    plt.title(\"Loss\")\n",
        "    plt.plot(result.history[\"loss\"], color=\"g\", label=\"Train\")\n",
        "    plt.plot(result.history[\"val_loss\"], color=\"b\", label=\"Test\")\n",
        "    plt.legend(loc=\"best\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OSx_rgGh9bf6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Tuning "
      ]
    },
    {
      "metadata": {
        "id": "Hc8iJGp89abo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_a,x_val_a)\n",
        "\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import Adadelta, Adam, rmsprop\n",
        "    \n",
        "space = {\n",
        "            'units1': hp.choice('units1', [16,32,64,128,256,512]),\n",
        "            'units2': hp.choice('units2', [16,32,64,128,256,512]),\n",
        "            'units3': hp.choice('units3', [16,32,64,128,256,512]),\n",
        "            'units4': hp.choice('units4', [16,32,64,128,256,512]),\n",
        "            'dropout1': hp.uniform('dropout1', .25,.75),\n",
        "            'dropout2': hp.uniform('dropout2',  .25,.75),\n",
        "            'dropout3': hp.uniform('dropout3',  .25,.75),\n",
        "            'dropout4': hp.uniform('dropout4',  .25,.75),\n",
        "\n",
        "            'batch_size' : hp.choice('batch_size', [32,64,128,256,512]),\n",
        "\n",
        "            'epochs' :  3,\n",
        "            'optimizer': hp.choice('optimizer',['sgd','adadelta','adam','rmsprop']),\n",
        "        }\n",
        "\n",
        "def gru_tune(params):   \n",
        "\n",
        "    # GRU Network\n",
        "\n",
        "    gru_model = Sequential()\n",
        "    gru_model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix],  input_length=max_len, trainable=False))\n",
        "    gru_model.add(SpatialDropout1D(0.3))\n",
        "    gru_model.add(GRU(params['units1'], dropout=params['dropout1'], recurrent_dropout=0.3, return_sequences=True))\n",
        "    gru_model.add(GRU(params['units2'], dropout=params['dropout2'], recurrent_dropout=0.3))\n",
        "\n",
        "    gru_model.add(Dense(params['units3'], activation='relu',kernel_initializer = \"glorot_uniform\"))\n",
        "    gru_model.add(Dropout(params['dropout3']))\n",
        "\n",
        "    gru_model.add(Dense(params['units4'], activation='relu',kernel_initializer = \"glorot_uniform\"))\n",
        "    gru_model.add(Dropout(params['dropout4']))\n",
        "    gru_model.add(Dense(1, activation='sigmoid'))\n",
        "                  \n",
        "    gru_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "    gru_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = gru_model.evaluate(x_val_pad, y_val_a)\n",
        "    print(loss)\n",
        "    return {'loss': loss[0], 'status': STATUS_OK}\n",
        "\n",
        "def lstm_tune(params):\n",
        "    lstm_model = Sequential()\n",
        "    lstm_model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix],  input_length=max_len, trainable=False))\n",
        "    lstm_model.add(SpatialDropout1D(0.3))\n",
        "    lstm_model.add(Bidirectional(LSTM(params['units1'], dropout=params['dropout1'], recurrent_dropout=0.3)))\n",
        "    lstm_model.add(Dense(params['units2'], activation='relu'))\n",
        "    lstm_model.add(Dropout(params['dropout2']))\n",
        "    lstm_model.add(Dense(params['units3'], activation='relu'))\n",
        "    lstm_model.add(Dropout(params['dropout3']))\n",
        "\n",
        "    lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    lstm_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "    lstm_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = lstm_model.evaluate(x_val_pad, y_val_a)\n",
        "    print('validation accuracy :', loss[1])\n",
        "    return {'loss': -loss[1], 'status': STATUS_OK}\n",
        "\n",
        "def etextcnn_tune(params):\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embedder = Embedding(len(word_index) + 1, 300, input_length = max_len, weights = [embedding_matrix], trainable = False)\n",
        "    embed = embedder(main_input)\n",
        "\n",
        "    # cnn1，kernel_size = 3\n",
        "    conv1_1 = Conv1D(params['units1'], 3, padding='same')(embed)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    relu1_1 = Activation('relu')(bn1_1)\n",
        "    drop1_1 = Dropout(params['dropout1'])(relu1_1)\n",
        "    conv1_2 = Conv1D(params['units2'], 3, padding='same')(drop1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    relu1_2 = Activation('relu')(bn1_2)\n",
        "    drop1_2 = Dropout(params['dropout2'])(relu1_2)\n",
        "    cnn1 = MaxPooling1D(pool_size=4)(drop1_1)\n",
        "    # cnn2，kernel_size = 4\n",
        "    conv2_1 = Conv1D(params['units1'], 4, padding='same')(embed)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    relu2_1 = Activation('relu')(bn2_1)\n",
        "    drop2_1 = Dropout(params['dropout1'])(relu2_1)\n",
        "    conv2_2 = Conv1D(params['units2'], 4, padding='same')(drop2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    relu2_2 = Activation('relu')(bn2_2)\n",
        "    drop2_2 = Dropout(params['dropout2'])(relu2_2)\n",
        "    cnn2 = MaxPooling1D(pool_size=4)(drop2_2)\n",
        "    # cnn3，kernel_size = 5\n",
        "    conv3_1 = Conv1D(params['units1'], 5, padding='same')(embed)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    relu3_1 = Activation('relu')(bn3_1)\n",
        "    drop3_1 = Dropout(params['dropout1'])(relu3_1)\n",
        "    conv3_2 = Conv1D(params['units2'], 5, padding='same')(drop3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    relu3_2 = Activation('relu')(bn3_2)\n",
        "    drop3_2 = Dropout(params['dropout2'])(relu3_2)\n",
        "    cnn3 = MaxPooling1D(pool_size=4)(drop3_2)\n",
        "\n",
        "    # Combine three block\n",
        "    cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
        "    flat = Flatten()(cnn)\n",
        "    drop = Dropout(params['dropout3'])(flat)\n",
        "    fc = Dense(params['units3'])(drop)\n",
        "    bn = BatchNormalization()(fc)\n",
        " \n",
        "    main_output = Dense(1, activation='sigmoid')(bn)\n",
        "    etextcnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    \n",
        "    etextcnn_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "    etextcnn_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = etextcnn_model.evaluate(x_val_pad, y_val_a)\n",
        "    print('validation accuracy :', loss[1])\n",
        "    return {'loss': loss[0], 'status': STATUS_OK}\n",
        "\n",
        "def rcnn_tune(params):\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embed = Embedding(len(word_index)+1, 300, input_length=max_len)(main_input)\n",
        "    cnn = Conv1D(params['units1'], 3, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn = MaxPooling1D(pool_size=4)(cnn)\n",
        "    cnn = Flatten()(cnn)\n",
        "    cnn = Dense(params['units2'])(cnn)\n",
        "    cnn = Dropout(params['dropout1'])(cnn)\n",
        "    rnn = Bidirectional(GRU(params['units1'], dropout=0.2, recurrent_dropout=0.1))(embed)\n",
        "    rnn = Dense(params['units3'])(rnn)\n",
        "    rnn = Dropout(params['dropout2'])(rnn)\n",
        "    con = concatenate([cnn,rnn], axis=-1)\n",
        "    main_output = Dense(1, activation='sigmoid')(con)\n",
        "    crnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    crnn_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "\n",
        "    crnn_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = crnn_model.evaluate(x_val_pad, y_val_a)\n",
        "    print('validation accuracy :', loss[1])\n",
        "    return {'loss': loss[0], 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "def cnn_tune(params):\n",
        "    cnn_model = Sequential()\n",
        "    cnn_model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],input_length=max_len, trainable=False))\n",
        "    cnn_model.add(Conv1D(params['units1'], 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(params['units2'], 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(params['units3'], 3, padding='same'))\n",
        "    cnn_model.add(Flatten())\n",
        "    cnn_model.add(Dropout(params['dropout1']))\n",
        "    cnn_model.add(BatchNormalization()) \n",
        "    cnn_model.add(Dense(params['units4'],activation='relu'))\n",
        "    cnn_model.add(Dropout(params['dropout2']))\n",
        "    cnn_model.add(Dense(1,activation ='sigmoid'))\n",
        "    \n",
        "    cnn_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "\n",
        "    cnn_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = cnn_model.evaluate(x_val_pad, y_val_a)\n",
        "    print('validation accuracy :', loss)\n",
        "    return {'loss': -loss[1], 'status': STATUS_OK}\n",
        "\n",
        "trials = Trials()\n",
        "best = fmin(lstm_tune, space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
        "print('best:', best)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "inYlGBtjQPWl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cnn_model_opti(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam', metrics=['accuracy']):\n",
        "\n",
        "\n",
        "    cnn_model = Sequential()\n",
        "    cnn_model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],input_length=max_len, trainable=False))\n",
        "    cnn_model.add(Conv1D(512, 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(16, 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(32, 3, padding='same'))\n",
        "    cnn_model.add(Flatten())\n",
        "    cnn_model.add(Dropout(0.74))\n",
        "    cnn_model.add(BatchNormalization()) \n",
        "    cnn_model.add(Dense(32,activation='relu'))\n",
        "    cnn_model.add(Dropout(0.38))\n",
        "    cnn_model.add(Dense(num_classes,activation =final_activation))\n",
        "                  \n",
        "    cnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return cnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vzt4jjeHyZRc",
        "colab_type": "code",
        "outputId": "99a74f38-d4ec-40c1-a74c-8177a8715970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        }
      },
      "cell_type": "code",
      "source": [
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "\n",
        "model = cnn_model_opti(word_index,max_len,embedding_matrix)\n",
        "model.fit(x_train_pad, y=y_train_a, batch_size=256, epochs=20, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_a), callbacks=[earlystop])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/20\n",
            "11916/11916 [==============================] - 25s 2ms/step - loss: 0.7834 - acc: 0.5719 - val_loss: 0.5851 - val_acc: 0.6994\n",
            "Epoch 2/20\n",
            "11916/11916 [==============================] - 2s 154us/step - loss: 0.6244 - acc: 0.6944 - val_loss: 0.5417 - val_acc: 0.7266\n",
            "Epoch 3/20\n",
            "11916/11916 [==============================] - 2s 148us/step - loss: 0.5444 - acc: 0.7444 - val_loss: 0.5383 - val_acc: 0.7538\n",
            "Epoch 4/20\n",
            "11916/11916 [==============================] - 2s 148us/step - loss: 0.4922 - acc: 0.7784 - val_loss: 0.5289 - val_acc: 0.7508\n",
            "Epoch 5/20\n",
            "11916/11916 [==============================] - 2s 150us/step - loss: 0.4365 - acc: 0.8054 - val_loss: 0.5765 - val_acc: 0.7508\n",
            "Epoch 6/20\n",
            "11916/11916 [==============================] - 2s 150us/step - loss: 0.3547 - acc: 0.8512 - val_loss: 0.5758 - val_acc: 0.7409\n",
            "Epoch 7/20\n",
            "11916/11916 [==============================] - 2s 151us/step - loss: 0.2767 - acc: 0.8887 - val_loss: 0.6066 - val_acc: 0.7424\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff6c658eb00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "psYSnNLz2ZVK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sub_task A"
      ]
    },
    {
      "metadata": {
        "id": "ergaPK7-FMjd",
        "colab_type": "code",
        "outputId": "13ae23e1-ce10-4d6f-92f9-d2e68bf98cf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_a,x_val_a)\n",
        "_,x_test_pad,_,_,_= tokenize_text(x_train_a,x_test_a)\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "\n",
        "model = gru_model(word_index,max_len,embedding_matrix)\n",
        "model.fit(x_train_pad, y=y_train_a, batch_size=512, epochs=100, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_a), callbacks=[earlystop])\n",
        "pred= model.predict(x_test_pad)\n",
        "pred=np.where(pred>=0.5,1,0).reshape(-1,)\n",
        "f1_score(np.zeros(len(x_test_pad)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19037/19037 [00:00<00:00, 405224.04it/s]\n",
            "100%|██████████| 19037/19037 [00:00<00:00, 433359.74it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/100\n",
            "11916/11916 [==============================] - 13s 1ms/step - loss: 0.6251 - f1: 0.0827 - val_loss: 0.5721 - val_f1: 0.4704\n",
            "Epoch 2/100\n",
            "11916/11916 [==============================] - 6s 469us/step - loss: 0.5620 - f1: 0.4735 - val_loss: 0.5589 - val_f1: 0.5323\n",
            "Epoch 3/100\n",
            "11916/11916 [==============================] - 6s 470us/step - loss: 0.5348 - f1: 0.5066 - val_loss: 0.5239 - val_f1: 0.5592\n",
            "Epoch 4/100\n",
            "11916/11916 [==============================] - 6s 466us/step - loss: 0.5164 - f1: 0.5682 - val_loss: 0.5139 - val_f1: 0.5810\n",
            "Epoch 5/100\n",
            "11916/11916 [==============================] - 6s 470us/step - loss: 0.5068 - f1: 0.5754 - val_loss: 0.5102 - val_f1: 0.5762\n",
            "Epoch 6/100\n",
            "11916/11916 [==============================] - 6s 472us/step - loss: 0.4916 - f1: 0.5935 - val_loss: 0.5091 - val_f1: 0.5851\n",
            "Epoch 7/100\n",
            "11916/11916 [==============================] - 6s 471us/step - loss: 0.4898 - f1: 0.5893 - val_loss: 0.5065 - val_f1: 0.5825\n",
            "Epoch 8/100\n",
            "11916/11916 [==============================] - 6s 472us/step - loss: 0.4831 - f1: 0.6126 - val_loss: 0.5086 - val_f1: 0.5997\n",
            "Epoch 9/100\n",
            "11916/11916 [==============================] - 6s 469us/step - loss: 0.4802 - f1: 0.6198 - val_loss: 0.5035 - val_f1: 0.5965\n",
            "Epoch 10/100\n",
            "11916/11916 [==============================] - 6s 467us/step - loss: 0.4771 - f1: 0.6156 - val_loss: 0.5106 - val_f1: 0.5368\n",
            "Epoch 11/100\n",
            "11916/11916 [==============================] - 6s 476us/step - loss: 0.4770 - f1: 0.6130 - val_loss: 0.4990 - val_f1: 0.5873\n",
            "Epoch 12/100\n",
            "11916/11916 [==============================] - 6s 473us/step - loss: 0.4689 - f1: 0.6199 - val_loss: 0.5027 - val_f1: 0.5816\n",
            "Epoch 13/100\n",
            "11916/11916 [==============================] - 6s 469us/step - loss: 0.4678 - f1: 0.6203 - val_loss: 0.5034 - val_f1: 0.6010\n",
            "Epoch 14/100\n",
            "11916/11916 [==============================] - 6s 471us/step - loss: 0.4693 - f1: 0.6198 - val_loss: 0.5028 - val_f1: 0.5509\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f22efbab7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "metadata": {
        "id": "slw8LHW2F14T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model.evaluate(x_val_pad, y_val_a)\n",
        "pred= model.predict(x_test_pad)\n",
        "pred=np.where(pred>=0.5,1,0).reshape(-1,)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xKEzaUkZx-7U",
        "colab_type": "code",
        "outputId": "ca82c2c2-7415-423a-fdc7-34032ff4d1bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "f1_score(np.zeros(len(x_test_pad)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4511805998723676"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "metadata": {
        "id": "Z_cmcuiOHfTE",
        "colab_type": "code",
        "outputId": "4ac40fe4-977b-4902-deb4-8afcc7cc225b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv, x_test_tfv = tf_idf(x_train_a, x_val_a,x_test_a)\n",
        "clf =logistic_regression(x_train_tfv,y_train_a, x_val_tfv, y_val_a)\n",
        "pred= clf.predict(x_test_tfv)\n",
        "f1_score(np.zeros(len(x_test_a)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression f1 score : 0.6505225610843589\n",
            "Logistic regression : 0.7484894259818731\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46716232961586124"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "metadata": {
        "id": "yq_ibz4zNuek",
        "colab_type": "code",
        "outputId": "26c2051c-7a64-4309-bf37-2be4f7abe946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_w2v,x_val_w2v = word_count(x_train_a,x_val_a)\n",
        "svm(x_train_w2v,y_train_a, x_val_w2v, y_val_a)\n",
        "naive_bayes(x_train_w2v,y_train_a, x_val_w2v, y_val_a)\n",
        "logistic_regression(x_train_w2v,y_train_a, x_val_w2v, y_val_a)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM : 0.7061933534743202\n",
            "Naive Bayes : 0.716012084592145\n",
            "Logistic regression : 0.7409365558912386\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "metadata": {
        "id": "EhgSgj-RMTBX",
        "colab_type": "code",
        "outputId": "2744d9a1-0d10-4a34-9453-b3e4ed0cd4f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "model = etextcnn_model(word_index,max_len,embedding_matrix)\n",
        "model.fit(x_train_pad, y=y_train_a, batch_size=512, epochs=100, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_a), callbacks=[earlystop])\n",
        "pred= model.predict(x_test_pad)\n",
        "pred=np.where(pred>=0.5,1,0).reshape(-1,)\n",
        "f1_score(np.zeros(len(x_test_pad)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/100\n",
            "11916/11916 [==============================] - 14s 1ms/step - loss: 1.3878 - acc: 0.6164 - val_loss: 1.2405 - val_acc: 0.6813\n",
            "Epoch 2/100\n",
            "11916/11916 [==============================] - 5s 393us/step - loss: 0.6044 - acc: 0.7349 - val_loss: 0.6317 - val_acc: 0.7258\n",
            "Epoch 3/100\n",
            "11916/11916 [==============================] - 5s 397us/step - loss: 0.5061 - acc: 0.7708 - val_loss: 0.6321 - val_acc: 0.7258\n",
            "Epoch 4/100\n",
            "11916/11916 [==============================] - 5s 393us/step - loss: 0.4708 - acc: 0.7849 - val_loss: 0.6212 - val_acc: 0.7319\n",
            "Epoch 5/100\n",
            "11916/11916 [==============================] - 5s 394us/step - loss: 0.4382 - acc: 0.8038 - val_loss: 0.6879 - val_acc: 0.7175\n",
            "Epoch 6/100\n",
            "11916/11916 [==============================] - 5s 393us/step - loss: 0.4076 - acc: 0.8162 - val_loss: 0.7296 - val_acc: 0.7190\n",
            "Epoch 7/100\n",
            "11916/11916 [==============================] - 5s 393us/step - loss: 0.3770 - acc: 0.8374 - val_loss: 0.7877 - val_acc: 0.7069\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4837935174069628"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "metadata": {
        "id": "j_VACJnNqujs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sub_task B\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "V3qSHBfcJ-d3",
        "colab_type": "code",
        "outputId": "d683962d-6dc5-43cd-8d5e-1ebc806e9682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv, x_test_tfv = tf_idf(x_train_b, x_val_b,x_test_b)\n",
        "clf =logistic_regression(x_train_tfv,y_train_b, x_val_tfv, y_val_b)\n",
        "pred= clf.predict(x_test_tfv)\n",
        "f1_score(np.ones(len(x_test_b)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression f1 score : 0.487041520939826\n",
            "Logistic regression : 0.8818181818181818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4989561586638831"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "metadata": {
        "id": "6Cn6RqC47nHV",
        "colab_type": "code",
        "outputId": "35260cef-951b-4feb-930a-55b924a8634d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_b,x_val_b)\n",
        "_,x_test_pad,_,_,_= tokenize_text(x_train_b,x_test_b)\n",
        "model = lstm_model(word_index,max_len,embedding_matrix)\n",
        "model.fit(x_train_pad, y=y_train_b, batch_size=512, epochs=100, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_b), callbacks=[earlystop])\n",
        "pred= model.predict(x_test_pad)\n",
        "pred=np.where(pred>=0.5,1,0).reshape(-1,)\n",
        "f1_score(np.ones(len(x_test_pad)),pred, average='macro')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10007/10007 [00:00<00:00, 405648.02it/s]\n",
            "100%|██████████| 10007/10007 [00:00<00:00, 389904.13it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 3960 samples, validate on 440 samples\n",
            "Epoch 1/100\n",
            "3960/3960 [==============================] - 13s 3ms/step - loss: 0.4700 - f1: 0.8965 - val_loss: 0.4065 - val_f1: 0.9372\n",
            "Epoch 2/100\n",
            "3960/3960 [==============================] - 2s 546us/step - loss: 0.3827 - f1: 0.9365 - val_loss: 0.3654 - val_f1: 0.9372\n",
            "Epoch 3/100\n",
            "3960/3960 [==============================] - 2s 528us/step - loss: 0.3576 - f1: 0.9366 - val_loss: 0.3438 - val_f1: 0.9372\n",
            "Epoch 4/100\n",
            "3960/3960 [==============================] - 2s 527us/step - loss: 0.3485 - f1: 0.9366 - val_loss: 0.3461 - val_f1: 0.9372\n",
            "Epoch 5/100\n",
            "3960/3960 [==============================] - 2s 528us/step - loss: 0.3402 - f1: 0.9365 - val_loss: 0.3479 - val_f1: 0.9372\n",
            "Epoch 6/100\n",
            "3960/3960 [==============================] - 2s 530us/step - loss: 0.3402 - f1: 0.9366 - val_loss: 0.3466 - val_f1: 0.9372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "metadata": {
        "id": "3RyU9NJZVVIp",
        "colab_type": "code",
        "outputId": "af46adc1-5493-489d-8e8b-eef32668e0bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_b,x_val_b)\n",
        "_,x_test_pad,_,_,_= tokenize_text(x_train_b,x_test_b)\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "model = etextcnn_model(word_index,max_len,embedding_matrix)\n",
        "model.fit(x_train_pad, y=y_train_b, batch_size=512, epochs=100, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_b), callbacks=[earlystop])\n",
        "pred= model.predict(x_test_pad)\n",
        "pred=np.where(pred>=0.5,1,0).reshape(-1,)\n",
        "f1_score(np.ones(len(x_test_pad)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10007/10007 [00:00<00:00, 458915.37it/s]\n",
            "100%|██████████| 10007/10007 [00:00<00:00, 467107.37it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 3960 samples, validate on 440 samples\n",
            "Epoch 1/100\n",
            "3960/3960 [==============================] - 13s 3ms/step - loss: 2.3099 - acc: 0.5886 - val_loss: 1.4039 - val_acc: 0.4273\n",
            "Epoch 2/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 1.2392 - acc: 0.6381 - val_loss: 0.5397 - val_acc: 0.8795\n",
            "Epoch 3/100\n",
            "3960/3960 [==============================] - 1s 355us/step - loss: 0.8273 - acc: 0.7063 - val_loss: 0.5826 - val_acc: 0.7364\n",
            "Epoch 4/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 0.7141 - acc: 0.7672 - val_loss: 0.5014 - val_acc: 0.8795\n",
            "Epoch 5/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 0.6213 - acc: 0.8306 - val_loss: 0.4214 - val_acc: 0.8795\n",
            "Epoch 6/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 0.5643 - acc: 0.8510 - val_loss: 0.3715 - val_acc: 0.8795\n",
            "Epoch 7/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 0.4990 - acc: 0.8591 - val_loss: 0.3483 - val_acc: 0.8818\n",
            "Epoch 8/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 0.4405 - acc: 0.8790 - val_loss: 0.3431 - val_acc: 0.8818\n",
            "Epoch 9/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 0.3990 - acc: 0.8864 - val_loss: 0.3547 - val_acc: 0.8818\n",
            "Epoch 10/100\n",
            "3960/3960 [==============================] - 1s 353us/step - loss: 0.3614 - acc: 0.9015 - val_loss: 0.3706 - val_acc: 0.8818\n",
            "Epoch 11/100\n",
            "3960/3960 [==============================] - 1s 356us/step - loss: 0.3269 - acc: 0.9134 - val_loss: 0.3827 - val_acc: 0.8818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4989561586638831"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "metadata": {
        "id": "MCIFOHbexhuW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Subtask C"
      ]
    },
    {
      "metadata": {
        "id": "ncrukCSa_06U",
        "colab_type": "code",
        "outputId": "d150335e-9d27-4f74-872c-f9e279452f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv, x_test_tfv = tf_idf(x_train_c, x_val_c,x_test_c)\n",
        "clf =logistic_regression(x_train_tfv,y_train_c, x_val_tfv, y_val_c)\n",
        "pred= clf.predict(x_test_tfv)\n",
        "print(pred)\n",
        "f1_score(np.ones(len(x_test_c)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Logistic regression f1 score : 0.4442148960250622\n",
            "Logistic regression : 0.6881443298969072\n",
            "[0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0\n",
            " 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.42857142857142855"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "metadata": {
        "id": "sxmUgN8uUUai",
        "colab_type": "code",
        "outputId": "a5ce9436-e5be-4920-ab90-9b7145573bfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(x_train_tfv, y_train_c_enc)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-198-af48c66456df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multinomial'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_tfv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_c_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1288\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    759\u001b[0m                         dtype=None)\n\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad input shape {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: bad input shape (3488, 3)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "sfTUq1siZgkB",
        "colab_type": "code",
        "outputId": "e2143ac5-6478-48dd-e642-dbb145f76bed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_test_c[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nopasaran unity demo oppose farright london  antifa oct  enough enough '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    }
  ]
}