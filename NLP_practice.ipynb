{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-practice.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chendingyan/NLP490H/blob/master/NLP_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_N5pSxDNnGfF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NLP Coursework"
      ]
    },
    {
      "metadata": {
        "id": "QnmwkL__fdDG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Initialization"
      ]
    },
    {
      "metadata": {
        "id": "mxXsBVkuocdI",
        "colab_type": "code",
        "outputId": "c9e30c6a-aadd-45ad-9277-a05e85e3c6b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JrLzlL1BCs9y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AHhTNuEWLAcz",
        "colab_type": "code",
        "outputId": "8cba4112-f463-4c46-8417-171d1f3b6cbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm \n",
        "import codecs\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score, classification_report,f1_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D,Dropout\n",
        "from keras.layers import Dense, Embedding, Activation, merge, Input, Lambda, Reshape\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import np_utils\n",
        "from keras.layers.merge import concatenate\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "stop = stopwords.words(\"english\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "#we fix the seeds to get consistent results\n",
        "\n",
        "SEED = 234\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rpePPeOvtW46",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ]
    },
    {
      "metadata": {
        "id": "IIyUwk6-LMC4",
        "colab_type": "code",
        "outputId": "7bbf1abb-4f5b-40fc-ba15-a0986a726cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "cell_type": "code",
      "source": [
        "sl=1000\n",
        "vocab_size=200000\n",
        "PATH = 'drive/My Drive/data/OffensEval_task_data/offenseval-training-v1.tsv'\n",
        "df = pd.read_csv(PATH,sep='\\t')\n",
        "\n",
        "\n",
        "print('Indexing word vectors.')\n",
        "glove_path = 'drive/My Drive/data/glove.6B.300d.txt'\n",
        "embeddings_index = {}\n",
        "f = open(glove_path, encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3d4ae37bbd02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'drive/My Drive/data/OffensEval_task_data/offenseval-training-v1.tsv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "K69PmvHpS2xM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "metadata": {
        "id": "waDE_uxbEhXx",
        "colab_type": "code",
        "outputId": "59ee67a4-6729-4125-e1d4-890486c29265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "print('-------Remove Stop Word--------')\n",
        "stopword_set = set(stopwords.words(\"english\"))\n",
        "\n",
        "# convert to lower case and split \n",
        "df.tweet = df.tweet.apply(lambda x: ' '.join([word for word in x.split() if word not in stopword_set]))\n",
        "\n",
        "# keep only words\n",
        "pat1 = r'@[A-Za-z0-9]+'\n",
        "pat2 =r'[^a-zA-Z\\s]'\n",
        "pat3 =r\"\\bURL\\b\"\n",
        "combined_pat = r'|'.join((pat1, pat2,pat3))\n",
        "regex_pat = re.compile(combined_pat, flags=re.IGNORECASE)\n",
        "df.tweet = df.tweet.str.replace(regex_pat, '')\n",
        "\n",
        "# join the cleaned words in a list\n",
        "df.tweet.str.join(\"\")\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------Remove Stop Word--------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>86426</td>\n",
              "      <td>She ask native Americans take is</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>90194</td>\n",
              "      <td>Go home youre drunk  MAGA Trump</td>\n",
              "      <td>OFF</td>\n",
              "      <td>TIN</td>\n",
              "      <td>IND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16820</td>\n",
              "      <td>Amazon investigating Chinese employees selling...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>62688</td>\n",
              "      <td>Someone shouldveTaken piece shit volcano</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>43605</td>\n",
              "      <td>Obama wanted liberals amp illegals move red ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                              tweet subtask_a  \\\n",
              "0  86426                   She ask native Americans take is       OFF   \n",
              "1  90194                  Go home youre drunk  MAGA Trump         OFF   \n",
              "2  16820  Amazon investigating Chinese employees selling...       NOT   \n",
              "3  62688          Someone shouldveTaken piece shit volcano        OFF   \n",
              "4  43605    Obama wanted liberals amp illegals move red ...       NOT   \n",
              "\n",
              "  subtask_b subtask_c  \n",
              "0       UNT       NaN  \n",
              "1       TIN       IND  \n",
              "2       NaN       NaN  \n",
              "3       UNT       NaN  \n",
              "4       NaN       NaN  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "CHx784gegkXc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.to_csv('drive/My Drive/data/OffensEval_task_data/clean_dataset.csv',encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XjuHofVLk99V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split data for different subtasks\n",
        "lbl = LabelEncoder()\n",
        "y = lbl.fit_transform(df.subtask_a.values)\n",
        "x = df.tweet.values\n",
        "\n",
        "# Need data cleaning here!\n",
        "x_train_a, x_val_a, y_train_a,y_val_a =train_test_split(x,y,stratify=y,random_state=SEED,test_size=0.1,shuffle=True)\n",
        "\n",
        "\n",
        "taskb_idx= y==1\n",
        "y_b =df.subtask_b.values[taskb_idx]\n",
        "y_b = np.where(y_b=='TIN',1,0)\n",
        "x_b = x[taskb_idx]\n",
        "x_train_b, x_val_b, y_train_b,y_val_b =train_test_split(x_b,y_b,stratify=y_b,random_state=SEED,test_size=0.1,shuffle=True)\n",
        "\n",
        "\n",
        "taskc_idx= y_b==1\n",
        "y_c =df.subtask_c.values[taskb_idx][taskc_idx]\n",
        "lbl_enc = preprocessing.LabelEncoder()\n",
        "y_c = lbl_enc.fit_transform(y_c)\n",
        "x_c = x_b[taskc_idx]\n",
        "x_train_c, x_val_c, y_train_c,y_val_c =train_test_split(x_c,y_c,stratify=y_c,random_state=SEED,test_size=0.1,shuffle=True)\n",
        "# one-hot encode\n",
        "y_train_c_enc = np_utils.to_categorical(y_train_c)\n",
        "y_val_c_enc = np_utils.to_categorical(y_val_c)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5J9P31KQnxyg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature extract"
      ]
    },
    {
      "metadata": {
        "id": "fV3K6QKOjtkb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tf_idf(x_train, x_val):\n",
        "\n",
        "    tfv = TfidfVectorizer(min_df=3, max_df=0.5, max_features=None,ngram_range=(1, 3),use_idf=True,smooth_idf=True)\n",
        "    tfv.fit(list(x_train) + list(x_val))\n",
        "    x_train_tfv =  tfv.transform(x_train) \n",
        "    x_val_tfv = tfv.transform(x_val)\n",
        "\n",
        "    return x_train_tfv, x_val_tfv\n",
        "        \n",
        "        \n",
        "def bag_of_word(x_train,x_val):\n",
        "    ctv = CountVectorizer(min_df=3,max_df=0.5,ngram_range=(1,2))\n",
        "    ctv.fit(list(x_train) + list(x_val))\n",
        "    x_train_ctv =  ctv.transform(x_train) \n",
        "    x_val_ctv = ctv.transform(x_val)\n",
        "    return x_train_ctv, x_val_ctv\n",
        "\n",
        "def word_vectors(x):\n",
        "    N =[]\n",
        "    for s in tqdm(x):\n",
        "        M = []\n",
        "        for w in s:\n",
        "            try:\n",
        "                M.append(embeddings_index[w])\n",
        "            except:\n",
        "                continue\n",
        "        M = np.array(M)\n",
        "        v = M.sum(axis=0)\n",
        "        if type(v) != np.ndarray:\n",
        "            return np.zeros(300)\n",
        "        N.append(v / np.sqrt((v ** 2).sum()))\n",
        "    N = np.array(N)\n",
        "    return N\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kVTpoXGmPOw6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classical Method "
      ]
    },
    {
      "metadata": {
        "id": "s6pJveipL2nF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def logistic_regression(x_train, y_train, x_val, y_val, if_tune=False):\n",
        "    if if_tune == True:\n",
        "        clf =logistic_tune(x_train, y_train)\n",
        "    else:\n",
        "\n",
        "        clf = LogisticRegression(C=1.0,solver='lbfgs')\n",
        "        clf.fit(x_train, y_train)\n",
        "    acc=clf.score(x_val, y_val)\n",
        "    pred = clf.predict(x_val)\n",
        "    print('Logistic regression f1 score :', f1_score(y_val,pred))\n",
        "    print('Logistic regression :',acc)\n",
        "    return clf\n",
        "        \n",
        "        \n",
        "def naive_bayes(x_train, y_train, x_val, y_val, if_tune=False):\n",
        "    if if_tune == True:\n",
        "        clf = naive_bayes(x_train, y_train)\n",
        "    else:\n",
        "        clf = MultinomialNB()\n",
        "        clf.fit(x_train, y_train)\n",
        "    acc = clf.score(x_val, y_val)\n",
        "    pred = clf.predict(x_val)\n",
        "    print('Naive Bayes f1 score :', f1_score(y_val,pred))\n",
        "    print('Naive Bayes :',acc)\n",
        "    return clf\n",
        "        \n",
        "\n",
        "def svm(x_train, y_train, x_val, y_val, if_tune=False):\n",
        "    if if_tune == True:\n",
        "        clf = svm_tune(x_train, y_train)\n",
        "    else:\n",
        "        svd = decomposition.TruncatedSVD(n_components=120)\n",
        "        svd.fit(x_train)\n",
        "        x_train_svd = svd.transform(x_train)\n",
        "        x_val_svd = svd.transform(x_val)\n",
        "\n",
        "        scl = StandardScaler()\n",
        "        scl.fit(x_train_svd)\n",
        "        x_train_svd_scl = scl.transform(x_train_svd)\n",
        "        x_val_svd_scl = scl.transform(x_val_svd)\n",
        "        clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
        "        clf.fit(x_train_svd_scl, y_train)\n",
        "    acc = clf.score(x_val, y_val)\n",
        "    pred = clf.predict(x_val)\n",
        "    print('SVM f1 score :', f1_score(y_val,pred))\n",
        "    print('SVM :',acc)\n",
        "    return clf\n",
        "\n",
        "\n",
        "def xgboost(x_train, y_train, x_val, y_val, if_tune=False):\n",
        "    if if_tune == True:\n",
        "        clf = xgboost_tune(x_train,y_train)\n",
        "    else:\n",
        "        clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
        "                            subsample=0.8, nthread=10, learning_rate=0.1)\n",
        "        clf.fit(x_train.tocsc(), y_train)\n",
        "    acc = clf.score(x_val.tocsc(),y_val)\n",
        "    pred = clf.predict(x_val)\n",
        "    print('Xgboost f1 score :', f1_score(y_val,pred))\n",
        "    print('Xgboost :',acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uzGQtZ4C3pdR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nxN0yoRCfaKc",
        "colab_type": "code",
        "outputId": "4eba3b5e-21fd-4342-a95d-c46e59801adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "naive_bayes(x_train_tfv, y_train_a, x_val_tfv, y_val_a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes f1 score : 0.38030560271646857\n",
            "Naive Bayes : 0.724320241691843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "Oc2t_dLKlgUu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "metadata": {
        "id": "ojNYSU85WXYV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "\n",
        "# Use pipeline for parameter tuning \n",
        "# mll_scorer = metrics.make_scorer(accuracy_score, greater_is_better=False)\n",
        "\n",
        "def svm_tune(x_train_tfv, y_train_a):\n",
        "    svd = TruncatedSVD()\n",
        "\n",
        "    # Standard Scaler\n",
        "    scl = preprocessing.StandardScaler()\n",
        "\n",
        "    svc = SVC()\n",
        "    # SVM\n",
        "    clf = pipeline.Pipeline([('svd',svd), ('scl',scl), ('svm',svc)])\n",
        "\n",
        "    param_grid = {'svd__n_components':[120,180],\n",
        "                 'svm__C':[1, 10],\n",
        "                  'svm__kernel':('linear', 'rbf'),\n",
        "                  'svm__gamma':['auto','scale']\n",
        "                 }\n",
        "    model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                         verbose=10, n_jobs=-1, iid=True, refit=True)\n",
        "    model.fit(x_train_tfv, y_train_a) \n",
        "\n",
        "    print(\"Best score: %0.3f\" % model.best_score_)\n",
        "\n",
        "    return model.best_estimator_\n",
        "\n",
        "\n",
        "\n",
        "#Logistic Regression\n",
        "def logistic_tune(x_train_tfv, y_train_a):\n",
        "    lr_model = LogisticRegression()\n",
        "\n",
        "    # pipeline \n",
        "    clf = pipeline.Pipeline([('lr', lr_model)])\n",
        "    param_grid = {\n",
        "                         'lr__C': [0.1, 1.0, 10], \n",
        "                        'lr__penalty': ['l1', 'l2']}\n",
        "\n",
        "    model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                     verbose=10, n_jobs=-1, iid=True, refit=True)\n",
        "    model.fit(x_train_tfv, y_train_a) \n",
        "\n",
        "    print(\"Best score: %0.3f\" % model.best_score_)\n",
        "    \n",
        "    return model.best_estimator_\n",
        "\n",
        "# Naive Bayes\n",
        "def naive_bayes_tune(x_train_tfv, y_train_a):\n",
        "    nb_model = MultinomialNB()\n",
        "\n",
        "    clf = pipeline.Pipeline([('nb', nb_model)])\n",
        "\n",
        "    param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "    # （Grid Search Model）\n",
        "    model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                     verbose=10, n_jobs=-1, iid=True, refit=True,return_train_score=True)\n",
        "\n",
        "    model.fit(x_train_tfv, y_train_a) \n",
        "    print(\"Best score: %0.3f\" % model.best_score_)\n",
        "    \n",
        "    return model.best_estimator_\n",
        "\n",
        "# Xgboost\n",
        "def xgboost_tune(x_train_tfv, y_train_a):\n",
        "    \n",
        "    xgb_model = xgb.XGBClassifier()\n",
        "    clf = pipeline.Pipeline([('xgb', xgb_model)])\n",
        "    param_grid = {'learning_rate' :[0.1,0.01,0.001],\n",
        "                        'n_estimators':[200,400,600,800,1000],\n",
        "                        'max_depth':[3,5,7],\n",
        "                        'min_child_weight': [1,3,5]}\n",
        "    \n",
        "    model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                     verbose=10, n_jobs=-1, iid=True, refit=True,return_train_score=True)\n",
        "\n",
        "    model.fit(x_train_tfv.tocsc(), y_train_a) \n",
        "    print(\"Best score: %0.3f\" % model.best_score_)\n",
        "    \n",
        "    return model.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ex30wVspovB",
        "colab_type": "code",
        "outputId": "58b7762d-6a54-4468-ab44-5362e2e7cd01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "cell_type": "code",
      "source": [
        "nb_model = MultinomialNB()\n",
        "\n",
        "clf = pipeline.Pipeline([('nb', nb_model)])\n",
        "\n",
        "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# （Grid Search Model）\n",
        "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                 verbose=10, n_jobs=-1, iid=True, refit=True,return_train_score=True)\n",
        "\n",
        "model.fit(x_train_tfv, y_train_a) # 为了减少计算量，这里我们仅使用xtrain\n",
        "sc= model.score( x_val_tfv, y_val_a)\n",
        "print('valiation score: %0.3f' % sc )\n",
        "print(\"Best score: %0.3f\" % model.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters = model.best_estimator_.get_params()\n",
        "for param_name in sorted(param_grid.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valiation score: 0.724\n",
            "Best score: 0.720\n",
            "Best parameters set:\n",
            "\tnb__alpha: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1873s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  18 | elapsed:    1.3s remaining:    0.3s\n",
            "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    1.4s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "cNTe6jjVqHkM",
        "colab_type": "code",
        "outputId": "e1bdfdf1-2ff1-42bc-f8c7-869d6ff3e8d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        }
      },
      "cell_type": "code",
      "source": [
        "from hypopt import GridSearch\n",
        "\n",
        "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "opt = GridSearch(model= MultinomialNB(), param_grid=param_grid)\n",
        "opt.fit(x_train_tfv, y_train_a, x_val_tfv, y_val_a, scoring = 'accuracy')\n",
        "print('Test Score for Optimized Parameters:', opt.score(x_val_tfv, y_val_a))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Score for Optimized Parameters: 0.724320241691843\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HFliM2YVYDua",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Embedding"
      ]
    },
    {
      "metadata": {
        "id": "_VykvKn0YHyj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize_text(train, val):\n",
        "    token = text.Tokenizer(num_words=None)\n",
        " \n",
        "    tokenized_corpus = [text.text_to_word_sequence(i) for i in train]\n",
        "    sent_lengths = [len(sent) for sent in tokenized_corpus]\n",
        "    max_len = np.max(np.array(sent_lengths))\n",
        "    token.fit_on_texts(list(train) + list(val))\n",
        "    x_train_seq = token.texts_to_sequences(train)\n",
        "    x_val_seq = token.texts_to_sequences(val)\n",
        "\n",
        "    # zero padding\n",
        "    x_train_pad = sequence.pad_sequences(x_train_seq, maxlen=max_len)\n",
        "    x_val_pad = sequence.pad_sequences(x_val_seq, maxlen=max_len)\n",
        "\n",
        "    word_index = token.word_index\n",
        "\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "    for word, i in tqdm(word_index.items()):\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "    return x_train_pad, x_val_pad, max_len, word_index, embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XRvvEopZhSOB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Deep Neural Network\n"
      ]
    },
    {
      "metadata": {
        "id": "cuzX9VzC1A3E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BLTZ0AXKcIBB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Fully Connected Network\n",
        "def fcn_model(num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam', metrics=[f1] ):\n",
        "    fcn_model = Sequential()\n",
        "    fcn_model.add(Dense(300, input_dim=300, activation='relu'))\n",
        "    fcn_model.add(Dropout(0.2))\n",
        "    fcn_model.add(BatchNormalization())\n",
        "\n",
        "    fcn_model.add(Dense(300, activation='relu'))\n",
        "    fcn_model.add(Dropout(0.3))\n",
        "    fcn_model.add(BatchNormalization())\n",
        "    fcn_model.add(Dense(num_classes, activation=final_activation))\n",
        "\n",
        "    fcn_model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
        "    return fcn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bPOt81aCjNOP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "def lstm_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam', metrics=[f1]):\n",
        "    lstm_model = Sequential()\n",
        "    lstm_model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix],  input_length=max_len, trainable=False))\n",
        "    lstm_model.add(SpatialDropout1D(0.3))\n",
        "    lstm_model.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.3)))\n",
        "    lstm_model.add(Dense(512, activation='relu'))\n",
        "    lstm_model.add(Dropout(0.3))\n",
        "    lstm_model.add(Dense(256, activation='relu'))\n",
        "    lstm_model.add(Dropout(0.3))\n",
        "\n",
        "    lstm_model.add(Dense(num_classes,activation=final_activation))\n",
        "    \n",
        "    lstm_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return lstm_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NYL2KFv6mJra",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# GRU Network\n",
        "def gru_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam',  metrics=[f1]):\n",
        "\n",
        "    gru_model = Sequential()\n",
        "    gru_model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix],  input_length=max_len, trainable=False))\n",
        "    gru_model.add(SpatialDropout1D(0.3))\n",
        "    gru_model.add(GRU(100, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
        "    gru_model.add(GRU(120, dropout=0.3, recurrent_dropout=0.3))\n",
        "\n",
        "    gru_model.add(Dense(512, activation='relu'))\n",
        "    gru_model.add(Dropout(0.3))\n",
        "\n",
        "    gru_model.add(Dense(256, activation='relu'))\n",
        "    gru_model.add(Dropout(0.3))\n",
        "    gru_model.add(Dense(num_classes, activation=final_activation))\n",
        "                  \n",
        "    gru_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return gru_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4FnHvXIz1PHB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# CNN network\n",
        "def cnn_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam',  metrics=[f1]):\n",
        "\n",
        "\n",
        "    cnn_model = Sequential()\n",
        "    cnn_model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],input_length=max_len, trainable=False))\n",
        "    cnn_model.add(Convolution1D(256, 3, padding='same'))\n",
        "    cnn_model.add(MaxPool1D(3,3,padding='same'))\n",
        "    cnn_model.add(Convolution1D(128, 3, padding='same'))\n",
        "    cnn_model.add(MaxPool1D(3,3,padding='same'))\n",
        "    cnn_model.add(Convolution1D(64, 3, padding='same'))\n",
        "    cnn_model.add(Flatten())\n",
        "    cnn_model.add(Dropout(0.3))\n",
        "    cnn_model.add(BatchNormalization()) \n",
        "    cnn_model.add(Dense(256,activation='relu'))\n",
        "    cnn_model.add(Dropout(0.1))\n",
        "    cnn_model.add(Dense(num_classes),activation =final_activation)\n",
        "                  \n",
        "    cnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return cnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VhJ3A4uU1V_m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TextCNN\n",
        "def textcnn_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam', metrics=[f1]):\n",
        "\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embedder = Embedding(len(word_index) + 1, 300, input_length = max_len, weights = [embedding_matrix], trainable = False)\n",
        "    embed = embedder(main_input)\n",
        "    cnn1 = Convolution1D(256, 3, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn1 = MaxPool1D(pool_size=4)(cnn1)\n",
        "    cnn2 = Convolution1D(256, 4, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn2 = MaxPool1D(pool_size=4)(cnn2)\n",
        "    cnn3 = Convolution1D(256, 5, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn3 = MaxPool1D(pool_size=4)(cnn3)\n",
        "\n",
        "    # Combine three model into one\n",
        "    cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
        "    flat = Flatten()(cnn)\n",
        "    drop = Dropout(0.2)(flat)\n",
        "    main_output = Dense(num_classes, activation=final_activation)(drop)\n",
        "    textcnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    textcnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return textcnn_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vlLbPaIQ1ePw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# CNN+RNN\n",
        "def crnn_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam',  metrics=[f1]):\n",
        "\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embed = Embedding(len(word_index)+1, 300, input_length=max_len)(main_input)\n",
        "    cnn = Convolution1D(256, 3, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn = MaxPool1D(pool_size=4)(cnn)\n",
        "    cnn = Flatten()(cnn)\n",
        "    cnn = Dense(256)(cnn)\n",
        "    cnn = Dropout(0.3)(cnn)\n",
        "    rnn = Bidirectional(GRU(256, dropout=0.2, recurrent_dropout=0.1))(embed)\n",
        "    rnn = Dense(256)(rnn)\n",
        "    rnn = Dropout(0.3)(rnn)\n",
        "    con = concatenate([cnn,rnn], axis=-1)\n",
        "    main_output = Dense(num_classes, activation=final_activation)(con)\n",
        "    crnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    crnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return crnn.model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YmYuFT_31lmU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extended textCNN\n",
        "def etextcnn_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam',  metrics=[f1]):\n",
        "\n",
        "\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embedder = Embedding(len(word_index) + 1, 300, input_length = max_len, weights = [embedding_matrix], trainable = False)\n",
        "    embed = embedder(main_input)\n",
        "\n",
        "    # cnn1，kernel_size = 3\n",
        "    conv1_1 = Conv1D(256, 3, padding='same')(embed)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    relu1_1 = Activation('relu')(bn1_1)\n",
        "    drop1_1 = Dropout(0.3)(relu1_1)\n",
        "    conv1_2 = Conv1D(128, 3, padding='same')(drop1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    relu1_2 = Activation('relu')(bn1_2)\n",
        "    drop1_2 = Dropout(0.3)(relu1_2)\n",
        "    cnn1 = MaxPooling1D(pool_size=4)(drop1_1)\n",
        "    # cnn2，kernel_size = 4\n",
        "    conv2_1 = Conv1D(256, 4, padding='same')(embed)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    relu2_1 = Activation('relu')(bn2_1)\n",
        "    drop2_1 = Dropout(0.3)(relu2_1)\n",
        "    conv2_2 = Conv1D(128, 4, padding='same')(drop2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    relu2_2 = Activation('relu')(bn2_2)\n",
        "    drop2_2 = Dropout(0.3)(relu2_2)\n",
        "    cnn2 = MaxPooling1D(pool_size=4)(drop2_2)\n",
        "    # cnn3，kernel_size = 5\n",
        "    conv3_1 = Conv1D(256, 5, padding='same')(embed)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    relu3_1 = Activation('relu')(bn3_1)\n",
        "    drop3_1 = Dropout(0.3)(relu3_1)\n",
        "    conv3_2 = Conv1D(128, 5, padding='same')(drop3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    relu3_2 = Activation('relu')(bn3_2)\n",
        "    drop3_2 = Dropout(0.3)(relu3_2)\n",
        "    cnn3 = MaxPooling1D(pool_size=4)(drop3_2)\n",
        "\n",
        "    # Combine three block\n",
        "    cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
        "    flat = Flatten()(cnn)\n",
        "    drop = Dropout(0.5)(flat)\n",
        "    fc = Dense(512)(drop)\n",
        "    bn = BatchNormalization()(fc)\n",
        " \n",
        "    main_output = Dense(num_classes, activation=final_activation)(bn)\n",
        "    etextcnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    etextcnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return etextcnn.model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MM0X0n192mBp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def simple_visualize(model):\n",
        "    plt.subplot(211)\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.plot(history.history[\"acc\"], color=\"g\", label=\"Train\")\n",
        "    plt.plot(history.history[\"val_acc color=\"b\", label=\"Test\")\n",
        "    plt.legend(loc=\"best\")\n",
        "\n",
        "    plt.subplot(212)\n",
        "    plt.title(\"Loss\")\n",
        "    plt.plot(result.history[\"loss\"], color=\"g\", label=\"Train\")\n",
        "    plt.plot(result.history[\"val_loss\"], color=\"b\", label=\"Test\")\n",
        "    plt.legend(loc=\"best\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OSx_rgGh9bf6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Tuning "
      ]
    },
    {
      "metadata": {
        "id": "Hc8iJGp89abo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_a,x_val_a)\n",
        "\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import Adadelta, Adam, rmsprop\n",
        "    \n",
        "space = {\n",
        "            'units1': hp.choice('units1', [16,32,64,128,256,512]),\n",
        "            'units2': hp.choice('units2', [16,32,64,128,256,512]),\n",
        "            'units3': hp.choice('units3', [16,32,64,128,256,512]),\n",
        "            'units4': hp.choice('units4', [16,32,64,128,256,512]),\n",
        "            'dropout1': hp.uniform('dropout1', .25,.75),\n",
        "            'dropout2': hp.uniform('dropout2',  .25,.75),\n",
        "            'dropout3': hp.uniform('dropout3',  .25,.75),\n",
        "            'dropout4': hp.uniform('dropout4',  .25,.75),\n",
        "\n",
        "            'batch_size' : hp.choice('batch_size', [32,64,128,256,512]),\n",
        "\n",
        "            'epochs' :  3,\n",
        "            'optimizer': hp.choice('optimizer',['sgd','adadelta','adam','rmsprop']),\n",
        "        }\n",
        "\n",
        "def gru_tune(params):   \n",
        "\n",
        "    # GRU Network\n",
        "\n",
        "    gru_model = Sequential()\n",
        "    gru_model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix],  input_length=max_len, trainable=False))\n",
        "    gru_model.add(SpatialDropout1D(0.3))\n",
        "    gru_model.add(GRU(params['units1'], dropout=params['dropout1'], recurrent_dropout=0.3, return_sequences=True))\n",
        "    gru_model.add(GRU(params['units2'], dropout=params['dropout2'], recurrent_dropout=0.3))\n",
        "\n",
        "    gru_model.add(Dense(params['units3'], activation='relu',kernel_initializer = \"glorot_uniform\"))\n",
        "    gru_model.add(Dropout(params['dropout3']))\n",
        "\n",
        "    gru_model.add(Dense(params['units4'], activation='relu',kernel_initializer = \"glorot_uniform\"))\n",
        "    gru_model.add(Dropout(params['dropout4']))\n",
        "    gru_model.add(Dense(1, activation='sigmoid'))\n",
        "                  \n",
        "    gru_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "    gru_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = gru_model.evaluate(x_val_pad, y_val_a)\n",
        "    print(loss)\n",
        "    return {'loss': loss[0], 'status': STATUS_OK}\n",
        "\n",
        "def lstm_tune(params):\n",
        "    lstm_model = Sequential()\n",
        "    lstm_model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix],  input_length=max_len, trainable=False))\n",
        "    lstm_model.add(SpatialDropout1D(0.3))\n",
        "    lstm_model.add(Bidirectional(LSTM(params['units1'], dropout=params['dropout1'], recurrent_dropout=0.3)))\n",
        "    lstm_model.add(Dense(params['units2'], activation='relu'))\n",
        "    lstm_model.add(Dropout(params['dropout2']))\n",
        "    lstm_model.add(Dense(params['units3'], activation='relu'))\n",
        "    lstm_model.add(Dropout(params['dropout3']))\n",
        "\n",
        "    lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    lstm_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "    lstm_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = lstm_model.evaluate(x_val_pad, y_val_a)\n",
        "    print('validation accuracy :', loss[1])\n",
        "    return {'loss': -loss[1], 'status': STATUS_OK}\n",
        "\n",
        "def etextcnn_tune(params):\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embedder = Embedding(len(word_index) + 1, 300, input_length = max_len, weights = [embedding_matrix], trainable = False)\n",
        "    embed = embedder(main_input)\n",
        "\n",
        "    # cnn1，kernel_size = 3\n",
        "    conv1_1 = Conv1D(params['units1'], 3, padding='same')(embed)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    relu1_1 = Activation('relu')(bn1_1)\n",
        "    drop1_1 = Dropout(params['dropout1'])(relu1_1)\n",
        "    conv1_2 = Conv1D(params['units2'], 3, padding='same')(drop1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    relu1_2 = Activation('relu')(bn1_2)\n",
        "    drop1_2 = Dropout(params['dropout2'])(relu1_2)\n",
        "    cnn1 = MaxPooling1D(pool_size=4)(drop1_1)\n",
        "    # cnn2，kernel_size = 4\n",
        "    conv2_1 = Conv1D(params['units1'], 4, padding='same')(embed)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    relu2_1 = Activation('relu')(bn2_1)\n",
        "    drop2_1 = Dropout(params['dropout1'])(relu2_1)\n",
        "    conv2_2 = Conv1D(params['units2'], 4, padding='same')(drop2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    relu2_2 = Activation('relu')(bn2_2)\n",
        "    drop2_2 = Dropout(params['dropout2'])(relu2_2)\n",
        "    cnn2 = MaxPooling1D(pool_size=4)(drop2_2)\n",
        "    # cnn3，kernel_size = 5\n",
        "    conv3_1 = Conv1D(params['units1'], 5, padding='same')(embed)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    relu3_1 = Activation('relu')(bn3_1)\n",
        "    drop3_1 = Dropout(params['dropout1'])(relu3_1)\n",
        "    conv3_2 = Conv1D(params['units2'], 5, padding='same')(drop3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    relu3_2 = Activation('relu')(bn3_2)\n",
        "    drop3_2 = Dropout(params['dropout2'])(relu3_2)\n",
        "    cnn3 = MaxPooling1D(pool_size=4)(drop3_2)\n",
        "\n",
        "    # Combine three block\n",
        "    cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
        "    flat = Flatten()(cnn)\n",
        "    drop = Dropout(params['dropout3'])(flat)\n",
        "    fc = Dense(params['units3'])(drop)\n",
        "    bn = BatchNormalization()(fc)\n",
        " \n",
        "    main_output = Dense(1, activation='sigmoid')(bn)\n",
        "    etextcnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    \n",
        "    etextcnn_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "    etextcnn_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = etextcnn_model.evaluate(x_val_pad, y_val_a)\n",
        "    print('validation accuracy :', loss[1])\n",
        "    return {'loss': loss[0], 'status': STATUS_OK}\n",
        "\n",
        "def rcnn_tune(params):\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embed = Embedding(len(word_index)+1, 300, input_length=max_len)(main_input)\n",
        "    cnn = Conv1D(params['units1'], 3, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn = MaxPooling1D(pool_size=4)(cnn)\n",
        "    cnn = Flatten()(cnn)\n",
        "    cnn = Dense(params['units2'])(cnn)\n",
        "    cnn = Dropout(params['dropout1'])(cnn)\n",
        "    rnn = Bidirectional(GRU(params['units1'], dropout=0.2, recurrent_dropout=0.1))(embed)\n",
        "    rnn = Dense(params['units3'])(rnn)\n",
        "    rnn = Dropout(params['dropout2'])(rnn)\n",
        "    con = concatenate([cnn,rnn], axis=-1)\n",
        "    main_output = Dense(1, activation='sigmoid')(con)\n",
        "    crnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    crnn_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "\n",
        "    crnn_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = crnn_model.evaluate(x_val_pad, y_val_a)\n",
        "    print('validation accuracy :', loss[1])\n",
        "    return {'loss': loss[0], 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "def cnn_tune(params):\n",
        "    cnn_model = Sequential()\n",
        "    cnn_model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],input_length=max_len, trainable=False))\n",
        "    cnn_model.add(Conv1D(params['units1'], 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(params['units2'], 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(params['units3'], 3, padding='same'))\n",
        "    cnn_model.add(Flatten())\n",
        "    cnn_model.add(Dropout(params['dropout1']))\n",
        "    cnn_model.add(BatchNormalization()) \n",
        "    cnn_model.add(Dense(params['units4'],activation='relu'))\n",
        "    cnn_model.add(Dropout(params['dropout2']))\n",
        "    cnn_model.add(Dense(1,activation ='sigmoid'))\n",
        "    \n",
        "    cnn_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "\n",
        "    cnn_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = cnn_model.evaluate(x_val_pad, y_val_a)\n",
        "    print('validation accuracy :', loss)\n",
        "    return {'loss': -loss[1], 'status': STATUS_OK}\n",
        "\n",
        "trials = Trials()\n",
        "best = fmin(lstm_tune, space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
        "print('best:', best)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "inYlGBtjQPWl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cnn_model_opti(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam', metrics=['accuracy']):\n",
        "\n",
        "\n",
        "    cnn_model = Sequential()\n",
        "    cnn_model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],input_length=max_len, trainable=False))\n",
        "    cnn_model.add(Conv1D(512, 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(16, 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(32, 3, padding='same'))\n",
        "    cnn_model.add(Flatten())\n",
        "    cnn_model.add(Dropout(0.74))\n",
        "    cnn_model.add(BatchNormalization()) \n",
        "    cnn_model.add(Dense(32,activation='relu'))\n",
        "    cnn_model.add(Dropout(0.38))\n",
        "    cnn_model.add(Dense(num_classes,activation =final_activation))\n",
        "                  \n",
        "    cnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return cnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vzt4jjeHyZRc",
        "colab_type": "code",
        "outputId": "99a74f38-d4ec-40c1-a74c-8177a8715970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        }
      },
      "cell_type": "code",
      "source": [
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "\n",
        "model = cnn_model_opti(word_index,max_len,embedding_matrix)\n",
        "model.fit(x_train_pad, y=y_train_a, batch_size=256, epochs=20, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_a), callbacks=[earlystop])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/20\n",
            "11916/11916 [==============================] - 25s 2ms/step - loss: 0.7834 - acc: 0.5719 - val_loss: 0.5851 - val_acc: 0.6994\n",
            "Epoch 2/20\n",
            "11916/11916 [==============================] - 2s 154us/step - loss: 0.6244 - acc: 0.6944 - val_loss: 0.5417 - val_acc: 0.7266\n",
            "Epoch 3/20\n",
            "11916/11916 [==============================] - 2s 148us/step - loss: 0.5444 - acc: 0.7444 - val_loss: 0.5383 - val_acc: 0.7538\n",
            "Epoch 4/20\n",
            "11916/11916 [==============================] - 2s 148us/step - loss: 0.4922 - acc: 0.7784 - val_loss: 0.5289 - val_acc: 0.7508\n",
            "Epoch 5/20\n",
            "11916/11916 [==============================] - 2s 150us/step - loss: 0.4365 - acc: 0.8054 - val_loss: 0.5765 - val_acc: 0.7508\n",
            "Epoch 6/20\n",
            "11916/11916 [==============================] - 2s 150us/step - loss: 0.3547 - acc: 0.8512 - val_loss: 0.5758 - val_acc: 0.7409\n",
            "Epoch 7/20\n",
            "11916/11916 [==============================] - 2s 151us/step - loss: 0.2767 - acc: 0.8887 - val_loss: 0.6066 - val_acc: 0.7424\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff6c658eb00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "psYSnNLz2ZVK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sub_task A"
      ]
    },
    {
      "metadata": {
        "id": "ergaPK7-FMjd",
        "colab_type": "code",
        "outputId": "c7539fdc-3791-4811-eab8-d0cbc9a36a54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_a,x_val_a)\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "\n",
        "model = lstm_model(word_index,max_len,embedding_matrix)\n",
        "model.fit(x_train_pad, y=y_train_a, batch_size=512, epochs=100, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_a), callbacks=[earlystop])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20138/20138 [00:00<00:00, 389918.36it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/100\n",
            "11916/11916 [==============================] - 12s 971us/step - loss: 0.6281 - f1: 0.0377 - val_loss: 0.5798 - val_f1: 0.3290\n",
            "Epoch 2/100\n",
            "11916/11916 [==============================] - 8s 667us/step - loss: 0.5554 - f1: 0.4863 - val_loss: 0.5332 - val_f1: 0.5395\n",
            "Epoch 3/100\n",
            "11916/11916 [==============================] - 8s 668us/step - loss: 0.5235 - f1: 0.5437 - val_loss: 0.5181 - val_f1: 0.5722\n",
            "Epoch 4/100\n",
            "11916/11916 [==============================] - 8s 669us/step - loss: 0.5130 - f1: 0.5602 - val_loss: 0.5080 - val_f1: 0.5455\n",
            "Epoch 5/100\n",
            "11916/11916 [==============================] - 8s 680us/step - loss: 0.5028 - f1: 0.5596 - val_loss: 0.5085 - val_f1: 0.5949\n",
            "Epoch 6/100\n",
            "11916/11916 [==============================] - 8s 702us/step - loss: 0.4932 - f1: 0.5832 - val_loss: 0.5060 - val_f1: 0.6002\n",
            "Epoch 7/100\n",
            "11916/11916 [==============================] - 8s 652us/step - loss: 0.4839 - f1: 0.5942 - val_loss: 0.4976 - val_f1: 0.5885\n",
            "Epoch 8/100\n",
            "11916/11916 [==============================] - 8s 661us/step - loss: 0.4806 - f1: 0.5974 - val_loss: 0.5042 - val_f1: 0.6271\n",
            "Epoch 9/100\n",
            "11916/11916 [==============================] - 8s 660us/step - loss: 0.4749 - f1: 0.5976 - val_loss: 0.4999 - val_f1: 0.6169\n",
            "Epoch 10/100\n",
            "11916/11916 [==============================] - 8s 659us/step - loss: 0.4693 - f1: 0.6150 - val_loss: 0.4983 - val_f1: 0.6122\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f948af2f860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "PHx-7dbBE4uL",
        "colab_type": "code",
        "outputId": "4c7ef3d0-398e-44c2-f619-7ca96b54c204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range(0, 1, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "metadata": {
        "id": "Z_cmcuiOHfTE",
        "colab_type": "code",
        "outputId": "35834af6-851b-46c5-9621-507f0644a773",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "svm(x_train_tfv,y_train_a, x_val_tfv, y_val_a)\n",
        "naive_bayes(x_train_tfv,y_train_a, x_val_tfv, y_val_a)\n",
        "logistic_regression(x_train_tfv,y_train_a, x_val_tfv, y_val_a)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM : 0.7182779456193353\n",
            "Naive Bayes : 0.724320241691843\n",
            "Logistic regression : 0.7515105740181269\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "metadata": {
        "id": "yq_ibz4zNuek",
        "colab_type": "code",
        "outputId": "26c2051c-7a64-4309-bf37-2be4f7abe946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_w2v,x_val_w2v = word_count(x_train_a,x_val_a)\n",
        "svm(x_train_w2v,y_train_a, x_val_w2v, y_val_a)\n",
        "naive_bayes(x_train_w2v,y_train_a, x_val_w2v, y_val_a)\n",
        "logistic_regression(x_train_w2v,y_train_a, x_val_w2v, y_val_a)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM : 0.7061933534743202\n",
            "Naive Bayes : 0.716012084592145\n",
            "Logistic regression : 0.7409365558912386\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "metadata": {
        "id": "EhgSgj-RMTBX",
        "colab_type": "code",
        "outputId": "b4060d50-a5d9-444a-e048-a21a5748caf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1008
        }
      },
      "cell_type": "code",
      "source": [
        "etextcnn_model.fit(x_train_pad, y=y_train_a, batch_size=512, epochs=100, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_a), callbacks=[earlystop])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/100\n",
            "11916/11916 [==============================] - 11s 916us/step - loss: 2.3970 - acc: 0.6234 - val_loss: 2.4487 - val_acc: 0.6526\n",
            "Epoch 2/100\n",
            "11916/11916 [==============================] - 8s 637us/step - loss: 0.7958 - acc: 0.7001 - val_loss: 0.7634 - val_acc: 0.6760\n",
            "Epoch 3/100\n",
            "11916/11916 [==============================] - 8s 639us/step - loss: 0.5646 - acc: 0.7461 - val_loss: 0.6176 - val_acc: 0.7062\n",
            "Epoch 4/100\n",
            "11916/11916 [==============================] - 8s 638us/step - loss: 0.4888 - acc: 0.7732 - val_loss: 0.5276 - val_acc: 0.7424\n",
            "Epoch 5/100\n",
            "11916/11916 [==============================] - 8s 640us/step - loss: 0.4676 - acc: 0.7845 - val_loss: 0.6080 - val_acc: 0.7198\n",
            "Epoch 6/100\n",
            "11916/11916 [==============================] - 8s 640us/step - loss: 0.4502 - acc: 0.7953 - val_loss: 0.7024 - val_acc: 0.7002\n",
            "Epoch 7/100\n",
            "11916/11916 [==============================] - 8s 640us/step - loss: 0.4241 - acc: 0.8102 - val_loss: 0.7309 - val_acc: 0.7017\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9ec5e0a630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "j_VACJnNqujs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sub_task B\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "V3qSHBfcJ-d3",
        "colab_type": "code",
        "outputId": "86386139-272e-4a3d-cd62-7bc9b5208d94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_b, x_val_b)\n",
        "svm(x_train_tfv,y_train_b, x_val_tfv, y_val_b)\n",
        "naive_bayes(x_train_tfv,y_train_b, x_val_tfv, y_val_b)\n",
        "logistic_regression(x_train_tfv,y_train_b, x_val_tfv, y_val_b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM : 0.884090909090909\n",
            "Naive Bayes : 0.8818181818181818\n",
            "Logistic regression : 0.8818181818181818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "6Cn6RqC47nHV",
        "colab_type": "code",
        "outputId": "34b2b242-2ac3-4236-f879-73bf921e59f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_b, x_val_b)\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "\n",
        "model = lstm_model(word_index,max_len,embedding_matrix)\n",
        "model.fit(x_train_pad, y=y_train_b, batch_size=512, epochs=100, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_b), callbacks=[earlystop])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10619/10619 [00:00<00:00, 458030.19it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 3960 samples, validate on 440 samples\n",
            "Epoch 1/100\n",
            "3960/3960 [==============================] - 5s 1ms/step - loss: 0.4851 - f1: 0.8676 - val_loss: 0.4029 - val_f1: 0.9372\n",
            "Epoch 2/100\n",
            "3960/3960 [==============================] - 2s 612us/step - loss: 0.3826 - f1: 0.9365 - val_loss: 0.3669 - val_f1: 0.9372\n",
            "Epoch 3/100\n",
            "3960/3960 [==============================] - 2s 594us/step - loss: 0.3609 - f1: 0.9366 - val_loss: 0.3450 - val_f1: 0.9372\n",
            "Epoch 4/100\n",
            "3960/3960 [==============================] - 2s 606us/step - loss: 0.3548 - f1: 0.9366 - val_loss: 0.3429 - val_f1: 0.9372\n",
            "Epoch 5/100\n",
            "3960/3960 [==============================] - 2s 595us/step - loss: 0.3475 - f1: 0.9366 - val_loss: 0.3525 - val_f1: 0.9372\n",
            "Epoch 6/100\n",
            "3960/3960 [==============================] - 2s 616us/step - loss: 0.3439 - f1: 0.9366 - val_loss: 0.3459 - val_f1: 0.9372\n",
            "Epoch 7/100\n",
            "3960/3960 [==============================] - 2s 609us/step - loss: 0.3358 - f1: 0.9366 - val_loss: 0.3499 - val_f1: 0.9372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9488d075c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "MCIFOHbexhuW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Subtask C"
      ]
    },
    {
      "metadata": {
        "id": "ncrukCSa_06U",
        "colab_type": "code",
        "outputId": "5a9e9bd1-93c4-4581-e988-886d7108a34c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_c, x_val_c)\n",
        "svm(x_train_tfv,y_train_c, x_val_tfv, y_val_c)\n",
        "naive_bayes(x_train_tfv,y_train_c, x_val_tfv, y_val_c)\n",
        "logistic_regression(x_train_tfv,y_train_c, x_val_tfv, y_val_c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM : 0.7061855670103093\n",
            "Naive Bayes : 0.6623711340206185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Logistic regression : 0.6958762886597938\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "sxmUgN8uUUai",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}