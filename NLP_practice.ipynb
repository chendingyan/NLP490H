{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-practice.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chendingyan/NLP490H/blob/master/NLP_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_N5pSxDNnGfF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NLP Coursework"
      ]
    },
    {
      "metadata": {
        "id": "QnmwkL__fdDG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Initialization"
      ]
    },
    {
      "metadata": {
        "id": "mxXsBVkuocdI",
        "colab_type": "code",
        "outputId": "947ac52a-ae3c-4c71-8243-80d410160fb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Etz7hcPYCCaB",
        "colab_type": "code",
        "outputId": "bd2480af-9f52-4f86-d7c3-9e7dcb2c8016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip drive/My\\ Drive/data/OffensEval_task_data/Test\\ C\\ Release.zip -d drive/My\\ Drive/data/OffensEval_task_data/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/My Drive/data/OffensEval_task_data/Test C Release.zip\n",
            "  inflating: drive/My Drive/data/OffensEval_task_data/test_set_taskc.tsv  \n",
            "  inflating: drive/My Drive/data/OffensEval_task_data/readme-testsetc-v1.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JrLzlL1BCs9y",
        "colab_type": "code",
        "outputId": "9c9ed8b0-9b6e-49a1-eba6-945a9e816b81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.9 GB  | Proc size: 142.3 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AHhTNuEWLAcz",
        "colab_type": "code",
        "outputId": "f4d7540d-19d3-4690-ab0f-e91f0b8805bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm \n",
        "import codecs\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score, classification_report,f1_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D,Dropout\n",
        "from keras.layers import Dense, Embedding, Activation, merge, Input, Lambda, Reshape\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import np_utils\n",
        "from keras.layers.merge import concatenate\n",
        "from keras import optimizers\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "stop = stopwords.words(\"english\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "#we fix the seeds to get consistent results\n",
        "\n",
        "SEED = 234\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rpePPeOvtW46",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ]
    },
    {
      "metadata": {
        "id": "IIyUwk6-LMC4",
        "colab_type": "code",
        "outputId": "c593db9a-7331-46cb-9ebf-74a814de4771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 96
        }
      },
      "cell_type": "code",
      "source": [
        "PATH = 'drive/My Drive/data/OffensEval_task_data/offenseval-training-v1.tsv'\n",
        "df = pd.read_csv(PATH,sep='\\t')\n",
        "\n",
        "\n",
        "print('Indexing word vectors.')\n",
        "glove_path = 'drive/My Drive/data/glove.6B.50d.txt'\n",
        "embeddings_index = {}\n",
        "f = open(glove_path, encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors.\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K69PmvHpS2xM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "metadata": {
        "id": "waDE_uxbEhXx",
        "colab_type": "code",
        "outputId": "a0b69424-1d54-4274-aacc-89db536c8bb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "def preprocess(df):\n",
        "    print('-------Remove Stop Word--------')\n",
        "    stopword_set = set(stopwords.words(\"english\"))\n",
        "\n",
        "    # convert to lower case and split \n",
        "    df.tweet = df.tweet.apply(lambda x: ' '.join([word.lower() for word in x.split() if word not in stopword_set]))\n",
        "\n",
        "    # keep only words\n",
        "    pat1 = r'@[A-Za-z0-9]+'\n",
        "    pat2 =r'[^a-zA-Z\\s]'\n",
        "    pat3 =r\"\\bURL\\b\"\n",
        "    combined_pat = r'|'.join((pat1, pat2,pat3))\n",
        "    regex_pat = re.compile(combined_pat, flags=re.IGNORECASE)\n",
        "    df.tweet = df.tweet.str.replace(regex_pat, '')\n",
        "\n",
        "    # join the cleaned words in a list\n",
        "    df.tweet.str.join(\"\")\n",
        "    return df\n",
        "df= preprocess(df)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------Remove Stop Word--------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>86426</td>\n",
              "      <td>she ask native americans take is</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>90194</td>\n",
              "      <td>go home youre drunk  maga trump</td>\n",
              "      <td>OFF</td>\n",
              "      <td>TIN</td>\n",
              "      <td>IND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16820</td>\n",
              "      <td>amazon investigating chinese employees selling...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>62688</td>\n",
              "      <td>someone shouldvetaken piece shit volcano</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>43605</td>\n",
              "      <td>obama wanted liberals amp illegals move red ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                              tweet subtask_a  \\\n",
              "0  86426                   she ask native americans take is       OFF   \n",
              "1  90194                  go home youre drunk  maga trump         OFF   \n",
              "2  16820  amazon investigating chinese employees selling...       NOT   \n",
              "3  62688          someone shouldvetaken piece shit volcano        OFF   \n",
              "4  43605    obama wanted liberals amp illegals move red ...       NOT   \n",
              "\n",
              "  subtask_b subtask_c  \n",
              "0       UNT       NaN  \n",
              "1       TIN       IND  \n",
              "2       NaN       NaN  \n",
              "3       UNT       NaN  \n",
              "4       NaN       NaN  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "z4yW6GYRDlNq",
        "colab_type": "code",
        "outputId": "3bc5f1b0-ce41-4572-ba90-44ed2af28db4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "testA_path ='drive/My Drive/data/OffensEval_task_data/testset-taska.tsv'\n",
        "testB_path = 'drive/My Drive/data/OffensEval_task_data/testset-taskb.tsv'\n",
        "testC_path ='drive/My Drive/data/OffensEval_task_data/test_set_taskc.tsv'\n",
        "\n",
        "df_a = pd.read_csv(testA_path,sep='\\t')\n",
        "df_a = preprocess(df_a)\n",
        "df_b = pd.read_csv(testB_path,sep='\\t')\n",
        "df_b = preprocess(df_b)\n",
        "df_c = pd.read_csv(testC_path,sep='\\t')\n",
        "df_c = preprocess(df_c)\n",
        "\n",
        "x_test_a = df_a.tweet.values\n",
        "x_test_b = df_b.tweet.values\n",
        "x_test_c = df_c.tweet.values\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------Remove Stop Word--------\n",
            "-------Remove Stop Word--------\n",
            "-------Remove Stop Word--------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CHx784gegkXc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.to_csv('drive/My Drive/data/OffensEval_task_data/clean_dataset.tsv',encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XjuHofVLk99V",
        "colab_type": "code",
        "outputId": "a3fc3b74-6ab8-4fdc-ac2e-3838cbfb3ec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "cell_type": "code",
      "source": [
        "# split data for different subtasks\n",
        "lbl = LabelEncoder()\n",
        "y = lbl.fit_transform(df.subtask_a.values)\n",
        "x = df.tweet.values\n",
        "\n",
        "# Need data cleaning here!\n",
        "x_train_a, x_val_a, y_train_a,y_val_a =train_test_split(x,y,stratify=y,random_state=SEED,test_size=0.1,shuffle=True)\n",
        "\n",
        "\n",
        "taskb_idx= y==1\n",
        "y_b =df.subtask_b.values[taskb_idx]\n",
        "y_b = np.where(y_b=='TIN',1,0)\n",
        "x_b = x[taskb_idx]\n",
        "x_train_b, x_val_b, y_train_b,y_val_b =train_test_split(x_b,y_b,stratify=y_b,random_state=SEED,test_size=0.1,shuffle=True)\n",
        "\n",
        "\n",
        "taskc_idx= y_b==1\n",
        "y_c =df.subtask_c.values[taskb_idx][taskc_idx]\n",
        "print(y_c)\n",
        "lbl_enc = preprocessing.LabelEncoder()\n",
        "y_c = lbl_enc.fit_transform(y_c)\n",
        "x_c = x_b[taskc_idx]\n",
        "x_train_c, x_val_c, y_train_c,y_val_c =train_test_split(x_c,y_c,stratify=y_c,random_state=SEED,test_size=0.1,shuffle=True)\n",
        "# one-hot encode\n",
        "y_train_c_enc = np_utils.to_categorical(y_train_c)\n",
        "y_val_c_enc = np_utils.to_categorical(y_val_c)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['IND' 'OTH' 'GRP' ... 'GRP' 'IND' 'OTH']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sddvjL3k7ZwT",
        "colab_type": "code",
        "outputId": "03c03d31-e89a-489e-fd6f-4d4544f10011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "print(y_b.shape)\n",
        "unique, counts = np.unique(y_b, return_counts=True)\n",
        "print(unique)\n",
        "print(counts)\n",
        "print(y_val_b.shape)\n",
        "unique, counts = np.unique(y_val_b, return_counts=True)\n",
        "print(unique)\n",
        "print(counts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4400,)\n",
            "[0 1]\n",
            "[ 524 3876]\n",
            "(440,)\n",
            "[0 1]\n",
            "[ 52 388]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LImqaPLYXKuM",
        "colab_type": "code",
        "outputId": "7037e749-1043-42f0-f15d-024bb86d0541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 0, ..., 0, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "metadata": {
        "id": "5J9P31KQnxyg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature extract"
      ]
    },
    {
      "metadata": {
        "id": "fV3K6QKOjtkb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_f=10000\n",
        "\n",
        "def tf_idf(x):\n",
        "\n",
        "    tfv = TfidfVectorizer(min_df=3, max_df=0.5, max_features=max_f,ngram_range=(1, 3),use_idf=True,smooth_idf=True)\n",
        "    tfv.fit(x)\n",
        "    return tfv\n",
        "        \n",
        "        \n",
        "def bag_of_word(x_train,x_val):\n",
        "    ctv = CountVectorizer(min_df=3,max_df=0.5,ngram_range=(1,2))\n",
        "    ctv.fit(list(x_train) + list(x_val))\n",
        "    x_train_ctv =  ctv.transform(x_train) \n",
        "    x_val_ctv = ctv.transform(x_val)\n",
        "    return x_train_ctv, x_val_ctv\n",
        "\n",
        "def word_vectors(x):\n",
        "    N =[]\n",
        "    for s in tqdm(x):\n",
        "        M = []\n",
        "        for w in s:\n",
        "            try:\n",
        "                M.append(embeddings_index[w])\n",
        "            except:\n",
        "                continue\n",
        "        M = np.array(M)\n",
        "        v = M.sum(axis=0)\n",
        "        if type(v) != np.ndarray:\n",
        "            return np.zeros(300)\n",
        "        N.append(v / np.sqrt((v ** 2).sum()))\n",
        "    N = np.array(N)\n",
        "    return N\n",
        "\n",
        "\n",
        "def tokenize_text(train, val):\n",
        "    token = text.Tokenizer(num_words=None)\n",
        " \n",
        "    tokenized_corpus = [text.text_to_word_sequence(i) for i in train]\n",
        "    sent_lengths = [len(sent) for sent in tokenized_corpus]\n",
        "    max_len = np.max(np.array(sent_lengths))\n",
        "    token.fit_on_texts(train)\n",
        "    x_train_seq = token.texts_to_sequences(train)\n",
        "    x_val_seq = token.texts_to_sequences(val)\n",
        "\n",
        "    # zero padding\n",
        "    x_train_pad = sequence.pad_sequences(x_train_seq, maxlen=max_len)\n",
        "    x_val_pad = sequence.pad_sequences(x_val_seq, maxlen=max_len)\n",
        "\n",
        "    word_index = token.word_index\n",
        "\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, 50))\n",
        "    for word, i in tqdm(word_index.items()):\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "    return x_train_pad, x_val_pad, max_len, word_index, embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XWsf0WOQO4fM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kSNcNv78oB2O",
        "colab_type": "code",
        "outputId": "1be97ddf-fc32-48df-af0a-e23ce7364e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1008
        }
      },
      "cell_type": "code",
      "source": [
        "tfv = tf_idf(x)\n",
        "x_train_tfv = tfv.transform(x_train_a)\n",
        "x_val_tfv = tfv.transform(x_val_a)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128,input_shape=(x_train_tfv.shape[1],)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(32))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "nb_epochs = 20\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "model.fit(x_train_tfv, y_train_a, batch_size=batch_size,validation_data=(x_val_tfv, y_val_a),epochs=nb_epochs,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/20\n",
            "11916/11916 [==============================] - 10s 838us/step - loss: 0.6071 - acc: 0.6802 - val_loss: 0.5687 - val_acc: 0.7205\n",
            "Epoch 2/20\n",
            "11916/11916 [==============================] - 2s 155us/step - loss: 0.4355 - acc: 0.8098 - val_loss: 0.5853 - val_acc: 0.7069\n",
            "Epoch 3/20\n",
            "11916/11916 [==============================] - 2s 157us/step - loss: 0.2765 - acc: 0.8927 - val_loss: 0.7005 - val_acc: 0.7047\n",
            "Epoch 4/20\n",
            "11916/11916 [==============================] - 2s 156us/step - loss: 0.1627 - acc: 0.9444 - val_loss: 0.9513 - val_acc: 0.7062\n",
            "Epoch 5/20\n",
            "11916/11916 [==============================] - 2s 156us/step - loss: 0.0877 - acc: 0.9738 - val_loss: 1.1987 - val_acc: 0.7107\n",
            "Epoch 6/20\n",
            "11916/11916 [==============================] - 2s 156us/step - loss: 0.0512 - acc: 0.9842 - val_loss: 1.4279 - val_acc: 0.6911\n",
            "Epoch 7/20\n",
            "11916/11916 [==============================] - 2s 156us/step - loss: 0.0385 - acc: 0.9891 - val_loss: 1.4939 - val_acc: 0.6971\n",
            "Epoch 8/20\n",
            "11916/11916 [==============================] - 2s 160us/step - loss: 0.0294 - acc: 0.9913 - val_loss: 1.7351 - val_acc: 0.7130\n",
            "Epoch 9/20\n",
            "11916/11916 [==============================] - 2s 157us/step - loss: 0.0251 - acc: 0.9916 - val_loss: 1.8229 - val_acc: 0.7077\n",
            "Epoch 10/20\n",
            "11916/11916 [==============================] - 2s 158us/step - loss: 0.0240 - acc: 0.9927 - val_loss: 1.7966 - val_acc: 0.7115\n",
            "Epoch 11/20\n",
            "11916/11916 [==============================] - 2s 159us/step - loss: 0.0216 - acc: 0.9931 - val_loss: 1.9202 - val_acc: 0.7122\n",
            "Epoch 12/20\n",
            "11916/11916 [==============================] - 2s 157us/step - loss: 0.0195 - acc: 0.9938 - val_loss: 1.9939 - val_acc: 0.6949\n",
            "Epoch 13/20\n",
            "11916/11916 [==============================] - 2s 158us/step - loss: 0.0189 - acc: 0.9936 - val_loss: 1.9822 - val_acc: 0.7069\n",
            "Epoch 14/20\n",
            "11916/11916 [==============================] - 2s 157us/step - loss: 0.0165 - acc: 0.9940 - val_loss: 2.1577 - val_acc: 0.7017\n",
            "Epoch 15/20\n",
            "11916/11916 [==============================] - 2s 159us/step - loss: 0.0167 - acc: 0.9944 - val_loss: 2.1788 - val_acc: 0.6979\n",
            "Epoch 16/20\n",
            "11916/11916 [==============================] - 2s 157us/step - loss: 0.0149 - acc: 0.9939 - val_loss: 2.1766 - val_acc: 0.6949\n",
            "Epoch 17/20\n",
            "11916/11916 [==============================] - 2s 157us/step - loss: 0.0177 - acc: 0.9937 - val_loss: 2.1403 - val_acc: 0.7054\n",
            "Epoch 18/20\n",
            "11916/11916 [==============================] - 2s 157us/step - loss: 0.0143 - acc: 0.9942 - val_loss: 2.2985 - val_acc: 0.7077\n",
            "Epoch 19/20\n",
            "11916/11916 [==============================] - 2s 159us/step - loss: 0.0127 - acc: 0.9947 - val_loss: 2.5513 - val_acc: 0.6994\n",
            "Epoch 20/20\n",
            "11916/11916 [==============================] - 2s 157us/step - loss: 0.0146 - acc: 0.9941 - val_loss: 2.3981 - val_acc: 0.7024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2f27123780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "kVTpoXGmPOw6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classical Method "
      ]
    },
    {
      "metadata": {
        "id": "s6pJveipL2nF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def logistic_regression(x_train, y_train, x_val, y_val, if_tune=False):\n",
        "    if if_tune == True:\n",
        "        param_grid, clf =logistic_tune(x_train, y_train)\n",
        "    else:\n",
        "\n",
        "        clf = LogisticRegression(C=1.0,solver='lbfgs')\n",
        "        clf.fit(x_train, y_train)\n",
        "    acc=clf.score(x_val, y_val)\n",
        "    pred = clf.predict(x_val)\n",
        "    print('Logistic regression f1 score on validation set:', f1_score(y_val,pred,average='macro'))\n",
        "    print('Logistic regression accuracy on validation set:', acc)\n",
        "    return param_grid, clf\n",
        "        \n",
        "        \n",
        "def naive_bayes(x_train, y_train, x_val, y_val, if_tune=False):\n",
        "    if if_tune == True:\n",
        "        param_grid, clf = naive_bayes_tune(x_train, y_train)\n",
        "    else:\n",
        "\n",
        "        clf = MultinomialNB()\n",
        "        clf.fit(x_train, y_train)\n",
        "    acc = clf.score(x_val, y_val)\n",
        "    pred = clf.predict(x_val)\n",
        "    print('Naive Bayes f1 score on validation set:', f1_score(y_val,pred,average='macro'))\n",
        "    print('Naive Bayes accuracy on validation set:', acc)\n",
        "    return param_grid, clf\n",
        "        \n",
        "\n",
        "def svm(x_train, y_train, x_val, y_val, if_tune=False):\n",
        "    if if_tune == True:\n",
        "        param_grid, clf = svm_tune(x_train, y_train)\n",
        "    else:\n",
        "\n",
        "        svd = decomposition.TruncatedSVD(n_components=120)\n",
        "        svd.fit(x_train)\n",
        "        x_train_svd = svd.transform(x_train)\n",
        "        x_val_svd = svd.transform(x_val)\n",
        "\n",
        "        scl = StandardScaler()\n",
        "        scl.fit(x_train_svd)\n",
        "        x_train_svd_scl = scl.transform(x_train_svd)\n",
        "        x_val_svd_scl = scl.transform(x_val_svd)\n",
        "        clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
        "        clf.fit(x_train_svd_scl, y_train)\n",
        "        \n",
        "        \n",
        "    acc = clf.score(x_val_svd_scl, y_val)\n",
        "    pred = clf.predict(x_val_svd_scl)\n",
        "    print('SVM f1 score on validation set:', f1_score(y_val,pred,average='macro'))\n",
        "    print('SVM accuracy on validation set:', acc)\n",
        "    return param_grid, clf\n",
        "\n",
        "\n",
        "def xgboost(x_train, y_train, x_val, y_val, if_tune=False):\n",
        "    if if_tune == True:\n",
        "        param_grid , clf = xgboost_tune(x_train,y_train)\n",
        "    else:\n",
        "\n",
        "        clf = XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
        "                            subsample=0.8, nthread=10, learning_rate=0.1)\n",
        "        clf.fit(x_train, y_train)\n",
        "    acc = clf.score(x_val,y_val)\n",
        "    pred = clf.predict(x_val)\n",
        "    print('Xgboost f1 score on validation set:', f1_score(y_val,pred,average='macro'))\n",
        "    print('Xgboost accuracy on validation set:', acc)\n",
        "    return param_grid, clf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oOpBv18zCADu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classical methods without tuning"
      ]
    },
    {
      "metadata": {
        "id": "uzGQtZ4C3pdR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "logistic_regression(x_train_tfv, y_train_a, x_val_tfv, y_val_a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nxN0yoRCfaKc",
        "colab_type": "code",
        "outputId": "4eba3b5e-21fd-4342-a95d-c46e59801adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "naive_bayes(x_train_tfv, y_train_a, x_val_tfv, y_val_a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes f1 score : 0.38030560271646857\n",
            "Naive Bayes : 0.724320241691843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "M2ng6v2GB5og",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "svm(x_train_tfv, y_train_a, x_val_tfv, y_val_a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GAReVShsB8qI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "xgboost(x_train_tfv, y_train_a, x_val_tfv, y_val_a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oc2t_dLKlgUu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "metadata": {
        "id": "ojNYSU85WXYV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "\n",
        "# Use pipeline for parameter tuning \n",
        "# mll_scorer = metrics.make_scorer(accuracy_score, greater_is_better=False)\n",
        "\n",
        "def svm_tune(x_train_tfv, y_train_a):\n",
        "    svd = TruncatedSVD()\n",
        "\n",
        "    # Standard Scaler\n",
        "    scl = preprocessing.StandardScaler()\n",
        "\n",
        "    svc = SVC()\n",
        "    # SVM\n",
        "    clf = pipeline.Pipeline([('svd',svd), ('scl',scl), ('svm',svc)])\n",
        "\n",
        "    param_grid = {'svd__n_components':[120,180],\n",
        "                 'svm__C':[1, 10],\n",
        "                  'svm__kernel':('linear', 'rbf'),\n",
        "                  'svm__gamma':['auto','scale']\n",
        "                 }\n",
        "    model = GridSearchCV(cv = 5, estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                         verbose=10, n_jobs=-1, iid=True, refit=True)\n",
        "    model.fit(x_train_tfv, y_train_a) \n",
        "\n",
        "    print(\"Best score while tuning: %0.3f\" % model.best_score_)\n",
        "\n",
        "    return param_grid, model\n",
        "\n",
        "\n",
        "#Logistic Regression\n",
        "def logistic_tune(x_train_tfv, y_train_a):\n",
        "    lr_model = LogisticRegression()\n",
        "\n",
        "    # pipeline \n",
        "    clf = pipeline.Pipeline([('lr', lr_model)])\n",
        "    param_grid = {\n",
        "                         'lr__C': [0.1, 1.0, 10], \n",
        "                        'lr__penalty': ['l1', 'l2']}\n",
        "\n",
        "    model = GridSearchCV(cv = 5, estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                     verbose=10, n_jobs=-1, iid=True, refit=True)\n",
        "    model.fit(x_train_tfv, y_train_a) \n",
        "\n",
        "    print(\"Best score while tuning: %0.3f\" % model.best_score_)\n",
        "    \n",
        "    return param_grid, model\n",
        "\n",
        "# Naive Bayes\n",
        "def naive_bayes_tune(x_train_tfv, y_train_a):\n",
        "    nb_model = MultinomialNB()\n",
        "\n",
        "    clf = pipeline.Pipeline([('nb', nb_model)])\n",
        "\n",
        "    param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "    # （Grid Search Model）\n",
        "    model = GridSearchCV(cv = 5,estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                     verbose=10, n_jobs=-1, iid=True, refit=True,return_train_score=True)\n",
        "\n",
        "    model.fit(x_train_tfv, y_train_a) \n",
        "    print(\"Best score while tuning: %0.3f\" % model.best_score_)\n",
        "    \n",
        "    return param_grid, model\n",
        "\n",
        "# Xgboost\n",
        "def xgboost_tune(x_train_tfv, y_train_a):\n",
        "    \n",
        "    xgb_model = XGBClassifier()\n",
        "    clf = pipeline.Pipeline([('xgb', xgb_model)])\n",
        "    param_grid = {'learning_rate' :[0.1,0.01,0.001],\n",
        "                        'n_estimators':[200,400,600,800,1000],\n",
        "                        'max_depth':[3,5,7],\n",
        "                        'min_child_weight': [1,3,5]}\n",
        "    \n",
        "    model = GridSearchCV(cv = 5, estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                     verbose=10, n_jobs=-1, iid=True, refit=True,return_train_score=True)\n",
        "\n",
        "    model.fit(x_train_tfv.tocsc(), y_train_a) \n",
        "    print(\"Best score while tuning: %0.3f\" % model.best_score_)\n",
        "    \n",
        "    return param_grid, model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0YZtm4ZjCJWW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classical methods with tuning"
      ]
    },
    {
      "metadata": {
        "id": "Z4lBl0brCNRw",
        "colab_type": "code",
        "outputId": "412be55b-f2af-4790-9538-07ee4e854b5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "param_grid, model = logistic_regression(x_train_tfv, y_train_a, x_val_tfv, y_val_a, True)\n",
        "best_param = model.best_estimator_\n",
        "best_parameters = best_param.get_params()\n",
        "for param_name in sorted(param_grid.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   10.5s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   22.2s\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   44.0s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.1min\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.6min\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  2.2min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score while tuning: 0.768\n",
            "Logistic regression f1 score on validation set: 0.6729564295353769\n",
            "Logistic regression accuracy on validation set: 0.7522658610271903\n",
            "\tlr__C: 1.0\n",
            "\tlr__penalty: 'l1'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qAaRNi1nCP6a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "param_grid, model = naive_bayes(x_train_tfv, y_train_a, x_val_tfv, y_val_a, True)\n",
        "best_param = model.best_estimator_\n",
        "best_parameters = best_param.get_params()\n",
        "for param_name in sorted(param_grid.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TOkV8vWgCR55",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "param_grid, model = svm(x_train_tfv, y_train_a, x_val_tfv, y_val_a, True)\n",
        "best_param = model.best_estimator_\n",
        "best_parameters = best_param.get_params()\n",
        "for param_name in sorted(param_grid.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XnShljliCTdq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv = tf_idf(x_train_a, x_val_a)\n",
        "param_grid, model = xgboost((x_train_tfv, y_train_a, x_val_tfv, y_val_a, True)\n",
        "best_param = model.best_estimator_\n",
        "best_parameters = best_param.get_params()\n",
        "for param_name in sorted(param_grid.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ex30wVspovB",
        "colab_type": "code",
        "outputId": "58b7762d-6a54-4468-ab44-5362e2e7cd01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "cell_type": "code",
      "source": [
        "nb_model = MultinomialNB()\n",
        "\n",
        "clf = pipeline.Pipeline([('nb', nb_model)])\n",
        "\n",
        "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# （Grid Search Model）\n",
        "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy',\n",
        "                                 verbose=10, n_jobs=-1, iid=True, refit=True,return_train_score=True)\n",
        "\n",
        "model.fit(x_train_tfv, y_train_a) # 为了减少计算量，这里我们仅使用xtrain\n",
        "sc= model.score( x_val_tfv, y_val_a)\n",
        "print('valiation score: %0.3f' % sc )\n",
        "print(\"Best score: %0.3f\" % model.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters = model.best_estimator_.get_params()\n",
        "for param_name in sorted(param_grid.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valiation score: 0.724\n",
            "Best score: 0.720\n",
            "Best parameters set:\n",
            "\tnb__alpha: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1873s.) Setting batch_size=2.\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Done  15 out of  18 | elapsed:    1.3s remaining:    0.3s\n",
            "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    1.4s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "cNTe6jjVqHkM",
        "colab_type": "code",
        "outputId": "e1bdfdf1-2ff1-42bc-f8c7-869d6ff3e8d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        }
      },
      "cell_type": "code",
      "source": [
        "from hypopt import GridSearch\n",
        "\n",
        "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "opt = GridSearch(model= MultinomialNB(), param_grid=param_grid)\n",
        "opt.fit(x_train_tfv, y_train_a, x_val_tfv, y_val_a, scoring = 'accuracy')\n",
        "print('Test Score for Optimized Parameters:', opt.score(x_val_tfv, y_val_a))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Score for Optimized Parameters: 0.724320241691843\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HFliM2YVYDua",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Embedding"
      ]
    },
    {
      "metadata": {
        "id": "XRvvEopZhSOB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Deep Neural Network\n"
      ]
    },
    {
      "metadata": {
        "id": "cuzX9VzC1A3E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from keras import backend as K\n",
        "\n",
        "# def f1(y_true, y_pred):\n",
        "#     def recall(y_true, y_pred):\n",
        "#         \"\"\"Recall metric.\n",
        "\n",
        "#         Only computes a batch-wise average of recall.\n",
        "\n",
        "#         Computes the recall, a metric for multi-label classification of\n",
        "#         how many relevant items are selected.\n",
        "#         \"\"\"\n",
        "#         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "#         possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "#         recall = true_positives / (possible_positives + K.epsilon())\n",
        "#         return recall\n",
        "\n",
        "#     def precision(y_true, y_pred):\n",
        "#         \"\"\"Precision metric.\n",
        "\n",
        "#         Only computes a batch-wise average of precision.\n",
        "\n",
        "#         Computes the precision, a metric for multi-label classification of\n",
        "#         how many selected items are relevant.\n",
        "#         \"\"\"\n",
        "#         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "#         predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "#         precision = true_positives / (predicted_positives + K.epsilon())\n",
        "#         return precision\n",
        "#     precision = precision(y_true, y_pred)\n",
        "#     recall = recall(y_true, y_pred)\n",
        "#     return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "# macro f1 score\n",
        "def f1(y_true, y_pred):\n",
        "    y_pred = K.round(y_pred)\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return K.mean(f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DnnC2LiKCi8m",
        "colab_type": "code",
        "outputId": "19746a7f-d1aa-4f80-9feb-2dab62872ad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_a,x_val_a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19037/19037 [00:00<00:00, 467031.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "BLTZ0AXKcIBB",
        "colab_type": "code",
        "outputId": "fb87c938-6dd7-4557-813b-f73244722fb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "cell_type": "code",
      "source": [
        "#Fully Connected Network\n",
        "def fcn_model(num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam', metrics=['accuracy'] ):\n",
        "    fcn_model = Sequential()\n",
        "    fcn_model.add(Dense(300, input_dim=300, activation='relu'))\n",
        "    fcn_model.add(Dropout(0.2))\n",
        "    fcn_model.add(BatchNormalization())\n",
        "\n",
        "    fcn_model.add(Dense(300, activation='relu'))\n",
        "    fcn_model.add(Dropout(0.3))\n",
        "    fcn_model.add(BatchNormalization())\n",
        "    fcn_model.add(Dense(num_classes, activation=final_activation))\n",
        "\n",
        "    fcn_model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
        "    return fcn_model\n",
        "  \n",
        "fcn_model = fcn_model()\n",
        "fcn_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_21 (Dense)             (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 301       \n",
            "=================================================================\n",
            "Total params: 183,301\n",
            "Trainable params: 182,101\n",
            "Non-trainable params: 1,200\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bPOt81aCjNOP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "def lstm_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam', metrics=['accuracy']):\n",
        "    lstm_model = Sequential()\n",
        "    lstm_model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix],  input_length=max_len, trainable=False))\n",
        "    lstm_model.add(SpatialDropout1D(0.3))\n",
        "    lstm_model.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.3)))\n",
        "    lstm_model.add(Dense(512, activation='relu'))\n",
        "    lstm_model.add(Dropout(0.3))\n",
        "    lstm_model.add(Dense(256, activation='relu'))\n",
        "    lstm_model.add(Dropout(0.3))\n",
        "\n",
        "    lstm_model.add(Dense(num_classes,activation=final_activation))\n",
        "    \n",
        "    lstm_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return lstm_model\n",
        "# model = lstm_model(word_index = word_index, max_len = max_len, embedding_matrix = embedding_matrix)\n",
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NYL2KFv6mJra",
        "colab_type": "code",
        "outputId": "52744700-29b9-4e09-ef21-4e6652d15f2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "# GRU Network\n",
        "def gru_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam',  metrics=['accuracy']):\n",
        "\n",
        "    gru_model = Sequential()\n",
        "    gru_model.add(Embedding(len(word_index) + 1, 50, weights=[embedding_matrix],  input_length=max_len, trainable=False))\n",
        "    gru_model.add(SpatialDropout1D(0.3))\n",
        "    gru_model.add(GRU(128, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
        "    gru_model.add(GRU(64, dropout=0.3, recurrent_dropout=0.3))\n",
        "\n",
        "    gru_model.add(Dense(256, activation='relu'))\n",
        "    gru_model.add(Dropout(0.3))\n",
        "\n",
        "    gru_model.add(Dense(128, activation='relu'))\n",
        "    gru_model.add(Dropout(0.3))\n",
        "    gru_model.add(Dense(num_classes, activation=final_activation))\n",
        "                  \n",
        "    gru_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return gru_model\n",
        "model = gru_model(word_index,max_len,embedding_matrix)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (None, 57, 300)           5711400   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_5 (Spatial (None, 57, 300)           0         \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  (None, 57, 128)           164736    \n",
            "_________________________________________________________________\n",
            "gru_8 (GRU)                  (None, 64)                37056     \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 5,962,857\n",
            "Trainable params: 251,457\n",
            "Non-trainable params: 5,711,400\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4FnHvXIz1PHB",
        "colab_type": "code",
        "outputId": "81f8e91b-8502-470c-b50d-5055631f10b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "cell_type": "code",
      "source": [
        "# CNN network\n",
        "def cnn_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam',  metrics=['accuracy']):\n",
        "\n",
        "    cnn_model = Sequential()\n",
        "    cnn_model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],input_length=max_len, trainable=False))\n",
        "    cnn_model.add(Conv1D(256, 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(128, 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(64, 3, padding='same'))\n",
        "    cnn_model.add(Flatten())\n",
        "    cnn_model.add(Dropout(0.3))\n",
        "    cnn_model.add(BatchNormalization()) \n",
        "    cnn_model.add(Dense(256,activation='relu'))\n",
        "    cnn_model.add(Dropout(0.1))\n",
        "    cnn_model.add(Dense(num_classes, activation=final_activation))\n",
        "    \n",
        "    cnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return cnn_model\n",
        "  \n",
        "model = cnn_model(word_index,max_len,embedding_matrix)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 57, 300)           5711400   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 57, 256)           230656    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 19, 256)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 19, 128)           98432     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 7, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 7, 64)             24640     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 448)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 448)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 448)               1792      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               114944    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 6,182,121\n",
            "Trainable params: 469,825\n",
            "Non-trainable params: 5,712,296\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VhJ3A4uU1V_m",
        "colab_type": "code",
        "outputId": "ba1f8ecb-7dec-4801-a260-97ec123aa5de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        }
      },
      "cell_type": "code",
      "source": [
        "# TextCNN\n",
        "def textcnn_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam', metrics=['accuracy']):\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embedder = Embedding(len(word_index) + 1, 300, input_length = max_len, weights = [embedding_matrix], trainable = False)\n",
        "    embed = embedder(main_input)\n",
        "    cnn1 = Conv1D(256, 3, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn1 = MaxPooling1D(pool_size=4)(cnn1)\n",
        "    cnn2 = Conv1D(256, 4, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn2 = MaxPooling1D(pool_size=4)(cnn2)\n",
        "    cnn3 = Conv1D(256, 5, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn3 = MaxPooling1D(pool_size=4)(cnn3)\n",
        "    \n",
        "    # Combine three model into one\n",
        "    cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
        "    flat = Flatten()(cnn)\n",
        "    drop = Dropout(0.2)(flat)\n",
        "    main_output = Dense(num_classes, activation=final_activation)(drop)\n",
        "    textcnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    textcnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return textcnn_model\n",
        "  \n",
        "model = textcnn_model(word_index,max_len,embedding_matrix)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 57)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 57, 300)      5711400     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 57, 256)      230656      embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 57, 256)      307456      embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 57, 256)      384256      embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 14, 256)      0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 14, 256)      0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 14, 256)      0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 14, 768)      0           max_pooling1d_3[0][0]            \n",
            "                                                                 max_pooling1d_4[0][0]            \n",
            "                                                                 max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 10752)        0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 10752)        0           flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_32 (Dense)                (None, 1)            10753       dropout_24[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 6,644,521\n",
            "Trainable params: 933,121\n",
            "Non-trainable params: 5,711,400\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vlLbPaIQ1ePw",
        "colab_type": "code",
        "outputId": "43fe8829-eea9-4655-8b89-94c73e609ab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "cell_type": "code",
      "source": [
        "# CNN+RNN\n",
        "def crnn_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam',  metrics=[f1]):\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embed = Embedding(len(word_index)+1, 300, input_length=max_len)(main_input)\n",
        "    cnn = Conv1D(256, 3, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn = MaxPooling1D(pool_size=4)(cnn)\n",
        "    cnn = Flatten()(cnn)\n",
        "    cnn = Dense(256)(cnn)\n",
        "    cnn = Dropout(0.3)(cnn)\n",
        "    rnn = Bidirectional(GRU(256, dropout=0.2, recurrent_dropout=0.1))(embed)\n",
        "    rnn = Dense(256)(rnn)\n",
        "    rnn = Dropout(0.3)(rnn)\n",
        "    con = concatenate([cnn,rnn], axis=-1)\n",
        "    main_output = Dense(num_classes, activation=final_activation)(con)\n",
        "    crnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    crnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return crnn_model\n",
        "\n",
        "model = crnn_model(word_index,max_len,embedding_matrix)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 57)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 57, 300)      5711400     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 57, 256)      230656      embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, 14, 256)      0           conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 3584)         0           max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 512)          855552      embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_33 (Dense)                (None, 256)          917760      flatten_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_34 (Dense)                (None, 256)          131328      bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 256)          0           dense_33[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 256)          0           dense_34[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 512)          0           dropout_25[0][0]                 \n",
            "                                                                 dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_35 (Dense)                (None, 1)            513         concatenate_2[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 7,847,209\n",
            "Trainable params: 7,847,209\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YmYuFT_31lmU",
        "colab_type": "code",
        "outputId": "86201e7e-65c8-47b2-fde3-d056bb924b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1295
        }
      },
      "cell_type": "code",
      "source": [
        "# Extended textCNN\n",
        "def etextcnn_model(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam',  metrics=['accuracy']):\n",
        "\n",
        "\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embedder = Embedding(len(word_index) + 1, 300, input_length = max_len, weights = [embedding_matrix], trainable = False)\n",
        "    embed = embedder(main_input)\n",
        "\n",
        "    # cnn1，kernel_size = 3\n",
        "    conv1_1 = Conv1D(256, 3, padding='same')(embed)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    relu1_1 = Activation('relu')(bn1_1)\n",
        "    drop1_1 = Dropout(0.3)(relu1_1)\n",
        "    conv1_2 = Conv1D(128, 3, padding='same')(drop1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    relu1_2 = Activation('relu')(bn1_2)\n",
        "    drop1_2 = Dropout(0.3)(relu1_2)\n",
        "    cnn1 = MaxPooling1D(pool_size=4)(drop1_1)\n",
        "    # cnn2，kernel_size = 4\n",
        "    conv2_1 = Conv1D(256, 4, padding='same')(embed)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    relu2_1 = Activation('relu')(bn2_1)\n",
        "    drop2_1 = Dropout(0.3)(relu2_1)\n",
        "    conv2_2 = Conv1D(128, 4, padding='same')(drop2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    relu2_2 = Activation('relu')(bn2_2)\n",
        "    drop2_2 = Dropout(0.3)(relu2_2)\n",
        "    cnn2 = MaxPooling1D(pool_size=4)(drop2_2)\n",
        "    # cnn3，kernel_size = 5\n",
        "    conv3_1 = Conv1D(256, 5, padding='same')(embed)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    relu3_1 = Activation('relu')(bn3_1)\n",
        "    drop3_1 = Dropout(0.3)(relu3_1)\n",
        "    conv3_2 = Conv1D(128, 5, padding='same')(drop3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    relu3_2 = Activation('relu')(bn3_2)\n",
        "    drop3_2 = Dropout(0.3)(relu3_2)\n",
        "    cnn3 = MaxPooling1D(pool_size=4)(drop3_2)\n",
        "\n",
        "    # Combine three block\n",
        "    cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
        "    flat = Flatten()(cnn)\n",
        "    drop = Dropout(0.5)(flat)\n",
        "    fc = Dense(512)(drop)\n",
        "    bn = BatchNormalization()(fc)\n",
        " \n",
        "    main_output = Dense(num_classes, activation=final_activation)(bn)\n",
        "    etextcnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    etextcnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return etextcnn_model\n",
        "\n",
        "model = etextcnn_model(word_index,max_len,embedding_matrix)\n",
        "model.summary()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 57)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 57, 300)      5711400     input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 57, 256)      307456      embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_12 (Conv1D)              (None, 57, 256)      384256      embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 57, 256)      1024        conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 57, 256)      1024        conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 57, 256)      0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 57, 256)      0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 57, 256)      0           activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 57, 256)      0           activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 57, 256)      230656      embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 57, 128)      131200      dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_13 (Conv1D)              (None, 57, 128)      163968      dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 57, 256)      1024        conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 57, 128)      512         conv1d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 57, 128)      512         conv1d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 57, 256)      0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 57, 128)      0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 57, 128)      0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 57, 256)      0           activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 57, 128)      0           activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 57, 128)      0           activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_7 (MaxPooling1D)  (None, 14, 256)      0           dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1D)  (None, 14, 128)      0           dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_9 (MaxPooling1D)  (None, 14, 128)      0           dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 14, 512)      0           max_pooling1d_7[0][0]            \n",
            "                                                                 max_pooling1d_8[0][0]            \n",
            "                                                                 max_pooling1d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 7168)         0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 7168)         0           flatten_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_36 (Dense)                (None, 512)          3670528     dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 512)          2048        dense_36[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_37 (Dense)                (None, 1)            513         batch_normalization_10[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 10,606,121\n",
            "Trainable params: 4,891,649\n",
            "Non-trainable params: 5,714,472\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MM0X0n192mBp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def simple_visualize(model):\n",
        "    plt.subplot(211)\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.plot(history.history[\"acc\"], color=\"g\", label=\"Train\")\n",
        "    plt.plot(history.history[\"val_acc color=\"b\", label=\"Test\")\n",
        "    plt.legend(loc=\"best\")\n",
        "\n",
        "    plt.subplot(212)\n",
        "    plt.title(\"Loss\")\n",
        "    plt.plot(result.history[\"loss\"], color=\"g\", label=\"Train\")\n",
        "    plt.plot(result.history[\"val_loss\"], color=\"b\", label=\"Test\")\n",
        "    plt.legend(loc=\"best\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OSx_rgGh9bf6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Tuning "
      ]
    },
    {
      "metadata": {
        "id": "Hc8iJGp89abo",
        "colab_type": "code",
        "outputId": "2d6f1105-3a34-4db4-a1a4-58b63af504ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1867
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_a,x_val_a)\n",
        "\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import Adadelta, Adam, rmsprop\n",
        "    \n",
        "space = {\n",
        "            'units1': hp.choice('units1', [16,32,64,128,256,512]),\n",
        "            'units2': hp.choice('units2', [16,32,64,128,256,512]),\n",
        "            'units3': hp.choice('units3', [16,32,64,128,256,512]),\n",
        "            'units4': hp.choice('units4', [16,32,64,128,256,512]),\n",
        "            'dropout1': hp.uniform('dropout1', .25,.75),\n",
        "            'dropout2': hp.uniform('dropout2',  .25,.75),\n",
        "            'dropout3': hp.uniform('dropout3',  .25,.75),\n",
        "            'dropout4': hp.uniform('dropout4',  .25,.75),\n",
        "\n",
        "            'batch_size' : hp.choice('batch_size', [32,64,128,256,512]),\n",
        "\n",
        "            'epochs' :  3,\n",
        "            'optimizer': hp.choice('optimizer',['sgd','adadelta','adam','rmsprop']),\n",
        "        }\n",
        "\n",
        "def gru_tune(params):   \n",
        "\n",
        "    # GRU Network\n",
        "\n",
        "    gru_model = Sequential()\n",
        "    gru_model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix],  input_length=max_len, trainable=False))\n",
        "    gru_model.add(SpatialDropout1D(0.3))\n",
        "    gru_model.add(GRU(params['units1'], dropout=params['dropout1'], recurrent_dropout=0.3, return_sequences=True))\n",
        "    gru_model.add(GRU(params['units2'], dropout=params['dropout2'], recurrent_dropout=0.3))\n",
        "\n",
        "    gru_model.add(Dense(params['units3'], activation='relu',kernel_initializer = \"glorot_uniform\"))\n",
        "    gru_model.add(Dropout(params['dropout3']))\n",
        "\n",
        "    gru_model.add(Dense(params['units4'], activation='relu',kernel_initializer = \"glorot_uniform\"))\n",
        "    gru_model.add(Dropout(params['dropout4']))\n",
        "    gru_model.add(Dense(1, activation='sigmoid'))\n",
        "                  \n",
        "    gru_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "    gru_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = gru_model.evaluate(x_val_pad, y_val_a)\n",
        "    print(loss)\n",
        "    return {'loss': loss[0], 'status': STATUS_OK}\n",
        "\n",
        "def lstm_tune(params):\n",
        "    lstm_model = Sequential()\n",
        "    lstm_model.add(Embedding(len(word_index) + 1, 300, weights=[embedding_matrix],  input_length=max_len, trainable=False))\n",
        "    lstm_model.add(SpatialDropout1D(0.3))\n",
        "    lstm_model.add(Bidirectional(LSTM(params['units1'], dropout=params['dropout1'], recurrent_dropout=0.3)))\n",
        "    lstm_model.add(Dense(params['units2'], activation='relu'))\n",
        "    lstm_model.add(Dropout(params['dropout2']))\n",
        "    lstm_model.add(Dense(params['units3'], activation='relu'))\n",
        "    lstm_model.add(Dropout(params['dropout3']))\n",
        "\n",
        "    lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    lstm_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "    lstm_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = lstm_model.evaluate(x_val_pad, y_val_a)\n",
        "    print('validation accuracy :', loss[1])\n",
        "    return {'loss': -loss[1], 'status': STATUS_OK}\n",
        "\n",
        "def etextcnn_tune(params):\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embedder = Embedding(len(word_index) + 1, 300, input_length = max_len, weights = [embedding_matrix], trainable = False)\n",
        "    embed = embedder(main_input)\n",
        "\n",
        "    # cnn1，kernel_size = 3\n",
        "    conv1_1 = Conv1D(params['units1'], 3, padding='same')(embed)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    relu1_1 = Activation('relu')(bn1_1)\n",
        "    drop1_1 = Dropout(params['dropout1'])(relu1_1)\n",
        "    conv1_2 = Conv1D(params['units2'], 3, padding='same')(drop1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    relu1_2 = Activation('relu')(bn1_2)\n",
        "    drop1_2 = Dropout(params['dropout2'])(relu1_2)\n",
        "    cnn1 = MaxPooling1D(pool_size=4)(drop1_1)\n",
        "    # cnn2，kernel_size = 4\n",
        "    conv2_1 = Conv1D(params['units1'], 4, padding='same')(embed)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    relu2_1 = Activation('relu')(bn2_1)\n",
        "    drop2_1 = Dropout(params['dropout1'])(relu2_1)\n",
        "    conv2_2 = Conv1D(params['units2'], 4, padding='same')(drop2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    relu2_2 = Activation('relu')(bn2_2)\n",
        "    drop2_2 = Dropout(params['dropout2'])(relu2_2)\n",
        "    cnn2 = MaxPooling1D(pool_size=4)(drop2_2)\n",
        "    # cnn3，kernel_size = 5\n",
        "    conv3_1 = Conv1D(params['units1'], 5, padding='same')(embed)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    relu3_1 = Activation('relu')(bn3_1)\n",
        "    drop3_1 = Dropout(params['dropout1'])(relu3_1)\n",
        "    conv3_2 = Conv1D(params['units2'], 5, padding='same')(drop3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    relu3_2 = Activation('relu')(bn3_2)\n",
        "    drop3_2 = Dropout(params['dropout2'])(relu3_2)\n",
        "    cnn3 = MaxPooling1D(pool_size=4)(drop3_2)\n",
        "\n",
        "    # Combine three block\n",
        "    cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
        "    flat = Flatten()(cnn)\n",
        "    drop = Dropout(params['dropout3'])(flat)\n",
        "    fc = Dense(params['units3'])(drop)\n",
        "    bn = BatchNormalization()(fc)\n",
        " \n",
        "    main_output = Dense(1, activation='sigmoid')(bn)\n",
        "    etextcnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    \n",
        "    etextcnn_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "    etextcnn_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = etextcnn_model.evaluate(x_val_pad, y_val_a)\n",
        "    print('validation accuracy :', loss[1])\n",
        "    return {'loss': loss[0], 'status': STATUS_OK}\n",
        "\n",
        "def rcnn_tune(params):\n",
        "    main_input = Input(shape=(max_len,), dtype='float64')\n",
        "    embed = Embedding(len(word_index)+1, 300, input_length=max_len)(main_input)\n",
        "    cnn = Conv1D(params['units1'], 3, padding='same', strides = 1, activation='relu')(embed)\n",
        "    cnn = MaxPooling1D(pool_size=4)(cnn)\n",
        "    cnn = Flatten()(cnn)\n",
        "    cnn = Dense(params['units2'])(cnn)\n",
        "    cnn = Dropout(params['dropout1'])(cnn)\n",
        "    rnn = Bidirectional(GRU(params['units1'], dropout=0.2, recurrent_dropout=0.1))(embed)\n",
        "    rnn = Dense(params['units3'])(rnn)\n",
        "    rnn = Dropout(params['dropout2'])(rnn)\n",
        "    con = concatenate([cnn,rnn], axis=-1)\n",
        "    main_output = Dense(1, activation='sigmoid')(con)\n",
        "    crnn_model = Model(inputs = main_input, outputs = main_output)                \n",
        "    crnn_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "\n",
        "    crnn_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = crnn_model.evaluate(x_val_pad, y_val_a)\n",
        "    print('validation accuracy :', loss[1])\n",
        "    return {'loss': loss[0], 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "def cnn_tune(params):\n",
        "    cnn_model = Sequential()\n",
        "    cnn_model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],input_length=max_len, trainable=False))\n",
        "    cnn_model.add(Conv1D(params['units1'], 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(params['units2'], 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(params['units3'], 3, padding='same'))\n",
        "    cnn_model.add(Flatten())\n",
        "    cnn_model.add(Dropout(params['dropout1']))\n",
        "    cnn_model.add(BatchNormalization()) \n",
        "    cnn_model.add(Dense(params['units4'],activation='relu'))\n",
        "    cnn_model.add(Dropout(params['dropout2']))\n",
        "    cnn_model.add(Dense(1,activation ='sigmoid'))\n",
        "    \n",
        "    cnn_model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n",
        "\n",
        "    cnn_model.fit(x_train_pad, y_train_a, epochs=params['epochs'], batch_size=params['batch_size'], verbose = 0)    \n",
        "    loss = cnn_model.evaluate(x_val_pad, y_val_a)\n",
        "    print('validation accuracy :', loss)\n",
        "    return {'loss': -loss[1], 'status': STATUS_OK}\n",
        "\n",
        "trials = Trials()\n",
        "best = fmin(lstm_tune, space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
        "print('best:', best)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19037/19037 [00:00<00:00, 379607.23it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-28ba5db721d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_tune\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-28ba5db721d4>\u001b[0m in \u001b[0;36mlstm_tune\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation accuracy :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "inYlGBtjQPWl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cnn_model_opti(word_index, max_len, embedding_matrix, num_classes =1, final_activation = 'sigmoid', loss = 'binary_crossentropy', optim='adam', metrics=['accuracy']):\n",
        "\n",
        "\n",
        "    cnn_model = Sequential()\n",
        "    cnn_model.add(Embedding(len(word_index) + 1,300,weights=[embedding_matrix],input_length=max_len, trainable=False))\n",
        "    cnn_model.add(Conv1D(512, 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(16, 3, padding='same'))\n",
        "    cnn_model.add(MaxPooling1D(3,3,padding='same'))\n",
        "    cnn_model.add(Conv1D(32, 3, padding='same'))\n",
        "    cnn_model.add(Flatten())\n",
        "    cnn_model.add(Dropout(0.74))\n",
        "    cnn_model.add(BatchNormalization()) \n",
        "    cnn_model.add(Dense(32,activation='relu'))\n",
        "    cnn_model.add(Dropout(0.38))\n",
        "    cnn_model.add(Dense(num_classes,activation =final_activation))\n",
        "                  \n",
        "    cnn_model.compile(loss=loss, optimizer=optim,metrics=metrics)\n",
        "    return cnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OEiyRFcQqrkN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vzt4jjeHyZRc",
        "colab_type": "code",
        "outputId": "4144a39c-87d0-456c-ead5-25c655e8703f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "cell_type": "code",
      "source": [
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "\n",
        "model = cnn_model_opti(word_index,max_len,embedding_matrix)\n",
        "model.fit(x_train_pad, y=y_train_a, batch_size=256, epochs=20, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_a), callbacks=[earlystop])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e0b181e98315>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mearlystop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_model_opti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m model.fit(x_train_pad, y=y_train_a, batch_size=256, epochs=20, \n\u001b[1;32m      5\u001b[0m           verbose=1, validation_data=(x_val_pad, y_val_a), callbacks=[earlystop])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word_index' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "JEZ1KDBEnohI",
        "colab_type": "code",
        "outputId": "d716ce6f-aa1f-41a4-b129-0976f198e984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_a,x_val_a)\n",
        "_,x_test_pad,_,_,_= tokenize_text(x_train_a,x_test_a)\n",
        "pred= model.predict(x_test_pad)\n",
        "\n",
        "pred=np.where(pred>=0.5,1,0).reshape(-1,)\n",
        "f1_score(np.zeros(len(x_test_pad)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19037/19037 [00:00<00:00, 543946.30it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5f9a9b5064d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_train_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_val_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test_pad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtokenize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x_test_a' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "psYSnNLz2ZVK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sub_task A"
      ]
    },
    {
      "metadata": {
        "id": "ergaPK7-FMjd",
        "colab_type": "code",
        "outputId": "64e84071-6dcc-4079-9c42-0101c7394e88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_a,x_val_a)\n",
        "_,x_test_pad,_,_,_= tokenize_text(x_train_a,x_test_a)\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "\n",
        "model = gru_model(word_index,max_len,embedding_matrix)\n",
        "model.fit(x_train_pad, y=y_train_a, batch_size=128, epochs=20, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_a), callbacks=[earlystop])\n",
        "pred= model.predict(x_test_pad)\n",
        "pred=np.where(pred>=0.5,1,0).reshape(-1,)\n",
        "f1_score(np.zeros(len(x_test_pad)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19037/19037 [00:00<00:00, 414839.00it/s]\n",
            "100%|██████████| 19037/19037 [00:00<00:00, 401721.48it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/20\n",
            "11916/11916 [==============================] - 24s 2ms/step - loss: 0.5973 - acc: 0.6960 - val_loss: 0.5435 - val_acc: 0.7356\n",
            "Epoch 2/20\n",
            "11916/11916 [==============================] - 21s 2ms/step - loss: 0.5290 - acc: 0.7450 - val_loss: 0.5155 - val_acc: 0.7508\n",
            "Epoch 3/20\n",
            "11916/11916 [==============================] - 21s 2ms/step - loss: 0.5051 - acc: 0.7656 - val_loss: 0.5103 - val_acc: 0.7515\n",
            "Epoch 4/20\n",
            "11916/11916 [==============================] - 21s 2ms/step - loss: 0.4946 - acc: 0.7663 - val_loss: 0.5047 - val_acc: 0.7598\n",
            "Epoch 5/20\n",
            "11916/11916 [==============================] - 21s 2ms/step - loss: 0.4896 - acc: 0.7730 - val_loss: 0.5016 - val_acc: 0.7591\n",
            "Epoch 6/20\n",
            "11916/11916 [==============================] - 21s 2ms/step - loss: 0.4847 - acc: 0.7774 - val_loss: 0.5066 - val_acc: 0.7659\n",
            "Epoch 7/20\n",
            "11916/11916 [==============================] - 21s 2ms/step - loss: 0.4800 - acc: 0.7752 - val_loss: 0.5050 - val_acc: 0.7591\n",
            "Epoch 8/20\n",
            "11916/11916 [==============================] - 21s 2ms/step - loss: 0.4745 - acc: 0.7807 - val_loss: 0.4961 - val_acc: 0.7583\n",
            "Epoch 9/20\n",
            "11916/11916 [==============================] - 21s 2ms/step - loss: 0.4672 - acc: 0.7838 - val_loss: 0.5022 - val_acc: 0.7636\n",
            "Epoch 10/20\n",
            "11916/11916 [==============================] - 21s 2ms/step - loss: 0.4630 - acc: 0.7851 - val_loss: 0.4942 - val_acc: 0.7591\n",
            "Epoch 11/20\n",
            "11916/11916 [==============================] - 21s 2ms/step - loss: 0.4587 - acc: 0.7920 - val_loss: 0.5019 - val_acc: 0.7470\n",
            "Epoch 12/20\n",
            "11916/11916 [==============================] - 21s 2ms/step - loss: 0.4543 - acc: 0.7925 - val_loss: 0.5147 - val_acc: 0.7613\n",
            "Epoch 13/20\n",
            "11916/11916 [==============================] - 21s 2ms/step - loss: 0.4449 - acc: 0.7958 - val_loss: 0.5240 - val_acc: 0.7606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4430051813471502"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "xfcP97wIsxT5",
        "colab_type": "code",
        "outputId": "37748edd-5812-4b69-af50-dbe6f2ef5394",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_a,x_val_a)\n",
        "_,x_test_pad,_,_,_= tokenize_text(x_train_a,x_test_a)\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "\n",
        "model = lstm_model(word_index = word_index, max_len = max_len, embedding_matrix = embedding_matrix)\n",
        "model.fit(x_train_pad, y=y_train_a, batch_size=128, epochs=20, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_a), callbacks=[earlystop])\n",
        "pred= model.predict(x_test_pad)\n",
        "pred=np.where(pred>=0.5,1,0).reshape(-1,)\n",
        "f1_score(np.zeros(len(x_test_pad)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19037/19037 [00:00<00:00, 469251.90it/s]\n",
            "100%|██████████| 19037/19037 [00:00<00:00, 463004.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/20\n",
            "11916/11916 [==============================] - 30s 3ms/step - loss: 0.5808 - acc: 0.7043 - val_loss: 0.5283 - val_acc: 0.7387\n",
            "Epoch 2/20\n",
            "11916/11916 [==============================] - 26s 2ms/step - loss: 0.5222 - acc: 0.7425 - val_loss: 0.5184 - val_acc: 0.7515\n",
            "Epoch 3/20\n",
            "11916/11916 [==============================] - 26s 2ms/step - loss: 0.5011 - acc: 0.7581 - val_loss: 0.5120 - val_acc: 0.7477\n",
            "Epoch 4/20\n",
            "11916/11916 [==============================] - 26s 2ms/step - loss: 0.4871 - acc: 0.7716 - val_loss: 0.5019 - val_acc: 0.7545\n",
            "Epoch 5/20\n",
            "11916/11916 [==============================] - 26s 2ms/step - loss: 0.4795 - acc: 0.7743 - val_loss: 0.5101 - val_acc: 0.7477\n",
            "Epoch 6/20\n",
            "11916/11916 [==============================] - 26s 2ms/step - loss: 0.4753 - acc: 0.7765 - val_loss: 0.4985 - val_acc: 0.7545\n",
            "Epoch 7/20\n",
            "11916/11916 [==============================] - 26s 2ms/step - loss: 0.4609 - acc: 0.7847 - val_loss: 0.5039 - val_acc: 0.7568\n",
            "Epoch 8/20\n",
            "11916/11916 [==============================] - 26s 2ms/step - loss: 0.4596 - acc: 0.7844 - val_loss: 0.4963 - val_acc: 0.7545\n",
            "Epoch 9/20\n",
            "11916/11916 [==============================] - 26s 2ms/step - loss: 0.4519 - acc: 0.7889 - val_loss: 0.5086 - val_acc: 0.7508\n",
            "Epoch 10/20\n",
            "11916/11916 [==============================] - 26s 2ms/step - loss: 0.4409 - acc: 0.7943 - val_loss: 0.5051 - val_acc: 0.7538\n",
            "Epoch 11/20\n",
            "11916/11916 [==============================] - 26s 2ms/step - loss: 0.4328 - acc: 0.7982 - val_loss: 0.5068 - val_acc: 0.7583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.44046844502277166"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "slw8LHW2F14T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model.evaluate(x_val_pad, y_val_a)\n",
        "pred= model.predict(x_test_pad)\n",
        "pred=np.where(pred>=0.5,1,0).reshape(-1,)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xKEzaUkZx-7U",
        "colab_type": "code",
        "outputId": "ca82c2c2-7415-423a-fdc7-34032ff4d1bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "f1_score(np.zeros(len(x_test_pad)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4511805998723676"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "metadata": {
        "id": "Z_cmcuiOHfTE",
        "colab_type": "code",
        "outputId": "4ac40fe4-977b-4902-deb4-8afcc7cc225b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv, x_test_tfv = tf_idf(x_train_a, x_val_a,x_test_a)\n",
        "clf =logistic_regression(x_train_tfv,y_train_a, x_val_tfv, y_val_a)\n",
        "pred= clf.predict(x_test_tfv)\n",
        "f1_score(np.zeros(len(x_test_a)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression f1 score : 0.6505225610843589\n",
            "Logistic regression : 0.7484894259818731\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46716232961586124"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "metadata": {
        "id": "yq_ibz4zNuek",
        "colab_type": "code",
        "outputId": "26c2051c-7a64-4309-bf37-2be4f7abe946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_w2v,x_val_w2v = word_count(x_train_a,x_val_a)\n",
        "svm(x_train_w2v,y_train_a, x_val_w2v, y_val_a)\n",
        "naive_bayes(x_train_w2v,y_train_a, x_val_w2v, y_val_a)\n",
        "logistic_regression(x_train_w2v,y_train_a, x_val_w2v, y_val_a)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM : 0.7061933534743202\n",
            "Naive Bayes : 0.716012084592145\n",
            "Logistic regression : 0.7409365558912386\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "metadata": {
        "id": "EhgSgj-RMTBX",
        "colab_type": "code",
        "outputId": "2744d9a1-0d10-4a34-9453-b3e4ed0cd4f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "model = etextcnn_model(word_index,max_len,embedding_matrix)\n",
        "model.fit(x_train_pad, y=y_train_a, batch_size=512, epochs=100, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_a), callbacks=[earlystop])\n",
        "pred= model.predict(x_test_pad)\n",
        "pred=np.where(pred>=0.5,1,0).reshape(-1,)\n",
        "f1_score(np.zeros(len(x_test_pad)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11916 samples, validate on 1324 samples\n",
            "Epoch 1/100\n",
            "11916/11916 [==============================] - 14s 1ms/step - loss: 1.3878 - acc: 0.6164 - val_loss: 1.2405 - val_acc: 0.6813\n",
            "Epoch 2/100\n",
            "11916/11916 [==============================] - 5s 393us/step - loss: 0.6044 - acc: 0.7349 - val_loss: 0.6317 - val_acc: 0.7258\n",
            "Epoch 3/100\n",
            "11916/11916 [==============================] - 5s 397us/step - loss: 0.5061 - acc: 0.7708 - val_loss: 0.6321 - val_acc: 0.7258\n",
            "Epoch 4/100\n",
            "11916/11916 [==============================] - 5s 393us/step - loss: 0.4708 - acc: 0.7849 - val_loss: 0.6212 - val_acc: 0.7319\n",
            "Epoch 5/100\n",
            "11916/11916 [==============================] - 5s 394us/step - loss: 0.4382 - acc: 0.8038 - val_loss: 0.6879 - val_acc: 0.7175\n",
            "Epoch 6/100\n",
            "11916/11916 [==============================] - 5s 393us/step - loss: 0.4076 - acc: 0.8162 - val_loss: 0.7296 - val_acc: 0.7190\n",
            "Epoch 7/100\n",
            "11916/11916 [==============================] - 5s 393us/step - loss: 0.3770 - acc: 0.8374 - val_loss: 0.7877 - val_acc: 0.7069\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4837935174069628"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "metadata": {
        "id": "j_VACJnNqujs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sub_task B\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "V3qSHBfcJ-d3",
        "colab_type": "code",
        "outputId": "d683962d-6dc5-43cd-8d5e-1ebc806e9682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv, x_test_tfv = tf_idf(x_train_b, x_val_b,x_test_b)\n",
        "clf =logistic_regression(x_train_tfv,y_train_b, x_val_tfv, y_val_b)\n",
        "pred= clf.predict(x_test_tfv)\n",
        "f1_score(np.ones(len(x_test_b)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression f1 score : 0.487041520939826\n",
            "Logistic regression : 0.8818181818181818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4989561586638831"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "metadata": {
        "id": "6Cn6RqC47nHV",
        "colab_type": "code",
        "outputId": "35260cef-951b-4feb-930a-55b924a8634d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_b,x_val_b)\n",
        "_,x_test_pad,_,_,_= tokenize_text(x_train_b,x_test_b)\n",
        "model = lstm_model(word_index,max_len,embedding_matrix)\n",
        "model.fit(x_train_pad, y=y_train_b, batch_size=512, epochs=100, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_b), callbacks=[earlystop])\n",
        "pred= model.predict(x_test_pad)\n",
        "pred=np.where(pred>=0.5,1,0).reshape(-1,)\n",
        "f1_score(np.ones(len(x_test_pad)),pred, average='macro')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10007/10007 [00:00<00:00, 405648.02it/s]\n",
            "100%|██████████| 10007/10007 [00:00<00:00, 389904.13it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 3960 samples, validate on 440 samples\n",
            "Epoch 1/100\n",
            "3960/3960 [==============================] - 13s 3ms/step - loss: 0.4700 - f1: 0.8965 - val_loss: 0.4065 - val_f1: 0.9372\n",
            "Epoch 2/100\n",
            "3960/3960 [==============================] - 2s 546us/step - loss: 0.3827 - f1: 0.9365 - val_loss: 0.3654 - val_f1: 0.9372\n",
            "Epoch 3/100\n",
            "3960/3960 [==============================] - 2s 528us/step - loss: 0.3576 - f1: 0.9366 - val_loss: 0.3438 - val_f1: 0.9372\n",
            "Epoch 4/100\n",
            "3960/3960 [==============================] - 2s 527us/step - loss: 0.3485 - f1: 0.9366 - val_loss: 0.3461 - val_f1: 0.9372\n",
            "Epoch 5/100\n",
            "3960/3960 [==============================] - 2s 528us/step - loss: 0.3402 - f1: 0.9365 - val_loss: 0.3479 - val_f1: 0.9372\n",
            "Epoch 6/100\n",
            "3960/3960 [==============================] - 2s 530us/step - loss: 0.3402 - f1: 0.9366 - val_loss: 0.3466 - val_f1: 0.9372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "metadata": {
        "id": "3RyU9NJZVVIp",
        "colab_type": "code",
        "outputId": "af46adc1-5493-489d-8e8b-eef32668e0bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_pad, x_val_pad, max_len, word_index, embedding_matrix = tokenize_text(x_train_b,x_val_b)\n",
        "_,x_test_pad,_,_,_= tokenize_text(x_train_b,x_test_b)\n",
        "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "model = etextcnn_model(word_index,max_len,embedding_matrix)\n",
        "model.fit(x_train_pad, y=y_train_b, batch_size=512, epochs=100, \n",
        "          verbose=1, validation_data=(x_val_pad, y_val_b), callbacks=[earlystop])\n",
        "pred= model.predict(x_test_pad)\n",
        "pred=np.where(pred>=0.5,1,0).reshape(-1,)\n",
        "f1_score(np.ones(len(x_test_pad)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10007/10007 [00:00<00:00, 458915.37it/s]\n",
            "100%|██████████| 10007/10007 [00:00<00:00, 467107.37it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 3960 samples, validate on 440 samples\n",
            "Epoch 1/100\n",
            "3960/3960 [==============================] - 13s 3ms/step - loss: 2.3099 - acc: 0.5886 - val_loss: 1.4039 - val_acc: 0.4273\n",
            "Epoch 2/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 1.2392 - acc: 0.6381 - val_loss: 0.5397 - val_acc: 0.8795\n",
            "Epoch 3/100\n",
            "3960/3960 [==============================] - 1s 355us/step - loss: 0.8273 - acc: 0.7063 - val_loss: 0.5826 - val_acc: 0.7364\n",
            "Epoch 4/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 0.7141 - acc: 0.7672 - val_loss: 0.5014 - val_acc: 0.8795\n",
            "Epoch 5/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 0.6213 - acc: 0.8306 - val_loss: 0.4214 - val_acc: 0.8795\n",
            "Epoch 6/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 0.5643 - acc: 0.8510 - val_loss: 0.3715 - val_acc: 0.8795\n",
            "Epoch 7/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 0.4990 - acc: 0.8591 - val_loss: 0.3483 - val_acc: 0.8818\n",
            "Epoch 8/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 0.4405 - acc: 0.8790 - val_loss: 0.3431 - val_acc: 0.8818\n",
            "Epoch 9/100\n",
            "3960/3960 [==============================] - 1s 354us/step - loss: 0.3990 - acc: 0.8864 - val_loss: 0.3547 - val_acc: 0.8818\n",
            "Epoch 10/100\n",
            "3960/3960 [==============================] - 1s 353us/step - loss: 0.3614 - acc: 0.9015 - val_loss: 0.3706 - val_acc: 0.8818\n",
            "Epoch 11/100\n",
            "3960/3960 [==============================] - 1s 356us/step - loss: 0.3269 - acc: 0.9134 - val_loss: 0.3827 - val_acc: 0.8818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4989561586638831"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "metadata": {
        "id": "MCIFOHbexhuW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Subtask C"
      ]
    },
    {
      "metadata": {
        "id": "ncrukCSa_06U",
        "colab_type": "code",
        "outputId": "d150335e-9d27-4f74-872c-f9e279452f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_tfv, x_val_tfv, x_test_tfv = tf_idf(x_train_c, x_val_c,x_test_c)\n",
        "clf =logistic_regression(x_train_tfv,y_train_c, x_val_tfv, y_val_c)\n",
        "pred= clf.predict(x_test_tfv)\n",
        "print(pred)\n",
        "f1_score(np.ones(len(x_test_c)),pred, average='macro')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Logistic regression f1 score : 0.4442148960250622\n",
            "Logistic regression : 0.6881443298969072\n",
            "[0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 1 1\n",
            " 1 1 0 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1\n",
            " 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0\n",
            " 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0\n",
            " 1 0 1 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n",
            " 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.42857142857142855"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "metadata": {
        "id": "sxmUgN8uUUai",
        "colab_type": "code",
        "outputId": "a5ce9436-e5be-4920-ab90-9b7145573bfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(x_train_tfv, y_train_c_enc)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-198-af48c66456df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multinomial'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_tfv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_c_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1288\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    759\u001b[0m                         dtype=None)\n\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad input shape {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: bad input shape (3488, 3)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "sfTUq1siZgkB",
        "colab_type": "code",
        "outputId": "e2143ac5-6478-48dd-e642-dbb145f76bed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_test_c[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nopasaran unity demo oppose farright london  antifa oct  enough enough '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    }
  ]
}